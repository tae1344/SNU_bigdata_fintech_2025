"""

ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸


"""
import pandas as pd
import numpy as np
import warnings
import sys
import os
import time
from datetime import datetime
from pathlib import Path
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import FunctionTransformer
import pickle

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.file_paths import (
    SAMPLE_DATA_PATH,
    REPORTS_DIR,
    ensure_directory_exists
)

class DataLoader(BaseEstimator, TransformerMixin):
    """ë°ì´í„° ë¡œë”"""
    
    def __init__(self, data_path=None):
        self.data_path = data_path or SAMPLE_DATA_PATH
        
    def fit(self, X=None, y=None):
        return self
        
    def transform(self, X=None):
        """ë°ì´í„° ë¡œë“œ"""
        # Xê°€ ì œê³µë˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì—ì„œ ì‚¬ìš©)
        if X is not None:
            return X
            
        # Xê°€ ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ë¡œë“œ (ì™¸ë¶€ì—ì„œ ì§ì ‘ í˜¸ì¶œ)
        try:
            df = pd.read_csv(self.data_path, low_memory=False)
            print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
            return df
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

class AnomalousRowRemover(BaseEstimator, TransformerMixin):
    """ì´ìƒ ë¡œìš° ì œê±°"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        if 'id' in X.columns:
            original_size = len(X)
            X = X[X['id'] != 'Loans that do not meet the credit policy'].copy()
            removed_count = original_size - len(X)
            print(f"âœ“ ì´ìƒ ë¡œìš° ì œê±°: {removed_count}ê°œ")
        return X

class TargetVariableCreator(BaseEstimator, TransformerMixin):
    """íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        loan_status_mapping = {
            'Fully Paid': 0, 'Current': 0, 'In Grace Period': 0,
            'Late (16-30 days)': 1, 'Late (31-120 days)': 1,
            'Charged Off': 1, 'Default': 1
        }
        
        X['target'] = X['loan_status'].map(loan_status_mapping)
        target_dist = X['target'].value_counts()
        print(f"âœ“ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±: ë¶€ë„ìœ¨ {target_dist[1]/len(X)*100:.2f}%")
        return X

class PercentageCleaner(BaseEstimator, TransformerMixin):
    """í¼ì„¼íŠ¸ ì»¬ëŸ¼ ì •ë¦¬"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        percentage_columns = ['int_rate', 'revol_util']
        
        for col in percentage_columns:
            if col in X.columns:
                X[col] = X[col].astype(str).str.replace('%', '').astype(float)
                print(f"âœ“ {col}: í¼ì„¼íŠ¸ ê¸°í˜¸ ì œê±°")
        
        return X

class HighMissingValueHandler(BaseEstimator, TransformerMixin):
    """ê³ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ ì²˜ë¦¬"""
    
    def __init__(self, missing_threshold=0):  # 5% â†’ 0%ë¡œ ë³€ê²½ (ëª¨ë“  ê²°ì¸¡ì¹˜ ì²˜ë¦¬)
        self.missing_threshold = missing_threshold
        self.imputers = {}
        
    def fit(self, X, y=None):
        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚°
        missing_ratios = (X.isnull().sum() / len(X)) * 100
        self.high_missing_features = missing_ratios[missing_ratios >= self.missing_threshold].index.tolist()
        
        # ê° ë³€ìˆ˜ë³„ imputer í•™ìŠµ
        for feature in self.high_missing_features:
            if feature in X.columns:
                if X[feature].dtype in ['object', 'category']:
                    # ë²”ì£¼í˜•: ìµœë¹ˆê°’
                    mode_value = X[feature].mode().iloc[0] if not X[feature].mode().empty else 'Unknown'
                    self.imputers[feature] = ('mode', mode_value)
                else:
                    # ìˆ˜ì¹˜í˜•: ì¤‘ì•™ê°’
                    median_value = X[feature].median()
                    self.imputers[feature] = ('median', median_value)
        
        print(f"âœ“ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤€ë¹„: {len(self.high_missing_features)}ê°œ (ì„ê³„ê°’: {self.missing_threshold}%)")
        return self
        
    def transform(self, X):
        for feature in self.high_missing_features:
            if feature in X.columns and feature in self.imputers:
                impute_type, impute_value = self.imputers[feature]
                X[feature] = X[feature].fillna(impute_value)
                print(f"  âœ“ {feature}: {impute_type}ë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬")
        
        return X

class FICOFeatureCreator(BaseEstimator, TransformerMixin):
    """FICO íŠ¹ì„± ìƒì„±"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # FICO ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
        fico_columns = ['fico_range_low', 'fico_range_high', 
                       'last_fico_range_low', 'last_fico_range_high']
        
        available_fico_cols = [col for col in fico_columns if col in X.columns]
        
        if len(available_fico_cols) >= 2:
            # FICO í‰ê· ê°’ ê³„ì‚°
            if 'fico_range_low' in X.columns and 'fico_range_high' in X.columns:
                X['fico_avg'] = (pd.to_numeric(X['fico_range_low'], errors='coerce') + 
                                pd.to_numeric(X['fico_range_high'], errors='coerce')) / 2
            
            if 'last_fico_range_low' in X.columns and 'last_fico_range_high' in X.columns:
                X['last_fico_avg'] = (pd.to_numeric(X['last_fico_range_low'], errors='coerce') + 
                                     pd.to_numeric(X['last_fico_range_high'], errors='coerce')) / 2
            
            # FICO ë³€í™”ìœ¨ ê³„ì‚°
            if 'fico_avg' in X.columns and 'last_fico_avg' in X.columns:
                X['fico_change'] = X['last_fico_avg'] - X['fico_avg']
                X['fico_change_rate'] = X['fico_change'] / (X['fico_avg'] + 1e-8)
            
            # FICO êµ¬ê°„í™”
            if 'fico_avg' in X.columns:
                fico_bins = list(range(300, 850, 50)) + [850]
                fico_labels = [f'{fico_bins[i]}-{fico_bins[i+1]-1}' for i in range(len(fico_bins)-1)]
                X['fico_range'] = pd.cut(X['fico_avg'], bins=fico_bins, labels=fico_labels, include_lowest=True)
            
            print(f"âœ“ FICO íŠ¹ì„± ìƒì„±: 5ê°œ íŠ¹ì„±")
        
        return X

class CategoricalEncoder(BaseEstimator, TransformerMixin):
    """ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©"""
    
    def __init__(self):
        self.sub_grade_mapping = None
        self.emp_length_mapping = None
        
    def fit(self, X, y=None):
        # sub_grade ìˆœì„œí˜• ì¸ì½”ë”© ë§¤í•‘
        if 'sub_grade' in X.columns:
            grade_order = ['A1', 'A2', 'A3', 'A4', 'A5',
                          'B1', 'B2', 'B3', 'B4', 'B5',
                          'C1', 'C2', 'C3', 'C4', 'C5',
                          'D1', 'D2', 'D3', 'D4', 'D5',
                          'E1', 'E2', 'E3', 'E4', 'E5',
                          'F1', 'F2', 'F3', 'F4', 'F5',
                          'G1', 'G2', 'G3', 'G4', 'G5']
            self.sub_grade_mapping = {grade: idx for idx, grade in enumerate(grade_order)}
        
        # emp_length ë§¤í•‘
        if 'emp_length' in X.columns:
            self.emp_length_mapping = {
                '< 1 year': 0.5,
                '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4, '5 years': 5,
                '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9, '10+ years': 10
            }
        
        return self
        
    def transform(self, X):
        # sub_grade ìˆœì„œí˜• ì¸ì½”ë”©
        if self.sub_grade_mapping and 'sub_grade' in X.columns:
            X['sub_grade_ordinal'] = X['sub_grade'].map(self.sub_grade_mapping).fillna(0)
        
        # emp_length ìˆ˜ì¹˜í™”
        if self.emp_length_mapping and 'emp_length' in X.columns:
            X['emp_length_numeric'] = X['emp_length'].map(self.emp_length_mapping).fillna(0)
            X['emp_length_is_na'] = X['emp_length'].isna().astype(int)
        
        # home_ownership ì¹´í…Œê³ ë¦¬ ì •ë¦¬
        if 'home_ownership' in X.columns:
            X['home_ownership'] = X['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')
        
        print(f"âœ“ ë²”ì£¼í˜• ì¸ì½”ë”©: 3ê°œ íŠ¹ì„± ìƒì„±")
        return X

class OutlierHandler(BaseEstimator, TransformerMixin):
    """ì´ìƒê°’ ì²˜ë¦¬"""
    
    def fit(self, X, y=None):
        self.outlier_bounds = {}
        
        # dti 999 ì´ìƒê°’ ì²˜ë¦¬
        if 'dti' in X.columns:
            self.outlier_bounds['dti'] = 999
        
        # revol_util 100% ì´ˆê³¼ê°’ í´ë¦¬í•‘
        if 'revol_util' in X.columns:
            self.outlier_bounds['revol_util'] = 100
        
        # annual_inc IQR ê¸°ë°˜ ì´ìƒê°’ ì²˜ë¦¬
        if 'annual_inc' in X.columns:
            Q1 = X['annual_inc'].quantile(0.25)
            Q3 = X['annual_inc'].quantile(0.75)
            IQR = Q3 - Q1
            self.outlier_bounds['annual_inc'] = (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)
        
        return self
        
    def transform(self, X):
        outlier_handled = 0
        
        # dti ì²˜ë¦¬
        if 'dti' in self.outlier_bounds and 'dti' in X.columns:
            X['dti'] = np.where(X['dti'] >= self.outlier_bounds['dti'], X['dti'].median(), X['dti'])
            outlier_handled += 1
        
        # revol_util ì²˜ë¦¬
        if 'revol_util' in self.outlier_bounds and 'revol_util' in X.columns:
            X['revol_util'] = np.clip(X['revol_util'], 0, self.outlier_bounds['revol_util'])
            outlier_handled += 1
        
        # annual_inc ì²˜ë¦¬
        if 'annual_inc' in self.outlier_bounds and 'annual_inc' in X.columns:
            lower, upper = self.outlier_bounds['annual_inc']
            X['annual_inc'] = np.clip(X['annual_inc'], lower, upper)
            outlier_handled += 1
        
        print(f"âœ“ ì´ìƒê°’ ì²˜ë¦¬: {outlier_handled}ê°œ ë³€ìˆ˜")
        return X

class StateOptimizer(BaseEstimator, TransformerMixin):
    """ì£¼(state) ë°ì´í„° ìµœì í™”"""
    
    def fit(self, X, y=None):
        if 'addr_state' in X.columns:
            state_counts = X['addr_state'].value_counts()
            total_count = len(X)
            cumulative_percent = (state_counts.cumsum() / total_count) * 100
            self.keep_states = cumulative_percent[cumulative_percent <= 99].index.tolist()
        return self
        
    def transform(self, X):
        if hasattr(self, 'keep_states') and 'addr_state' in X.columns:
            X['addr_state_optimized'] = X['addr_state'].apply(
                lambda x: x if x in self.keep_states else 'OTHER'
            )
            print(f"âœ“ ì£¼ ë°ì´í„° ìµœì í™”: {X['addr_state'].nunique()} â†’ {X['addr_state_optimized'].nunique()}ê°œ")
        return X

class TimeFeatureCreator(BaseEstimator, TransformerMixin):
    """ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
        date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d']
        available_date_cols = [col for col in date_columns if col in X.columns]
        
        if len(available_date_cols) >= 2:
            # ëŒ€ì¶œ ë°œí–‰ ì‹œì  ì •ë³´ ì¶”ì¶œ
            if 'issue_d' in X.columns:
                X['issue_date'] = pd.to_datetime(X['issue_d'], format='%b-%Y', errors='coerce')
                X['issue_year'] = X['issue_date'].dt.year
                X['issue_month'] = X['issue_date'].dt.month
                X['issue_quarter'] = X['issue_date'].dt.quarter
                
                # ê³„ì ˆì„± íŠ¹ì„±
                X['issue_season'] = X['issue_month'].map({
                    12: 'Winter', 1: 'Winter', 2: 'Winter',
                    3: 'Spring', 4: 'Spring', 5: 'Spring',
                    6: 'Summer', 7: 'Summer', 8: 'Summer',
                    9: 'Fall', 10: 'Fall', 11: 'Fall'
                })
                
                # ê²½ì œ ì‚¬ì´í´ íŠ¹ì„±
                X['is_recession_year'] = X['issue_year'].isin([2008, 2009, 2020]).astype(int)
            
            # ì‹ ìš© ì´ë ¥ ê¸°ê°„ ê³„ì‚°
            if 'earliest_cr_line' in X.columns and 'issue_d' in X.columns:
                X['earliest_cr_date'] = pd.to_datetime(X['earliest_cr_line'], format='%b-%Y', errors='coerce')
                X['credit_history_months'] = ((X['issue_date'] - X['earliest_cr_date']).dt.days / 30.44).fillna(0)
                X['credit_history_years'] = X['credit_history_months'] / 12
            
                # ì‹ ìš© ì´ë ¥ êµ¬ê°„í™”
                X['credit_history_category'] = pd.cut(
                    X['credit_history_years'],
                    bins=[0, 2, 5, 10, 50],
                    labels=['New', 'Young', 'Established', 'Veteran']
                )
            
            # ë§ˆì§€ë§‰ ê²°ì œì¼ ì •ë³´
            if 'last_pymnt_d' in X.columns:
                X['last_pymnt_date'] = pd.to_datetime(X['last_pymnt_d'], format='%b-%Y', errors='coerce')
                X['days_since_last_payment'] = (pd.Timestamp.now() - X['last_pymnt_date']).dt.days.fillna(0)
            
            print(f"âœ“ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±: 12ê°œ íŠ¹ì„± ìƒì„±")
        
        return X

class CompositeFeatureCreator(BaseEstimator, TransformerMixin):
    """ë³µí•© ì§€í‘œ ìƒì„±"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ì‹ ìš© ì ìˆ˜ ë³€í™”ìœ¨ ê³„ì‚°
        if 'fico_change_rate' in X.columns:
            X['fico_improvement'] = (X['fico_change_rate'] > 0).astype(int)
            X['fico_decline'] = (X['fico_change_rate'] < 0).astype(int)
        
        # ì†Œë“ ëŒ€ë¹„ ë¶€ì±„ ë¹„ìœ¨ ì„¸ë¶„í™”
        if 'annual_inc' in X.columns and 'dti' in X.columns:
            X['debt_to_income_ratio'] = X['dti']
            X['income_category'] = pd.cut(
                X['annual_inc'],
                bins=[0, 30000, 60000, 100000, float('inf')],
                labels=['Low', 'Medium', 'High', 'Very High']
            )
        
        # ì—°ì²´ ì‹¬ê°ë„ ì ìˆ˜ ì²´ê³„í™”
        if 'delinq_2yrs' in X.columns:
            # ë””ë²„ê·¸: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ delinq_2yrs ìƒíƒœ í™•ì¸
            print(f"  ğŸ” delinq_2yrs ê²°ì¸¡ì¹˜: {X['delinq_2yrs'].isnull().sum()}ê°œ")
            print(f"  ğŸ” delinq_2yrs ê°’ ë¶„í¬: {X['delinq_2yrs'].value_counts().head(10).to_dict()}")
            
            # ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
            delinq_2yrs_clean = X['delinq_2yrs'].fillna(0).astype(int)
            
            # ë””ë²„ê·¸ ì •ë³´
            print(f"  ğŸ“Š delinq_2yrs ê°’ ë¶„í¬: {delinq_2yrs_clean.value_counts().to_dict()}")
            
            # ëª¨ë“  ê²½ìš°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
            X['delinquency_severity'] = 'None'  # ê¸°ë³¸ê°’
            
            # ì¡°ê±´ë³„ í• ë‹¹ (ëª¨ë“  ê²½ìš°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬)
            mask_none = (delinq_2yrs_clean == 0)
            mask_low = (delinq_2yrs_clean >= 1) & (delinq_2yrs_clean < 3)
            mask_medium = (delinq_2yrs_clean >= 3) & (delinq_2yrs_clean < 5)
            mask_high = (delinq_2yrs_clean >= 5)
            
            X.loc[mask_none, 'delinquency_severity'] = 'None'
            X.loc[mask_low, 'delinquency_severity'] = 'Low'
            X.loc[mask_medium, 'delinquency_severity'] = 'Medium'
            X.loc[mask_high, 'delinquency_severity'] = 'High'
            
            # ê²€ì¦: ê²°ì¸¡ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸
            missing_count = X['delinquency_severity'].isnull().sum()
            if missing_count > 0:
                print(f"  âš ï¸ delinquency_severityì— {missing_count}ê°œ ê²°ì¸¡ì¹˜ ë°œê²¬, 'None'ìœ¼ë¡œ ì²˜ë¦¬")
                X['delinquency_severity'] = X['delinquency_severity'].fillna('None')
            
            # ìµœì¢… ê²€ì¦
            final_dist = X['delinquency_severity'].value_counts()
            print(f"  ğŸ“Š delinquency_severity ìµœì¢… ë¶„í¬: {final_dist.to_dict()}")
        
        # ì‹ ìš© ì´ìš©ë¥  ìœ„í—˜ë„ ì •êµí™”
        if 'revol_util' in X.columns:
            X['credit_utilization_risk'] = pd.cut(
                X['revol_util'],
                bins=[0, 30, 60, 80, 100],
                labels=['Low', 'Medium', 'High', 'Very High']
            )
        
        # ê³„ì¢Œ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
        if 'total_acc' in X.columns:
            if 'open_acc' in X.columns:
                X['account_diversity_ratio'] = X['open_acc'] / (X['total_acc'] + 1e-8)
            else:
                X['account_diversity_ratio'] = 0.5
            
            X['account_diversity_score'] = pd.cut(
                X['account_diversity_ratio'],
                bins=[0, 0.3, 0.6, 0.8, 1.0],
                labels=['Low', 'Medium', 'High', 'Very High']
            )
        
        print(f"âœ“ ë³µí•© ì§€í‘œ: 8ê°œ íŠ¹ì„± ìƒì„±")
        return X

class FinancialFeatureCreator(BaseEstimator, TransformerMixin):
    """ê¸ˆìœµ ëª¨ë¸ë§ íŠ¹ì„± ìƒì„±"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ëŒ€ì¶œ ì¡°ê±´ ê´€ë ¨ íŠ¹ì„±
        if 'loan_amnt' in X.columns and 'annual_inc' in X.columns:
            X['loan_to_income_ratio'] = X['loan_amnt'] / (X['annual_inc'] + 1e-8)
        
        if 'installment' in X.columns and 'annual_inc' in X.columns:
            X['monthly_payment_ratio'] = (X['installment'] * 12) / (X['annual_inc'] + 1e-8)
        
        # ì‹ ìš© ë“±ê¸‰ë³„ ìœ„í—˜ë„ ì ìˆ˜
        if 'grade' in X.columns:
            grade_risk_mapping = {
                'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7
            }
            X['grade_risk_score'] = X['grade'].map(grade_risk_mapping).fillna(4)
        
        # ëŒ€ì¶œ ê¸°ê°„ë³„ ìœ„í—˜ë„
        if 'term' in X.columns:
            X['term_months'] = X['term'].str.extract(r'(\d+)').astype(float)
            X['term_risk_score'] = pd.cut(
                X['term_months'],
                bins=[0, 36, 60, float('inf')],
                labels=['Low', 'Medium', 'High']
            )
        
        # ì˜ˆìƒ ìˆ˜ìµë¥ 
        if 'int_rate' in X.columns and 'grade_risk_score' in X.columns:
            X['expected_return_rate'] = X['int_rate'] - (X['grade_risk_score'] * 0.5)
        
        # ìœ„í—˜ì¡°ì •ìˆ˜ìµë¥ 
        if 'expected_return_rate' in X.columns and 'grade_risk_score' in X.columns:
            X['risk_adjusted_return'] = X['expected_return_rate'] / (X['grade_risk_score'] + 1e-8)
        
        print(f"âœ“ ê¸ˆìœµ íŠ¹ì„±: 7ê°œ íŠ¹ì„± ìƒì„±")
        return X

class FeatureSelector(BaseEstimator, TransformerMixin):
    """íŠ¹ì„± ì„ íƒ"""
    
    def __init__(self, correlation_threshold=0.95):
        self.correlation_threshold = correlation_threshold
        self.financial_critical_features = [
            'term', 'int_rate', 'loan_amnt', 'funded_amnt', 'installment', 'total_pymnt',
            'grade', 'sub_grade', 'annual_inc', 'dti', 'revol_util', 'purpose', 'verification_status',
            'delinq_2yrs', 'earliest_cr_line', 'fico_range_low', 'fico_range_high',
            'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'total_acc',
            'bc_open_to_buy', 'bc_util', 'percent_bc_gt_75', 'num_actv_bc_tl', 'num_actv_rev_tl',
            'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_tl_bal_gt_0',
            'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'tot_cur_bal', 'avg_cur_bal',
            'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',
            'collections_12_mths_ex_med', 'acc_now_delinq', 'pub_rec_bankruptcies', 'tax_liens',
            'chargeoff_within_12_mths', 'loan_to_income_ratio', 'monthly_payment_ratio',
            'grade_risk_score', 'term_risk_score', 'expected_return_rate', 'risk_adjusted_return'
        ]
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # í›„í–‰ì§€í‘œ ë³€ìˆ˜ ì œê±°
        posterior_variables = [
            'total_pymnt_inv', 'total_rec_int', 'total_rec_prncp', 'total_rec_late_fee',
            'recoveries', 'collection_recovery_fee', 'last_pymnt_amnt', 'next_pymnt_d',
            'last_fico_range_high', 'last_fico_range_low'
        ]
        
        for var in posterior_variables:
            if var in X.columns and var not in self.financial_critical_features:
                X = X.drop(columns=[var])
        
        # ìƒê´€ê´€ê³„ ë¶„ì„
        numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        if len(numerical_cols) > 1:
            corr_matrix = X[numerical_cols].corr().abs()
            upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
            high_corr_pairs = [(col1, col2) for col1, col2 in zip(upper_tri.index, upper_tri.columns) 
                             if upper_tri.loc[col1, col2] > self.correlation_threshold]
            
            removed_vars = set()
            for var1, var2 in high_corr_pairs:
                if (var1 not in removed_vars and var2 not in removed_vars and 
                    var1 not in self.financial_critical_features and var2 not in self.financial_critical_features):
                    var_to_remove = var1 if X[var1].nunique() < X[var2].nunique() else var2
                    X = X.drop(columns=[var_to_remove])
                    removed_vars.add(var_to_remove)
        
        print(f"âœ“ íŠ¹ì„± ì„ íƒ: {len(X.columns)}ê°œ íŠ¹ì„± ìœ ì§€")
        return X

class UnnecessaryFeatureRemover(BaseEstimator, TransformerMixin):
    """ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±° (PRD ì„¹ì…˜ 3.1ì˜ 9ë²ˆ í•­ëª©)"""
    
    def __init__(self):
        # PRDì—ì„œ ëª…ì‹œëœ ì‚¬í›„ ì •ë³´(Post-Origination / Leakage Variables)
        self.posterior_variables = [
            'last_pymnt_d', 'last_pymnt_amnt', 'total_pymnt', 'total_pymnt_inv',
            'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries',
            'collection_recovery_fee', 'out_prncp', 'out_prncp_inv', 'next_pymnt_d',
            'last_credit_pull_d', 'last_fico_range_high', 'last_fico_range_low'
        ]
        
        # ì •ì±…/ë©”íƒ€ë°ì´í„°(ëª¨ë¸ ëª©í‘œì™€ ë¬´ê´€)
        self.policy_metadata = [
            'url', 'policy_code', 'collection_recovery_fee', 'application_type'
        ]
        
        # í¬ê·€/ì¤‘ë³µ ë³€ìˆ˜
        self.rare_duplicate_variables = [
            'tax_liens', 'pub_rec_bankruptcies', 'chargeoff_within_12_mths',
            'sec_app_chargeoff_within_12_mths', 'mths_since_recent_bc_dlq',
            'mths_since_recent_revol_delinq', 'orig_projected_additional_accrued_interest'
        ]
        
        # ê¸°íƒ€ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ë“¤
        self.other_unnecessary = [
            'id', 'title', 'zip_code', 'emp_title', 'desc', 'verification_status_joint',
            'hardship_reason', 'hardship_type', 'hardship_status', 'issue_d', 'earliest_cr_line',
            'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date',
            'fico_range_high', 'sec_app_fico_range_low', 'sec_app_fico_range_high',
            'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths', 'sec_app_mort_acc',
            'sec_app_open_acc', 'sec_app_revol_util', 'sec_app_open_act_il',
            'sec_app_num_rev_accts', 'sec_app_collections_12_mths_ex_med',
            'deferral_term', 'hardship_amount', 'hardship_length', 'hardship_dpd',
            'hardship_loan_status', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount'
        ]
        
        # ëª¨ë“  ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ í†µí•©
        self.unnecessary_features = (
            self.posterior_variables + 
            self.policy_metadata + 
            self.rare_duplicate_variables + 
            self.other_unnecessary
        )
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        existing_unnecessary = [feature for feature in self.unnecessary_features if feature in X.columns]
        
        if existing_unnecessary:
            X = X.drop(columns=existing_unnecessary)
            print(f"âœ“ ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±°: {len(existing_unnecessary)}ê°œ")
        
        return X

class FinalMissingValueHandler(BaseEstimator, TransformerMixin):
    """ìµœì¢… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ëª¨ë“  ë‚¨ì€ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ëª¨ë“  ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ ì°¾ê¸°
        missing_columns = X.columns[X.isnull().any()].tolist()
        
        if missing_columns:
            print(f"  ğŸ”§ ìµœì¢… ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {len(missing_columns)}ê°œ ë³€ìˆ˜")
            print(f"  ğŸ“‹ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜ë“¤: {missing_columns}")
            
            for col in missing_columns:
                if col in X.columns:
                    missing_count = X[col].isnull().sum()
                    print(f"    ğŸ” {col}: {missing_count}ê°œ ê²°ì¸¡ì¹˜ ë°œê²¬")
                    
                    if X[col].dtype in ['object', 'category']:
                        # ë²”ì£¼í˜•: ìµœë¹ˆê°’ ë˜ëŠ” 'Unknown'
                        mode_value = X[col].mode().iloc[0] if not X[col].mode().empty else 'Unknown'
                        X[col] = X[col].fillna(mode_value)
                        print(f"    âœ“ {col}: modeë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
                    else:
                        # ìˆ˜ì¹˜í˜•: ì¤‘ì•™ê°’
                        median_value = X[col].median()
                        X[col] = X[col].fillna(median_value)
                        print(f"    âœ“ {col}: medianìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        
        # delinquency_severity íŠ¹ë³„ ì²˜ë¦¬
        if 'delinq_2yrs' in X.columns:
            print(f"  ğŸ”§ delinquency_severity ì¬ìƒì„±")
            delinq_2yrs_clean = X['delinq_2yrs'].fillna(0).astype(int)
            
            # ê°•ì œë¡œ ì¬ìƒì„±
            X['delinquency_severity'] = 'None'  # ê¸°ë³¸ê°’
            
            # ì¡°ê±´ë³„ í• ë‹¹
            mask_none = (delinq_2yrs_clean == 0)
            mask_low = (delinq_2yrs_clean >= 1) & (delinq_2yrs_clean < 3)
            mask_medium = (delinq_2yrs_clean >= 3) & (delinq_2yrs_clean < 5)
            mask_high = (delinq_2yrs_clean >= 5)
            
            X.loc[mask_none, 'delinquency_severity'] = 'None'
            X.loc[mask_low, 'delinquency_severity'] = 'Low'
            X.loc[mask_medium, 'delinquency_severity'] = 'Medium'
            X.loc[mask_high, 'delinquency_severity'] = 'High'
            
            # ìµœì¢… ê²€ì¦
            final_dist = X['delinquency_severity'].value_counts()
            missing_count = X['delinquency_severity'].isnull().sum()
            print(f"    âœ“ delinquency_severity ì¬ìƒì„± ì™„ë£Œ: {final_dist.to_dict()}")
            if missing_count > 0:
                print(f"    âš ï¸ delinquency_severityì— {missing_count}ê°œ ê²°ì¸¡ì¹˜, 'None'ìœ¼ë¡œ ì²˜ë¦¬")
                X['delinquency_severity'] = X['delinquency_severity'].fillna('None')
        
        # ìµœì¢… ê²€ì¦
        final_missing = X.columns[X.isnull().any()].tolist()
        if final_missing:
            print(f"  âš ï¸ ìµœì¢… ê²€ì¦: {len(final_missing)}ê°œ ë³€ìˆ˜ì— ì—¬ì „íˆ ê²°ì¸¡ì¹˜ ì¡´ì¬")
            for col in final_missing:
                print(f"    - {col}: {X[col].isnull().sum()}ê°œ")
        else:
            print(f"  âœ… ìµœì¢… ê²€ì¦: ëª¨ë“  ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        
        return X

class Scaler(BaseEstimator, TransformerMixin):
    """ìŠ¤ì¼€ì¼ë§ ë° ì •ê·œí™”"""
    
    def __init__(self, method='standard'):
        self.method = method  # 'standard', 'minmax', 'robust'
        self.scalers = {}
        
    def fit(self, X, y=None):
        numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        
        # target ë³€ìˆ˜ëŠ” ìŠ¤ì¼€ì¼ë§ì—ì„œ ì œì™¸
        if 'target' in numerical_cols:
            numerical_cols.remove('target')
        
        for col in numerical_cols:
            if col in X.columns:
                if self.method == 'standard':
                    # í‘œì¤€í™”: (x-Î¼)/Ïƒ
                    mean_val = X[col].mean()
                    std_val = X[col].std()
                    self.scalers[col] = ('standard', mean_val, std_val)
                elif self.method == 'minmax':
                    # ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”: (x-min)/(max-min)
                    min_val = X[col].min()
                    max_val = X[col].max()
                    self.scalers[col] = ('minmax', min_val, max_val)
                elif self.method == 'robust':
                    # Robust Scaler: (x-median)/IQR
                    median_val = X[col].median()
                    Q1 = X[col].quantile(0.25)
                    Q3 = X[col].quantile(0.75)
                    IQR = Q3 - Q1
                    self.scalers[col] = ('robust', median_val, IQR)
        
        return self
        
    def transform(self, X):
        for col, (method, param1, param2) in self.scalers.items():
            if col in X.columns:
                if method == 'standard':
                    X[col] = (X[col] - param1) / param2
                elif method == 'minmax':
                    X[col] = (X[col] - param1) / (param2 - param1)
                elif method == 'robust':
                    X[col] = (X[col] - param1) / param2
        
        print(f"âœ“ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: {self.method} ë°©ì‹")
        return X

class OneHotEncoder(BaseEstimator, TransformerMixin):
    """One-Hot ì¸ì½”ë”©"""
    
    def __init__(self, max_categories=10):
        self.max_categories = max_categories
        self.categorical_cols = []
        
    def fit(self, X, y=None):
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¤‘ ì¹´í…Œê³ ë¦¬ê°€ ì ì€ ê²ƒë§Œ ì„ íƒ
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        for col in categorical_cols:
            if col in X.columns:
                unique_count = X[col].nunique()
                if unique_count <= self.max_categories:
                    self.categorical_cols.append(col)
        
        return self
        
    def transform(self, X):
        for col in self.categorical_cols:
            if col in X.columns:
                # One-Hot ì¸ì½”ë”©
                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)
                X = pd.concat([X, dummies], axis=1)
                X = X.drop(columns=[col])  # ì›ë³¸ ì»¬ëŸ¼ ì œê±°
        
        print(f"âœ“ One-Hot ì¸ì½”ë”© ì™„ë£Œ: {len(self.categorical_cols)}ê°œ ë³€ìˆ˜")
        return X

class DataSplitter(BaseEstimator, TransformerMixin):
    """ë°ì´í„° ë¶„í• """
    
    def __init__(self, test_size=0.2, val_size=0.2, random_state=42):
        self.test_size = test_size
        self.val_size = val_size
        self.random_state = random_state
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ë°ì´í„° ë¶„í•  (ì‹¤ì œë¡œëŠ” ë³„ë„ í•¨ìˆ˜ë¡œ ì²˜ë¦¬)
        print(f"âœ“ ë°ì´í„° ë¶„í•  ì¤€ë¹„: Train/Val/Test = {1-self.test_size-self.val_size:.1f}/{self.val_size:.1f}/{self.test_size:.1f}")
        return X

class ImbalanceHandler(BaseEstimator, TransformerMixin):
    """ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬"""
    
    def __init__(self, method='class_weight'):
        self.method = method  # 'class_weight', 'smote', 'undersample'
        
    def fit(self, X, y=None):
        if 'target' in X.columns:
            # target ë³€ìˆ˜ë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
            target_series = X['target'].astype(int)
            target_dist = target_series.value_counts()
            print(f"  ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬: {target_dist.to_dict()}")
            
            if self.method == 'class_weight':
                # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
                total = len(X)
                self.class_weights = {
                    0: total / (2 * target_dist[0]),
                    1: total / (2 * target_dist[1])
                }
                print(f"  âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {self.class_weights}")
        
        return self
        
    def transform(self, X):
        print(f"âœ“ ë¶ˆê· í˜• ì²˜ë¦¬ ì¤€ë¹„: {self.method} ë°©ì‹")
        return X

class TargetEncoder(BaseEstimator, TransformerMixin):
    """Target Encoding - ë²”ì£¼ë³„ íƒ€ê²Ÿ ë³€ìˆ˜ í‰ê· """
    
    def __init__(self, smoothing=1.0):
        self.smoothing = smoothing
        self.target_means = {}
        
    def fit(self, X, y=None):
        if 'target' in X.columns:
            categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
            
            for col in categorical_cols:
                if col in X.columns and col != 'target':
                    # ê° ë²”ì£¼ë³„ íƒ€ê²Ÿ í‰ê·  ê³„ì‚°
                    target_means = X.groupby(col)['target'].mean()
                    global_mean = X['target'].mean()
                    
                    # ìŠ¤ë¬´ë”© ì ìš©
                    smoothed_means = {}
                    for category in target_means.index:
                        count = (X[col] == category).sum()
                        smoothed_mean = (count * target_means[category] + self.smoothing * global_mean) / (count + self.smoothing)
                        smoothed_means[category] = smoothed_mean
                    
                    self.target_means[col] = smoothed_means
        
        return self
        
    def transform(self, X):
        for col, means in self.target_means.items():
            if col in X.columns:
                # Categorical ë³€ìˆ˜ ì²˜ë¦¬
                if X[col].dtype.name == 'category':
                    # Categoricalì„ objectë¡œ ë³€í™˜
                    X[col] = X[col].astype(str)
                
                # Target encoding ì ìš©
                encoded_values = X[col].map(means)
                global_mean = X['target'].mean() if 'target' in X.columns else 0.125
                X[f'{col}_target_encoded'] = encoded_values.fillna(global_mean)
        
        print(f"âœ“ Target Encoding ì™„ë£Œ: {len(self.target_means)}ê°œ ë³€ìˆ˜")
        return X

class SMOTEHandler(BaseEstimator, TransformerMixin):
    """SMOTE ì˜¤ë²„ìƒ˜í”Œë§"""
    
    def __init__(self, k_neighbors=5, random_state=42):
        self.k_neighbors = k_neighbors
        self.random_state = random_state
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        if 'target' in X.columns:
            try:
                from imblearn.over_sampling import SMOTE
                
                # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
                y = X['target']
                X_features = X.drop(['target'], axis=1)
                
                # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ
                numeric_cols = X_features.select_dtypes(include=[np.number]).columns.tolist()
                X_numeric = X_features[numeric_cols]
                
                # SMOTE ì ìš©
                smote = SMOTE(k_neighbors=self.k_neighbors, random_state=self.random_state)
                X_resampled, y_resampled = smote.fit_resample(X_numeric, y)
                
                # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
                X_resampled_df = pd.DataFrame(X_resampled, columns=numeric_cols)
                X_resampled_df['target'] = y_resampled
                
                print(f"âœ“ SMOTE ì™„ë£Œ: {len(X)} â†’ {len(X_resampled_df)}ê°œ ìƒ˜í”Œ")
                return X_resampled_df
                
            except ImportError:
                print("âš ï¸ imbalanced-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ SMOTEë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return X
        
        return X

class CreditHistoryFeatureCreator(BaseEstimator, TransformerMixin):
    """ì‹ ìš© ì´ë ¥ ì§€í‘œ íŠ¹ì„± ìƒì„± (PRD ì„¹ì…˜ 3.1ì˜ 2ë²ˆ í•­ëª©)"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ì‹ ìš© ì´ë ¥ ê´€ë ¨ ë³€ìˆ˜ë“¤ ì²˜ë¦¬
        credit_history_features = [
            'mths_since_last_record', 'mths_since_recent_bc_dlq',
            'mths_since_last_major_derog', 'mths_since_recent_revol_delinq',
            'mths_since_last_delinq', 'mths_since_rcnt_il'
        ]
        
        for col in credit_history_features:
            if col in X.columns:
                # ê²°ì¸¡ì¹˜ë¥¼ í° ê°’ìœ¼ë¡œ ì²˜ë¦¬ (ìµœê·¼ì— ê¸°ë¡ì´ ì—†ìŒì„ ì˜ë¯¸)
                X[col] = X[col].fillna(999)
                # ë¡œê·¸ ë³€í™˜ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •
                X[f'{col}_log'] = np.log1p(X[col])
        
        print(f"âœ“ ì‹ ìš© ì´ë ¥ íŠ¹ì„±: {len([col for col in credit_history_features if col in X.columns])}ê°œ ë³€ìˆ˜ ì²˜ë¦¬")
        return X

class AccountActivityFeatureCreator(BaseEstimator, TransformerMixin):
    """ê³„ì • í™œë™ ì§€í‘œ íŠ¹ì„± ìƒì„± (PRD ì„¹ì…˜ 3.1ì˜ 3ë²ˆ í•­ëª©)"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ê³„ì • í™œë™ ê´€ë ¨ ë³€ìˆ˜ë“¤
        account_activity_features = [
            'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m',
            'open_rv_12m', 'open_rv_24m', 'total_cu_tl', 'num_tl_30dpd',
            'num_tl_120dpd_2m'
        ]
        
        for col in account_activity_features:
            if col in X.columns:
                # ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬
                X[col] = X[col].fillna(0)
        
        # ê³„ì • ë‹¤ì–‘ì„± ë¹„ìœ¨ ê³„ì‚°
        if 'open_acc' in X.columns and 'total_acc' in X.columns:
            X['account_activity_ratio'] = X['open_acc'] / (X['total_acc'] + 1e-8)
        
        print(f"âœ“ ê³„ì • í™œë™ íŠ¹ì„±: {len([col for col in account_activity_features if col in X.columns])}ê°œ ë³€ìˆ˜ ì²˜ë¦¬")
        return X

class DebtRatioFeatureCreator(BaseEstimator, TransformerMixin):
    """ë¶€ì±„ ë¹„ìœ¨ ë° ìƒí™˜ ì§€í‘œ íŠ¹ì„± ìƒì„± (PRD ì„¹ì…˜ 3.1ì˜ 4ë²ˆ í•­ëª©)"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ë¶€ì±„ ë¹„ìœ¨ ê´€ë ¨ ë³€ìˆ˜ë“¤
        debt_ratio_features = [
            'il_util', 'all_util', 'bc_util', 'percent_bc_gt_75',
            'tot_bal_il', 'max_bal_bc', 'bc_open_to_buy', 'mths_since_recent_bc'
        ]
        
        for col in debt_ratio_features:
            if col in X.columns:
                # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                if col in ['il_util', 'all_util', 'bc_util']:
                    X[col] = X[col].fillna(0)  # ì´ìš©ë¥ ì€ 0ìœ¼ë¡œ
                else:
                    X[col] = X[col].fillna(X[col].median())
        
        # ë¶€ì±„ ë¹„ìœ¨ ë³µí•© ì§€í‘œ
        if 'dti' in X.columns and 'annual_inc' in X.columns:
            X['debt_income_ratio'] = X['dti'] / 100  # í¼ì„¼íŠ¸ë¥¼ ì†Œìˆ˜ë¡œ ë³€í™˜
        
        print(f"âœ“ ë¶€ì±„ ë¹„ìœ¨ íŠ¹ì„±: {len([col for col in debt_ratio_features if col in X.columns])}ê°œ ë³€ìˆ˜ ì²˜ë¦¬")
        return X

class HardshipFeatureCreator(BaseEstimator, TransformerMixin):
    """í•˜ë“œì‹­ í”Œëœ ì§€í‘œ íŠ¹ì„± ìƒì„± (PRD ì„¹ì…˜ 3.1ì˜ 6ë²ˆ í•­ëª©)"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # í•˜ë“œì‹­ ê´€ë ¨ ë³€ìˆ˜ë“¤
        hardship_features = [
            'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status',
            'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date',
            'hardship_length', 'deferral_term', 'hardship_dpd', 'hardship_loan_status',
            'hardship_amount', 'orig_projected_additional_accrued_interest',
            'hardship_payoff_balance_amount', 'hardship_last_payment_amount'
        ]
        
        hardship_processed = 0
        for col in hardship_features:
            if col in X.columns:
                if col in ['hardship_flag']:
                    # ë¶ˆë¦° ë³€ìˆ˜ë¡œ ë³€í™˜
                    X[col] = (X[col] == 'Y').astype(int)
                elif col in ['hardship_type', 'hardship_reason', 'hardship_status']:
                    # ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
                    X[col] = X[col].fillna('None')
                else:
                    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì²˜ë¦¬
                    X[col] = X[col].fillna(0)
                hardship_processed += 1
        
        # í•˜ë“œì‹­ ë³µí•© ì§€í‘œ
        if 'hardship_flag' in X.columns:
            X['has_hardship'] = X['hardship_flag']
        
        print(f"âœ“ í•˜ë“œì‹­ íŠ¹ì„±: {hardship_processed}ê°œ ë³€ìˆ˜ ì²˜ë¦¬")
        return X

class SecondaryApplicantFeatureCreator(BaseEstimator, TransformerMixin):
    """ì„¸ì»¨ë”ë¦¬ ì‹ ì²­ì¸ ì •ë³´ íŠ¹ì„± ìƒì„± (PRD ì„¹ì…˜ 3.1ì˜ 7ë²ˆ í•­ëª©)"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ì„¸ì»¨ë”ë¦¬ ì‹ ì²­ì¸ ê´€ë ¨ ë³€ìˆ˜ë“¤
        secondary_features = [
            'application_type', 'annual_inc_joint', 'dti_joint', 'verification_status_joint',
            'revol_bal_joint', 'sec_app_earliest_cr_line', 'sec_app_inq_last_6mths',
            'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util',
            'sec_app_open_act_il', 'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths',
            'sec_app_collections_12_mths_ex_med'
        ]
        
        secondary_processed = 0
        for col in secondary_features:
            if col in X.columns:
                if col == 'application_type':
                    # ê°œë³„/ê³µë™ ì‹ ì²­ êµ¬ë¶„
                    X['is_joint_application'] = (X[col] == 'Joint App').astype(int)
                elif col in ['annual_inc_joint', 'dti_joint', 'revol_bal_joint']:
                    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì²˜ë¦¬
                    X[col] = X[col].fillna(0)
                else:
                    # ê¸°íƒ€ ë³€ìˆ˜ë“¤ ì²˜ë¦¬
                    X[col] = X[col].fillna(0)
                secondary_processed += 1
        
        print(f"âœ“ ì„¸ì»¨ë”ë¦¬ ì‹ ì²­ì¸ íŠ¹ì„±: {secondary_processed}ê°œ ë³€ìˆ˜ ì²˜ë¦¬")
        return X

class LogTransformationFeatureCreator(BaseEstimator, TransformerMixin):
    """ë¡œê·¸ ë³€í™˜ íŠ¹ì„± ìƒì„± (PRD ì„¹ì…˜ 3.2ì˜ 3ë²ˆ í•­ëª©)"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ë¡œê·¸ ë³€í™˜ì´ í•„ìš”í•œ ë³€ìˆ˜ë“¤
        log_transform_features = [
            'revol_bal', 'annual_inc', 'loan_amnt', 'funded_amnt',
            'total_bal_ex_mort', 'tot_bal_il', 'max_bal_bc'
        ]
        
        for col in log_transform_features:
            if col in X.columns:
                # ìŒìˆ˜ë‚˜ 0ê°’ ì²˜ë¦¬ í›„ ë¡œê·¸ ë³€í™˜
                X[f'{col}_log'] = np.log1p(X[col].clip(lower=0))
        
        print(f"âœ“ ë¡œê·¸ ë³€í™˜ íŠ¹ì„±: {len([col for col in log_transform_features if col in X.columns])}ê°œ ë³€ìˆ˜")
        return X

class InteractionFeatureCreator(BaseEstimator, TransformerMixin):
    """ìƒí˜¸ì‘ìš© ë³€ìˆ˜ ìƒì„± (PRD ì„¹ì…˜ 3.2ì˜ 5ë²ˆ í•­ëª©)"""
    
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # ìƒí˜¸ì‘ìš© ë³€ìˆ˜ë“¤ ìƒì„±
        
        # ëŒ€ì¶œ ì¡°ê±´ ìƒí˜¸ì‘ìš©
        if 'int_rate' in X.columns and 'term' in X.columns:
            X['rate_term_interaction'] = X['int_rate'] * X['term'].str.extract(r'(\d+)').astype(float)
        
        # ì†Œë“ ëŒ€ë¹„ ëŒ€ì¶œ ë¹„ìœ¨
        if 'loan_amnt' in X.columns and 'annual_inc' in X.columns:
            X['loan_to_income_ratio'] = X['loan_amnt'] / (X['annual_inc'] + 1e-8)
        
        # ì‹ ìš© ì ìˆ˜ì™€ ì†Œë“ ìƒí˜¸ì‘ìš©
        if 'fico_avg' in X.columns and 'annual_inc' in X.columns:
            X['fico_income_interaction'] = X['fico_avg'] * np.log1p(X['annual_inc'])
        
        # ì—°ì²´ ì´ë ¥ê³¼ ì‹ ìš© ì ìˆ˜ ìƒí˜¸ì‘ìš©
        if 'delinq_2yrs' in X.columns and 'fico_avg' in X.columns:
            X['delinq_fico_interaction'] = X['delinq_2yrs'].fillna(0) * X['fico_avg']
        
        print(f"âœ“ ìƒí˜¸ì‘ìš© íŠ¹ì„±: 4ê°œ ë³€ìˆ˜ ìƒì„±")
        return X

class AdvancedFeatureSelector(BaseEstimator, TransformerMixin):
    """ê³ ê¸‰ íŠ¹ì„± ì„ íƒ (PRD ì„¹ì…˜ 4ì˜ 1ë²ˆ í•­ëª©)"""
    
    def __init__(self, correlation_threshold=0.95, mi_threshold=0.01):
        self.correlation_threshold = correlation_threshold
        self.mi_threshold = mi_threshold
        self.selected_features = []
        
    def fit(self, X, y=None):
        if 'target' in X.columns:
            y = X['target']
            X_features = X.drop(['target'], axis=1)
            
            # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ
            numeric_cols = X_features.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) > 1:
                # ìƒê´€ê´€ê³„ ë¶„ì„
                corr_matrix = X_features[numeric_cols].corr().abs()
                upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
                high_corr_pairs = [(col1, col2) for col1, col2 in zip(upper_tri.index, upper_tri.columns) 
                                 if upper_tri.loc[col1, col2] > self.correlation_threshold]
                
                # ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ ì œê±°
                removed_vars = set()
                for var1, var2 in high_corr_pairs:
                    if var1 not in removed_vars and var2 not in removed_vars:
                        var_to_remove = var1 if X_features[var1].nunique() < X_features[var2].nunique() else var2
                        removed_vars.add(var_to_remove)
                
                # ìƒí˜¸ ì •ë³´ëŸ‰ ë¶„ì„ (ì„ íƒì )
                try:
                    from sklearn.feature_selection import mutual_info_classif
                    mi_scores = mutual_info_classif(X_features[numeric_cols], y)
                    mi_series = pd.Series(mi_scores, index=numeric_cols)
                    low_mi_features = mi_series[mi_series < self.mi_threshold].index.tolist()
                    removed_vars.update(low_mi_features)
                except ImportError:
                    print("âš ï¸ scikit-learn ë²„ì „ìœ¼ë¡œ ì¸í•´ ìƒí˜¸ ì •ë³´ëŸ‰ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                
                # ìµœì¢… ì„ íƒëœ íŠ¹ì„±ë“¤
                self.selected_features = [col for col in numeric_cols if col not in removed_vars]
                
        return self
        
    def transform(self, X):
        if self.selected_features:
            # ì„ íƒëœ íŠ¹ì„±ë“¤ë§Œ ìœ ì§€
            available_features = [col for col in self.selected_features if col in X.columns]
            X = X[available_features + ['target'] if 'target' in X.columns else available_features]
            print(f"âœ“ ê³ ê¸‰ íŠ¹ì„± ì„ íƒ: {len(available_features)}ê°œ íŠ¹ì„± ìœ ì§€")
        
        return X

class EDAAnalyzer(BaseEstimator, TransformerMixin):
    """EDA ë¶„ì„ ë° ì‹œê°í™” (PRD ì„¹ì…˜ 3.2 ë° 4.1)"""
    
    def __init__(self, output_dir=None):
        self.output_dir = output_dir or REPORTS_DIR
        os.makedirs(self.output_dir, exist_ok=True)
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        print("ğŸ” EDA ë¶„ì„ ì‹œì‘...")
        
        # 1. ê¸°ë³¸ í†µê³„ ì •ë³´
        self._basic_statistics(X)
        
        # 2. ê²°ì¸¡ì¹˜ ë¶„ì„
        self._missing_value_analysis(X)
        
        # 3. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„
        if 'target' in X.columns:
            self._target_distribution_analysis(X)
        
        # 4. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„
        self._numerical_distribution_analysis(X)
        
        # 5. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„
        self._categorical_analysis(X)
        
        # 6. ìƒê´€ê´€ê³„ ë¶„ì„
        self._correlation_analysis(X)
        
        print("âœ“ EDA ë¶„ì„ ì™„ë£Œ")
        return X
    
    def _basic_statistics(self, X):
        """ê¸°ë³¸ í†µê³„ ì •ë³´"""
        stats_info = {
            'ë°ì´í„° í¬ê¸°': X.shape,
            'ë³€ìˆ˜ ìˆ˜': len(X.columns),
            'ìˆ˜ì¹˜í˜• ë³€ìˆ˜': len(X.select_dtypes(include=[np.number]).columns),
            'ë²”ì£¼í˜• ë³€ìˆ˜': len(X.select_dtypes(include=['object', 'category']).columns),
            'ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜': X.columns[X.isnull().any()].tolist()
        }
        
        # í†µê³„ ì •ë³´ ì €ì¥
        stats_path = os.path.join(self.output_dir, 'eda_basic_statistics.txt')
        with open(stats_path, 'w', encoding='utf-8') as f:
            for key, value in stats_info.items():
                f.write(f"{key}: {value}\n")
        
        print(f"  ğŸ“Š ê¸°ë³¸ í†µê³„ ì €ì¥: {stats_path}")
    
    def _missing_value_analysis(self, X):
        """ê²°ì¸¡ì¹˜ ë¶„ì„"""
        missing_info = X.isnull().sum()
        missing_percent = (missing_info / len(X)) * 100
        
        missing_df = pd.DataFrame({
            'ê²°ì¸¡ì¹˜ ê°œìˆ˜': missing_info,
            'ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%)': missing_percent
        }).sort_values('ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%)', ascending=False)
        
        # ê²°ì¸¡ì¹˜ ì •ë³´ ì €ì¥
        missing_path = os.path.join(self.output_dir, 'missing_value_analysis.csv')
        missing_df.to_csv(missing_path)
        
        print(f"  ğŸ“Š ê²°ì¸¡ì¹˜ ë¶„ì„ ì €ì¥: {missing_path}")
    
    def _target_distribution_analysis(self, X):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„"""
        target_dist = X['target'].value_counts()
        target_percent = (target_dist / len(X)) * 100
        
        target_info = pd.DataFrame({
            'ê°œìˆ˜': target_dist,
            'ë¹„ìœ¨(%)': target_percent
        })
        
        # íƒ€ê²Ÿ ë¶„í¬ ì •ë³´ ì €ì¥
        target_path = os.path.join(self.output_dir, 'target_distribution_analysis.csv')
        target_info.to_csv(target_path)
        
        print(f"  ğŸ“Š íƒ€ê²Ÿ ë¶„í¬ ë¶„ì„ ì €ì¥: {target_path}")
        print(f"    - ë¶€ë„ìœ¨: {target_percent[1]:.2f}%")
    
    def _numerical_distribution_analysis(self, X):
        """ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„"""
        numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        
        if 'target' in numerical_cols:
            numerical_cols.remove('target')
        
        if numerical_cols:
            # ì£¼ìš” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì˜ í†µê³„
            stats_df = X[numerical_cols].describe()
            
            # í†µê³„ ì •ë³´ ì €ì¥
            stats_path = os.path.join(self.output_dir, 'numerical_statistics.csv')
            stats_df.to_csv(stats_path)
            
            print(f"  ğŸ“Š ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„ ì €ì¥: {stats_path}")
    
    def _categorical_analysis(self, X):
        """ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„"""
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        if categorical_cols:
            cat_info = {}
            for col in categorical_cols:
                if col in X.columns:
                    value_counts = X[col].value_counts()
                    cat_info[col] = {
                        'ê³ ìœ ê°’ ê°œìˆ˜': len(value_counts),
                        'ìµœë¹ˆê°’': value_counts.index[0] if len(value_counts) > 0 else None,
                        'ìµœë¹ˆê°’ ë¹„ìœ¨(%)': (value_counts.iloc[0] / len(X)) * 100 if len(value_counts) > 0 else 0
                    }
            
            cat_df = pd.DataFrame(cat_info).T
            cat_path = os.path.join(self.output_dir, 'categorical_analysis.csv')
            cat_df.to_csv(cat_path)
            
            print(f"  ğŸ“Š ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„ ì €ì¥: {cat_path}")
    
    def _correlation_analysis(self, X):
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        
        if 'target' in numerical_cols:
            numerical_cols.remove('target')
        
        if len(numerical_cols) > 1:
            # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
            corr_matrix = X[numerical_cols].corr()
            
            # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ (íƒ€ê²Ÿì´ ìˆëŠ” ê²½ìš°)
            if 'target' in X.columns:
                target_corr = X[numerical_cols + ['target']].corr()['target'].abs().sort_values(ascending=False)
                target_corr_path = os.path.join(self.output_dir, 'target_correlation.csv')
                target_corr.to_csv(target_corr_path)
                print(f"  ğŸ“Š íƒ€ê²Ÿ ìƒê´€ê´€ê³„ ì €ì¥: {target_corr_path}")
            
            # ì „ì²´ ìƒê´€ê´€ê³„ í–‰ë ¬ ì €ì¥
            corr_path = os.path.join(self.output_dir, 'correlation_matrix.csv')
            corr_matrix.to_csv(corr_path)
            print(f"  ğŸ“Š ìƒê´€ê´€ê³„ í–‰ë ¬ ì €ì¥: {corr_path}")

class DataQualityReporter(BaseEstimator, TransformerMixin):
    """ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
    
    def __init__(self, output_dir=None):
        self.output_dir = output_dir or REPORTS_DIR
        os.makedirs(self.output_dir, exist_ok=True)
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        print("ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±...")
        
        # ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        quality_metrics = {
            'ì´ í–‰ ìˆ˜': len(X),
            'ì´ ì—´ ìˆ˜': len(X.columns),
            'ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜ ìˆ˜': X.columns[X.isnull().any()].sum(),
            'ì™„ì „í•œ í–‰ ìˆ˜': len(X.dropna()),
            'ë°ì´í„° ì™„ì„±ë„(%)': (len(X.dropna()) / len(X)) * 100,
            'ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìˆ˜': len(X.select_dtypes(include=[np.number]).columns),
            'ë²”ì£¼í˜• ë³€ìˆ˜ ìˆ˜': len(X.select_dtypes(include=['object', 'category']).columns)
        }
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ì •ë³´ (ìˆëŠ” ê²½ìš°)
        if 'target' in X.columns:
            target_dist = X['target'].value_counts()
            quality_metrics.update({
                'íƒ€ê²Ÿ ë³€ìˆ˜ ë¶€ë„ìœ¨(%)': (target_dist[1] / len(X)) * 100 if 1 in target_dist else 0,
                'íƒ€ê²Ÿ ë³€ìˆ˜ ì •ìƒìœ¨(%)': (target_dist[0] / len(X)) * 100 if 0 in target_dist else 0
            })
        
        # í’ˆì§ˆ ë³´ê³ ì„œ ì €ì¥
        report_path = os.path.join(self.output_dir, 'data_quality_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=== ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ===\n\n")
            for metric, value in quality_metrics.items():
                f.write(f"{metric}: {value}\n")
        
        print(f"  ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        return X

class SklearnPreprocessingPipeline:
    """scikit-learn ê¸°ë°˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, output_dir=None):
        self.output_dir = output_dir or REPORTS_DIR
        self.pipeline = None
        self._create_pipeline()
        
    def _create_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        self.pipeline = Pipeline([
            ('anomalous_remover', AnomalousRowRemover()),
            ('target_creator', TargetVariableCreator()),
            ('percentage_cleaner', PercentageCleaner()),
            ('missing_handler', HighMissingValueHandler()),
            ('fico_creator', FICOFeatureCreator()),
            ('categorical_encoder', CategoricalEncoder()),
            ('outlier_handler', OutlierHandler()),
            ('state_optimizer', StateOptimizer()),
            ('time_creator', TimeFeatureCreator()),
            ('credit_history_creator', CreditHistoryFeatureCreator()),  # PRD ì¶”ê°€
            ('account_activity_creator', AccountActivityFeatureCreator()),  # PRD ì¶”ê°€
            ('debt_ratio_creator', DebtRatioFeatureCreator()),  # PRD ì¶”ê°€
            # ('hardship_creator', HardshipFeatureCreator()),  # PRD ì¶”ê°€
            ('secondary_applicant_creator', SecondaryApplicantFeatureCreator()),  # PRD ì¶”ê°€
            ('log_transformation_creator', LogTransformationFeatureCreator()),  # PRD ì¶”ê°€
            ('interaction_creator', InteractionFeatureCreator()),  # PRD ì¶”ê°€
            ('unnecessary_remover', UnnecessaryFeatureRemover()),
            ('feature_selector', FeatureSelector()),
            ('composite_creator', CompositeFeatureCreator()),
            ('financial_creator', FinancialFeatureCreator()),
            ('final_missing_handler', FinalMissingValueHandler()),
            ('advanced_feature_selector', AdvancedFeatureSelector()),  # PRD ì¶”ê°€
            ('target_encoder', TargetEncoder()),
            ('scaler', Scaler()),
            ('one_hot_encoder', OneHotEncoder()),
            ('data_splitter', DataSplitter()),
            ('smote_handler', SMOTEHandler()),
            ('imbalance_handler', ImbalanceHandler()),
            # ('eda_analyzer', EDAAnalyzer()), # EDA ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¶”ê°€
            # ('data_quality_reporter', DataQualityReporter()) # ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ì¶”ê°€
        ])
        
    def fit_transform(self, data_path=None):
        """í›ˆë ¨ ë° ë³€í™˜"""
        print("=" * 80)
        print("scikit-learn ê¸°ë°˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        print("=" * 80)
        
        start_time = time.time()
        
        # ë°ì´í„° ë¡œë“œ (íŒŒì´í”„ë¼ì¸ ì™¸ë¶€ì—ì„œ ì²˜ë¦¬)
        data_loader = DataLoader(data_path)
        df = data_loader.transform(X=None)  # ëª…ì‹œì ìœ¼ë¡œ None ì „ë‹¬
        
        if df is None:
            return None
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë°ì´í„°ê°€ ì´ë¯¸ ë¡œë“œëœ ìƒíƒœ)
        result_df = self.pipeline.fit_transform(df)
        
        # ê²°ê³¼ ì €ì¥
        if result_df is not None:
            self._save_results(result_df)
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! (ì´ {total_time:.2f}ì´ˆ)")
        
        return result_df
    
    def transform(self, data_path):
        """ë³€í™˜ë§Œ ì‹¤í–‰ (í›ˆë ¨ëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)"""
        data_loader = DataLoader(data_path)
        df = data_loader.transform(X=None)  # ëª…ì‹œì ìœ¼ë¡œ None ì „ë‹¬
        
        if df is None:
            return None
        
        return self.pipeline.transform(df)
    
    def _save_results(self, df):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        output_path = os.path.join(self.output_dir, 'sklearn_preprocessed_data.csv')
        df.to_csv(output_path, index=False)
        print(f"âœ“ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {output_path}")
        
        # íŒŒì´í”„ë¼ì¸ ì €ì¥
        pipeline_path = os.path.join(self.output_dir, 'sklearn_pipeline.pkl')
        with open(pipeline_path, 'wb') as f:
            pickle.dump(self.pipeline, f)
        print(f"âœ“ íŒŒì´í”„ë¼ì¸ ì €ì¥: {pipeline_path}")
        
        print(f"âœ“ ìµœì¢… ë°ì´í„° í¬ê¸°: {df.shape}")
        print(f"âœ“ íŠ¹ì„± ìˆ˜: {len(df.columns)}ê°œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("scikit-learn ê¸°ë°˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
    pipeline = SklearnPreprocessingPipeline()
    result = pipeline.fit_transform()
    
    if result is not None:
        print("\nâœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 