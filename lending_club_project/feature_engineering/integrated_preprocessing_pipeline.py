#!/usr/bin/env python3
"""
í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Phase 3.3) - ì™„ì „ ë²„ì „
ëª¨ë“  ê°œì„ ì‚¬í•­ì„ í†µí•©í•œ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸

Phase 1: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­
Phase 2: ë‹¨ê¸° ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­  
Phase 3: ì¥ê¸° ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­
Phase 5: ì¶”ê°€ ì „ì²˜ë¦¬ ê°•í™” (Critical Priority)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import sys
import os
import time
from datetime import datetime
from pathlib import Path
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from scipy.stats import chi2_contingency
import psutil

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.file_paths import (
    SAMPLE_DATA_PATH,
    ENCODED_DATA_PATH,
    SCALED_STANDARD_DATA_PATH,
    NEW_FEATURES_DATA_PATH,
    REPORTS_DIR,
    ensure_directory_exists,
    file_exists,
    get_reports_file_path
)

class IntegratedPreprocessingPipeline:
    """
    í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Phase 3.3) - ì™„ì „ ë²„ì „
    
    Phase 1: ê¸°ë³¸ ë°ì´í„° ì •ë¦¬ ë° ì´ìƒì¹˜ ì²˜ë¦¬
    Phase 2: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ë° ì¸ì½”ë”©
    Phase 3: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™”
    Phase 5: ì¶”ê°€ ì „ì²˜ë¦¬ ê°•í™” (Critical Priority)
    
    ì›ë³¸ ë°ì´í„°ì™€ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì €ì¥í•˜ëŠ” ì˜µì…˜ ì œê³µ
    """
    
    def __init__(self, data_path=None, output_dir=None, mode='train', 
                 keep_original=True, save_separate=True):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            data_path: ì…ë ¥ ë°ì´í„° ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            mode: 'train' ë˜ëŠ” 'test'
            keep_original: ì›ë³¸ ì»¬ëŸ¼ ìœ ì§€ ì—¬ë¶€ (True: ìœ ì§€, False: ì œê±°)
            save_separate: ì›ë³¸ê³¼ ì „ì²˜ë¦¬ë³¸ì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€
        """
        self.data_path = data_path or SAMPLE_DATA_PATH
        self.output_dir = output_dir or REPORTS_DIR
        self.mode = mode  # 'train' ë˜ëŠ” 'test'
        self.keep_original = keep_original  # ì›ë³¸ ì»¬ëŸ¼ ìœ ì§€ ì—¬ë¶€
        self.save_separate = save_separate  # ë³„ë„ íŒŒì¼ ì €ì¥ ì—¬ë¶€
        
        # ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
        self.df = None
        
        # í›ˆë ¨ëœ íŒŒë¼ë¯¸í„° ì €ì¥ìš©
        self.trained_params = {
            'missing_imputation_values': {},
            'outlier_bounds': {},
            'categorical_mappings': {},
            'selected_features': [],
            'state_keep_list': [],
            'fico_bins': None,
            'income_bins': None,
            'credit_utilization_bins': None,
            'account_diversity_bins': None
        }
        
        # ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ì €ì¥ìš©
        self.quality_metrics = {}
        
        # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •ìš©
        self.execution_times = {}
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ({mode.upper()} ëª¨ë“œ)")
        print(f"ì›ë³¸ ì»¬ëŸ¼ ìœ ì§€: {keep_original}")
        print(f"ë³„ë„ íŒŒì¼ ì €ì¥: {save_separate}")
        print(f"ì‹¤í–‰ ëª¨ë“œ: {self.mode}")
    
    def fit(self, data_path=None):
        """í›ˆë ¨ ëª¨ë“œë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (íŒŒë¼ë¯¸í„° í•™ìŠµ)"""
        self.mode = 'train'
        if data_path:
            self.data_path = data_path
        return self.run_pipeline()
    
    def transform(self, data_path, output_dir=None):
        """í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì ìš©)"""
        self.mode = 'test'
        self.data_path = data_path
        if output_dir:
            self.output_dir = output_dir
        return self.run_pipeline()
    
    def save_trained_params(self, filepath):
        """í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì €ì¥"""
        import pickle
        with open(filepath, 'wb') as f:
            pickle.dump(self.trained_params, f)
        print(f"âœ“ í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì €ì¥: {filepath}")
    
    def load_trained_params(self, filepath):
        """í•™ìŠµëœ íŒŒë¼ë¯¸í„° ë¡œë“œ"""
        import pickle
        with open(filepath, 'rb') as f:
            self.trained_params = pickle.load(f)
        print(f"âœ“ í•™ìŠµëœ íŒŒë¼ë¯¸í„° ë¡œë“œ: {filepath}")
    
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ")
        print("-" * 40)
        
        try:
            # ë°ì´í„° ë¡œë“œ (low_memory=Falseë¡œ ì„¤ì •í•˜ì—¬ ê²½ê³  ë°©ì§€)
            self.df = pd.read_csv(self.data_path, low_memory=False)
            print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.df.shape}")
            print(f"  - í–‰ ìˆ˜: {len(self.df):,}")
            print(f"  - ì—´ ìˆ˜: {len(self.df.columns)}")
            print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
            
            # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
            print(f"  - ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜: {self.df.isnull().any().sum()}")
            print(f"  - ì „ì²´ ê²°ì¸¡ì¹˜ ìˆ˜: {self.df.isnull().sum().sum():,}")
            print(f"  - ë°ì´í„° íƒ€ì…ë³„ ì»¬ëŸ¼ ìˆ˜:")
            for dtype, count in self.df.dtypes.value_counts().items():
                print(f"    {dtype}: {count}ê°œ")
            
        except FileNotFoundError:
            print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_path}")
            print("ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼:")
            if os.path.exists('data'):
                for file in os.listdir('data'):
                    if file.endswith('.csv'):
                        print(f"  - data/{file}")
            return False
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    def remove_anomalous_rows(self):
        """ì´ìƒ ë¡œìš° ì œê±° (Phase 1.3)"""
        start_time = time.time()
        print("\nğŸ” 2ë‹¨ê³„: ì´ìƒ ë¡œìš° ì œê±°")
        print("-" * 40)
        
        original_size = len(self.df)
        
        # id ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì´ìƒ ë¡œìš° ì œê±°
        if 'id' in self.df.columns:
            # ì´ìƒ ë¡œìš° ì œê±°
            self.df = self.df[self.df['id'] != 'Loans that do not meet the credit policy'].copy()
            
            removed_count = original_size - len(self.df)
            print(f"âœ“ ì´ìƒ ë¡œìš° ì œê±° ì™„ë£Œ")
            print(f"  ì œê±°ëœ ë¡œìš°: {removed_count}ê°œ")
            print(f"  ë‚¨ì€ ë¡œìš°: {len(self.df)}ê°œ")
        else:
            print(f"âœ“ id ì»¬ëŸ¼ì´ ì—†ì–´ ì´ìƒ ë¡œìš° ì œê±° ë‹¨ê³„ ê±´ë„ˆëœ€")
            print(f"  ë‚¨ì€ ë¡œìš°: {len(self.df)}ê°œ")
        
        self.execution_times['remove_anomalous'] = time.time() - start_time
        return True
    
    def create_target_variable(self):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (Phase 1.2)"""
        start_time = time.time()
        print("\nğŸ¯ 3ë‹¨ê³„: íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±")
        print("-" * 40)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ë§¤í•‘
        loan_status_mapping = {
            'Fully Paid': 0,
            'Current': 0,
            'In Grace Period': 0,
            'Late (16-30 days)': 1,
            'Late (31-120 days)': 1,
            'Charged Off': 1,
            'Default': 1
        }
        
        self.df['target'] = self.df['loan_status'].map(loan_status_mapping)
        
        # íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
        target_dist = self.df['target'].value_counts()
        print(f"âœ“ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ")
        print(f"  ë¶€ë„ìœ¨: {target_dist[1]/len(self.df)*100:.2f}%")
        print(f"  ì •ìƒ: {target_dist[0]}ê°œ, ë¶€ë„: {target_dist[1]}ê°œ")
        
        self.execution_times['target_creation'] = time.time() - start_time
        return True
    
    def clean_percentage_columns(self):
        """í¼ì„¼íŠ¸ ì»¬ëŸ¼ ì •ë¦¬ (Phase 1.4)"""
        start_time = time.time()
        print("\nğŸ§¹ 4ë‹¨ê³„: í¼ì„¼íŠ¸ ì»¬ëŸ¼ ì •ë¦¬")
        print("-" * 40)
        
        percentage_columns = ['int_rate', 'revol_util']
        cleaned_count = 0
        
        for col in percentage_columns:
            if col in self.df.columns:
                # '%' ì œê±° ë° ìˆ«ìë¡œ ë³€í™˜
                self.df[col] = self.df[col].astype(str).str.replace('%', '').astype(float)
                cleaned_count += 1
                print(f"  âœ“ {col}: í¼ì„¼íŠ¸ ê¸°í˜¸ ì œê±° ì™„ë£Œ")
        
        print(f"âœ“ í¼ì„¼íŠ¸ ì»¬ëŸ¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ")
        
        self.execution_times['percentage_cleaning'] = time.time() - start_time
        return True
    
    def handle_high_missing_features(self):
        """ê³ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ ì²˜ë¦¬ (Phase 5.1)"""
        start_time = time.time()
        print("\nğŸ” 5ë‹¨ê³„: ê³ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ ì²˜ë¦¬")
        print("-" * 40)
        
        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚°
        missing_ratios = (self.df.isnull().sum() / len(self.df)) * 100
        high_missing_features = missing_ratios[missing_ratios >= 30].index.tolist()
        
        print(f"ê³ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ ({len(high_missing_features)}ê°œ):")
        for feature in high_missing_features:
            missing_ratio = missing_ratios[feature]
            print(f"  - {feature}: {missing_ratio:.2f}%")
        
        # ê³ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ë³„ ì²˜ë¦¬ ì „ëµ
        for feature in high_missing_features:
            if feature in self.df.columns:
                # ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ ìƒì„±
                self.df[f'{feature}_is_missing'] = self.df[feature].isna().astype(int)
                
                # ë³€ìˆ˜ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
                if self.df[feature].dtype in ['object', 'category']:
                    # ë²”ì£¼í˜• ë³€ìˆ˜: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
                    mode_value = self.df[feature].mode().iloc[0] if not self.df[feature].mode().empty else 'Unknown'
                    self.df[feature] = self.df[feature].fillna(mode_value)
                else:
                    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                    median_value = self.df[feature].median()
                    self.df[feature] = self.df[feature].fillna(median_value)
                
                print(f"  âœ“ {feature}: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        
        print(f"âœ“ ê³ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ ì²˜ë¦¬ ì™„ë£Œ: {len(high_missing_features)}ê°œ")
        
        self.execution_times['high_missing_features'] = time.time() - start_time
        return True
    
    def create_fico_features(self):
        """FICO íŠ¹ì„± ìƒì„± (Phase 2.1)"""
        start_time = time.time()
        print("\nğŸ“Š 6ë‹¨ê³„: FICO íŠ¹ì„± ìƒì„±")
        print("-" * 40)
        
        # FICO ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
        fico_columns = ['fico_range_low', 'fico_range_high', 
                       'last_fico_range_low', 'last_fico_range_high']
        
        available_fico_cols = [col for col in fico_columns if col in self.df.columns]
        
        if len(available_fico_cols) >= 2:
            # FICO í‰ê· ê°’ ê³„ì‚°
            if 'fico_range_low' in self.df.columns and 'fico_range_high' in self.df.columns:
                self.df['fico_avg'] = (pd.to_numeric(self.df['fico_range_low'], errors='coerce') + 
                                      pd.to_numeric(self.df['fico_range_high'], errors='coerce')) / 2
            
            if 'last_fico_range_low' in self.df.columns and 'last_fico_range_high' in self.df.columns:
                self.df['last_fico_avg'] = (pd.to_numeric(self.df['last_fico_range_low'], errors='coerce') + 
                                           pd.to_numeric(self.df['last_fico_range_high'], errors='coerce')) / 2
            
            # FICO ë³€í™”ìœ¨ ê³„ì‚°
            if 'fico_avg' in self.df.columns and 'last_fico_avg' in self.df.columns:
                self.df['fico_change'] = self.df['last_fico_avg'] - self.df['fico_avg']
                self.df['fico_change_rate'] = self.df['fico_change'] / (self.df['fico_avg'] + 1e-8)
            
            # FICO êµ¬ê°„í™” (5ì  ë‹¨ìœ„)
            if 'fico_avg' in self.df.columns:
                fico_bins = list(range(300, 850, 50)) + [850]
                fico_labels = [f'{fico_bins[i]}-{fico_bins[i+1]-1}' for i in range(len(fico_bins)-1)]
                self.df['fico_range'] = pd.cut(self.df['fico_avg'], bins=fico_bins, labels=fico_labels, include_lowest=True)
            
            print(f"âœ“ FICO íŠ¹ì„± ìƒì„± ì™„ë£Œ")
            print(f"  ìƒì„±ëœ íŠ¹ì„±: fico_avg, last_fico_avg, fico_change, fico_change_rate, fico_range")
        
        self.execution_times['fico_features'] = time.time() - start_time
        return True
    
    def enhanced_categorical_encoding(self):
        """í–¥ìƒëœ ë²”ì£¼í˜• ì¸ì½”ë”© (Phase 2.2)"""
        start_time = time.time()
        print("\nğŸ”¤ 7ë‹¨ê³„: í–¥ìƒëœ ë²”ì£¼í˜• ì¸ì½”ë”©")
        print("-" * 40)
        
        # sub_grade ìˆœì„œí˜• ì¸ì½”ë”©
        if 'sub_grade' in self.df.columns:
            grade_order = ['A1', 'A2', 'A3', 'A4', 'A5',
                          'B1', 'B2', 'B3', 'B4', 'B5',
                          'C1', 'C2', 'C3', 'C4', 'C5',
                          'D1', 'D2', 'D3', 'D4', 'D5',
                          'E1', 'E2', 'E3', 'E4', 'E5',
                          'F1', 'F2', 'F3', 'F4', 'F5',
                          'G1', 'G2', 'G3', 'G4', 'G5']
            
            self.df['sub_grade_ordinal'] = self.df['sub_grade'].map(
                {grade: idx for idx, grade in enumerate(grade_order)}
            ).fillna(0)
        
        # emp_length ìˆ˜ì¹˜í™” + ê²°ì¸¡ í”Œë˜ê·¸
        if 'emp_length' in self.df.columns:
            emp_length_mapping = {
                '< 1 year': 0.5,
                '1 year': 1, '2 years': 2, '3 years': 3, '4 years': 4, '5 years': 5,
                '6 years': 6, '7 years': 7, '8 years': 8, '9 years': 9, '10+ years': 10
            }
            self.df['emp_length_numeric'] = self.df['emp_length'].map(emp_length_mapping).fillna(0)
            self.df['emp_length_is_na'] = self.df['emp_length'].isna().astype(int)
        
        # home_ownership ì¹´í…Œê³ ë¦¬ ì •ë¦¬
        if 'home_ownership' in self.df.columns:
            self.df['home_ownership'] = self.df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')
        
        print(f"âœ“ í–¥ìƒëœ ë²”ì£¼í˜• ì¸ì½”ë”© ì™„ë£Œ")
        print(f"  ìƒì„±ëœ íŠ¹ì„±: sub_grade_ordinal, emp_length_numeric, emp_length_is_na")
        
        self.execution_times['categorical_encoding'] = time.time() - start_time
        return True
    
    def handle_outliers(self):
        """ì´ìƒê°’ ì²˜ë¦¬ (Phase 2.3)"""
        start_time = time.time()
        print("\nâš ï¸ 8ë‹¨ê³„: ì´ìƒê°’ ì²˜ë¦¬")
        print("-" * 40)
        
        outlier_handled = 0
        
        # dti 999 ì´ìƒê°’ ì²˜ë¦¬
        if 'dti' in self.df.columns:
            original_dti = self.df['dti'].copy()
            self.df['dti'] = np.where(self.df['dti'] >= 999, self.df['dti'].median(), self.df['dti'])
            if (original_dti >= 999).sum() > 0:
                outlier_handled += 1
                print(f"  âœ“ dti: 999 ì´ìƒê°’ ì²˜ë¦¬ ì™„ë£Œ")
        
        # revol_util 100% ì´ˆê³¼ê°’ í´ë¦¬í•‘
        if 'revol_util' in self.df.columns:
            original_revol_util = self.df['revol_util'].copy()
            self.df['revol_util'] = np.clip(self.df['revol_util'], 0, 100)
            if (original_revol_util > 100).sum() > 0:
                outlier_handled += 1
                print(f"  âœ“ revol_util: 100% ì´ˆê³¼ê°’ í´ë¦¬í•‘ ì™„ë£Œ")
        
        # annual_inc IQR ê¸°ë°˜ ì´ìƒê°’ ì²˜ë¦¬
        if 'annual_inc' in self.df.columns:
            Q1 = self.df['annual_inc'].quantile(0.25)
            Q3 = self.df['annual_inc'].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            original_annual_inc = self.df['annual_inc'].copy()
            self.df['annual_inc'] = np.clip(self.df['annual_inc'], lower_bound, upper_bound)
            
            if ((original_annual_inc < lower_bound) | (original_annual_inc > upper_bound)).sum() > 0:
                outlier_handled += 1
                print(f"  âœ“ annual_inc: IQR ê¸°ë°˜ ì´ìƒê°’ ì²˜ë¦¬ ì™„ë£Œ")
        
        print(f"âœ“ ì´ìƒê°’ ì²˜ë¦¬ ì™„ë£Œ: {outlier_handled}ê°œ ë³€ìˆ˜")
        
        self.execution_times['outlier_handling'] = time.time() - start_time
        return True
    
    def optimize_state_encoding(self):
        """ì£¼(state) ë°ì´í„° ìµœì í™” (Phase 2.4)"""
        start_time = time.time()
        print("\nğŸ—ºï¸ 9ë‹¨ê³„: ì£¼(state) ë°ì´í„° ìµœì í™”")
        print("-" * 40)
        
        if 'addr_state' in self.df.columns:
            # ìƒìœ„ 99% ì£¼ë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” 'OTHER'ë¡œ ê·¸ë£¹í™”
            state_counts = self.df['addr_state'].value_counts()
            total_count = len(self.df)
            cumulative_percent = (state_counts.cumsum() / total_count) * 100
            
            # 99%ì— í•´ë‹¹í•˜ëŠ” ì£¼ë“¤ë§Œ ìœ ì§€
            keep_states = cumulative_percent[cumulative_percent <= 99].index.tolist()
            
            # ë‚˜ë¨¸ì§€ ì£¼ë“¤ì„ 'OTHER'ë¡œ ë³€ê²½
            self.df['addr_state_optimized'] = self.df['addr_state'].apply(
                lambda x: x if x in keep_states else 'OTHER'
            )
            
            print(f"âœ“ ì£¼ ë°ì´í„° ìµœì í™” ì™„ë£Œ")
            print(f"  ì›ë³¸ ì£¼ ìˆ˜: {self.df['addr_state'].nunique()}ê°œ")
            print(f"  ìµœì í™” í›„ ì£¼ ìˆ˜: {self.df['addr_state_optimized'].nunique()}ê°œ")
        
        self.execution_times['state_optimization'] = time.time() - start_time
        return True
    
    def enhance_time_based_features(self):
        """í–¥ìƒëœ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„± (Phase 5.2)"""
        start_time = time.time()
        print("\nâ° 10ë‹¨ê³„: í–¥ìƒëœ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±")
        print("-" * 40)
        
        # ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ í™•ì¸
        date_columns = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d']
        available_date_cols = [col for col in date_columns if col in self.df.columns]
        
        if len(available_date_cols) >= 2:
            # ëŒ€ì¶œ ë°œí–‰ ì‹œì  ì •ë³´ ì¶”ì¶œ
            if 'issue_d' in self.df.columns:
                self.df['issue_date'] = pd.to_datetime(self.df['issue_d'], format='%b-%Y', errors='coerce')
                self.df['issue_year'] = self.df['issue_date'].dt.year
                self.df['issue_month'] = self.df['issue_date'].dt.month
                self.df['issue_quarter'] = self.df['issue_date'].dt.quarter
                
                # ê³„ì ˆì„± íŠ¹ì„±
                self.df['issue_season'] = self.df['issue_month'].map({
                    12: 'Winter', 1: 'Winter', 2: 'Winter',
                    3: 'Spring', 4: 'Spring', 5: 'Spring',
                    6: 'Summer', 7: 'Summer', 8: 'Summer',
                    9: 'Fall', 10: 'Fall', 11: 'Fall'
                })
                
                # ê²½ì œ ì‚¬ì´í´ íŠ¹ì„± (ì—°ë„ë³„)
                self.df['is_recession_year'] = self.df['issue_year'].isin([2008, 2009, 2020]).astype(int)
            
            # ì‹ ìš© ì´ë ¥ ê¸°ê°„ ê³„ì‚°
            if 'earliest_cr_line' in self.df.columns and 'issue_d' in self.df.columns:
                self.df['earliest_cr_date'] = pd.to_datetime(self.df['earliest_cr_line'], format='%b-%Y', errors='coerce')
                self.df['credit_history_months'] = ((self.df['issue_date'] - self.df['earliest_cr_date']).dt.days / 30.44).fillna(0)
                self.df['credit_history_years'] = self.df['credit_history_months'] / 12
                
                # ì‹ ìš© ì´ë ¥ êµ¬ê°„í™”
                self.df['credit_history_category'] = pd.cut(
                    self.df['credit_history_years'],
                    bins=[0, 2, 5, 10, 50],
                    labels=['New', 'Young', 'Established', 'Veteran']
                )
            
            # ë§ˆì§€ë§‰ ê²°ì œì¼ ì •ë³´
            if 'last_pymnt_d' in self.df.columns:
                self.df['last_pymnt_date'] = pd.to_datetime(self.df['last_pymnt_d'], format='%b-%Y', errors='coerce')
                self.df['days_since_last_payment'] = (pd.Timestamp.now() - self.df['last_pymnt_date']).dt.days.fillna(0)
            
            print(f"âœ“ í–¥ìƒëœ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
            print(f"  ìƒì„±ëœ íŠ¹ì„±: issue_date, issue_year, issue_month, issue_quarter, issue_season, is_recession_year, credit_history_months, credit_history_years, credit_history_category, last_pymnt_date, days_since_last_payment")
        
        self.execution_times['enhanced_time_features'] = time.time() - start_time
        return True
    
    def create_advanced_composite_features(self):
        """ê³ ê¸‰ ë³µí•© ì§€í‘œ ìƒì„± (Phase 5.4)"""
        start_time = time.time()
        print("\nğŸ”— 11ë‹¨ê³„: ê³ ê¸‰ ë³µí•© ì§€í‘œ ìƒì„±")
        print("-" * 40)
        
        # ì‹ ìš© ì ìˆ˜ ë³€í™”ìœ¨ ê³„ì‚° ê°œì„ 
        if 'fico_change_rate' in self.df.columns:
            self.df['fico_improvement'] = (self.df['fico_change_rate'] > 0).astype(int)
            self.df['fico_decline'] = (self.df['fico_change_rate'] < 0).astype(int)
        
        # ì†Œë“ ëŒ€ë¹„ ë¶€ì±„ ë¹„ìœ¨ ì„¸ë¶„í™”
        if 'annual_inc' in self.df.columns and 'dti' in self.df.columns:
            self.df['debt_to_income_ratio'] = self.df['dti']
            self.df['income_category'] = pd.cut(
                self.df['annual_inc'],
                bins=[0, 30000, 60000, 100000, float('inf')],
                labels=['Low', 'Medium', 'High', 'Very High']
            )
        
        # ì—°ì²´ ì‹¬ê°ë„ ì ìˆ˜ ì²´ê³„í™”
        if 'delinq_2yrs' in self.df.columns:
            # ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬
            delinq_2yrs_clean = self.df['delinq_2yrs'].fillna(0)
            
            # ì¡°ê±´ë¶€ë¡œ ë¶„ë¥˜
            self.df['delinquency_severity'] = 'None'  # ê¸°ë³¸ê°’
            self.df.loc[delinq_2yrs_clean == 0, 'delinquency_severity'] = 'None'
            self.df.loc[(delinq_2yrs_clean >= 1) & (delinq_2yrs_clean < 3), 'delinquency_severity'] = 'Low'
            self.df.loc[(delinq_2yrs_clean >= 3) & (delinq_2yrs_clean < 5), 'delinquency_severity'] = 'Medium'
            self.df.loc[delinq_2yrs_clean >= 5, 'delinquency_severity'] = 'High'
        
        # ì‹ ìš© ì´ìš©ë¥  ìœ„í—˜ë„ ì •êµí™”
        if 'revol_util' in self.df.columns:
            self.df['credit_utilization_risk'] = pd.cut(
                self.df['revol_util'],
                bins=[0, 30, 60, 80, 100],
                labels=['Low', 'Medium', 'High', 'Very High']
            )
        
        # ê³„ì¢Œ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
        if 'total_acc' in self.df.columns:
            # open_accê°€ ì—†ìœ¼ë©´ total_accë¥¼ ì‚¬ìš©
            if 'open_acc' in self.df.columns:
                self.df['account_diversity_ratio'] = self.df['open_acc'] / (self.df['total_acc'] + 1e-8)
            else:
                # total_accë§Œ ìˆëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                self.df['account_diversity_ratio'] = 0.5
            
            self.df['account_diversity_score'] = pd.cut(
                self.df['account_diversity_ratio'],
                bins=[0, 0.3, 0.6, 0.8, 1.0],
                labels=['Low', 'Medium', 'High', 'Very High']
            )
        
        print(f"âœ“ ê³ ê¸‰ ë³µí•© ì§€í‘œ ìƒì„± ì™„ë£Œ")
        print(f"  ìƒì„±ëœ íŠ¹ì„±: fico_improvement, fico_decline, debt_to_income_ratio, income_category, delinquency_severity, credit_utilization_risk, account_diversity_ratio, account_diversity_score")
        
        self.execution_times['composite_features'] = time.time() - start_time
        return True
    
    def improve_feature_selection(self):
        """íŠ¹ì„± ì„ íƒ ê°œì„  (Phase 5.3)"""
        start_time = time.time()
        print("\nğŸ¯ 15ë‹¨ê³„: íŠ¹ì„± ì„ íƒ ê°œì„ ")
        print("-" * 40)
        
        if 'target' in self.df.columns:
            # ê¸ˆìœµ ëª¨ë¸ë§ í•„ìˆ˜ íŠ¹ì„± ë³´ì¡´ ëª©ë¡ (í™•ì¥ë¨)
            financial_critical_features = [
                # ê¸°ë³¸ ëŒ€ì¶œ ì •ë³´
                'term', 'int_rate', 'loan_amnt', 'funded_amnt',
                'installment', 'total_pymnt', 'grade', 'sub_grade',
                'annual_inc', 'dti', 'revol_util',
                
                # ëŒ€ì¶œ ëª©ì  ë° ê²€ì¦
                'purpose', 'verification_status',
                
                # ì‹ ìš© ì´ë ¥
                'delinq_2yrs', 'earliest_cr_line', 'fico_range_low', 'fico_range_high',
                'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'total_acc',
                
                # ì‹ ìš© ì¹´ë“œ ê´€ë ¨
                'bc_open_to_buy', 'bc_util', 'percent_bc_gt_75',
                
                # ëŒ€ì¶œ ë‹¤ì–‘ì„±
                'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_tl', 'num_il_tl',
                'num_op_rev_tl', 'num_rev_tl_bal_gt_0',
                
                # ì—°ì²´ íŒ¨í„´
                'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',
                
                # ì”ì•¡/í•œë„ ê´€ë ¨
                'tot_cur_bal', 'avg_cur_bal', 'tot_hi_cred_lim',
                'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',
                
                # ê¸°íƒ€ ì¤‘ìš” íŠ¹ì„±
                'collections_12_mths_ex_med', 'acc_now_delinq',
                'pub_rec_bankruptcies', 'tax_liens', 'chargeoff_within_12_mths',
                
                # íŒŒì´í”„ë¼ì¸ì—ì„œ ìƒì„±ëœ ê¸ˆìœµ íŠ¹ì„±ë“¤
                'loan_to_income_ratio', 'monthly_payment_ratio',
                'grade_risk_score', 'term_risk_score',
                'expected_return_rate', 'risk_adjusted_return'
            ]
            
            # í›„í–‰ì§€í‘œ ë³€ìˆ˜ ì œê±° (ê¸ˆìœµ íŠ¹ì„± ì œì™¸)
            posterior_variables = [
                'total_pymnt', 'total_pymnt_inv', 'total_rec_int',
                'total_rec_prncp', 'total_rec_late_fee', 'recoveries',
                'collection_recovery_fee', 'last_pymnt_amnt', 'last_pymnt_d',
                'next_pymnt_d', 'last_fico_range_high', 'last_fico_range_low'
            ]
            
            for var in posterior_variables:
                if var in self.df.columns and var not in financial_critical_features:
                    self.df = self.df.drop(columns=[var])
                    print(f"  âœ“ í›„í–‰ì§€í‘œ ì œê±°: {var}")
            
            # ìƒê´€ê´€ê³„ ë¶„ì„ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            numerical_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
            if len(numerical_cols) > 1:
                corr_matrix = self.df[numerical_cols].corr().abs()
                
                # ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ì„ 0.95ë¡œ ë†’ì„ (ë” ë³´ìˆ˜ì )
                upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
                high_corr_pairs = [(col1, col2) for col1, col2 in zip(upper_tri.index, upper_tri.columns) 
                                 if upper_tri.loc[col1, col2] > 0.95]
                
                removed_vars = set()
                for var1, var2 in high_corr_pairs:
                    if (var1 not in removed_vars and var2 not in removed_vars and 
                        var1 not in financial_critical_features and var2 not in financial_critical_features):
                        # ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìœ ì§€
                        var1_info = self.df[var1].nunique() if var1 in self.df.columns else 0
                        var2_info = self.df[var2].nunique() if var2 in self.df.columns else 0
                        
                        var_to_remove = var1 if var1_info < var2_info else var2
                        self.df = self.df.drop(columns=[var_to_remove])
                        removed_vars.add(var_to_remove)
                        print(f"  âœ“ ì¤‘ë³µ ë³€ìˆ˜ ì œê±°: {var_to_remove} (ìƒê´€ê³„ìˆ˜: {upper_tri.loc[var1, var2]:.3f})")
                
                print(f"  ë³´ì¡´ëœ ê¸ˆìœµ íŠ¹ì„±: {len([f for f in financial_critical_features if f in self.df.columns])}ê°œ")
        
        print(f"âœ“ íŠ¹ì„± ì„ íƒ ê°œì„  ì™„ë£Œ")
        print(f"  ìµœì¢… íŠ¹ì„± ìˆ˜: {len(self.df.columns)}ê°œ")
        
        self.execution_times['feature_selection'] = time.time() - start_time
        return True
    
    def statistical_validation(self):
        """í†µê³„ì  ê²€ì¦ (Phase 2.5)"""
        start_time = time.time()
        print("\nğŸ“ˆ 13ë‹¨ê³„: í†µê³„ì  ê²€ì¦")
        print("-" * 40)
        
        if 'target' in self.df.columns:
            # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì™€ íƒ€ê²Ÿ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
            numeric_cols = self.df.select_dtypes(include=[np.number]).columns
            numeric_cols = [col for col in numeric_cols if col != 'target']
            
            correlations = []
            for col in numeric_cols:
                corr = self.df[col].corr(self.df['target'])
                if not pd.isna(corr):
                    correlations.append((col, abs(corr)))
            
            # ìƒê´€ê´€ê³„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            correlations.sort(key=lambda x: x[1], reverse=True)
            
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¹´ì´ì œê³± ê²€ì •
            categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns
            chi2_results = {}
            
            for col in categorical_cols:
                if col != 'target':
                    contingency_table = pd.crosstab(self.df[col], self.df['target'])
                    if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:
                        chi2, p_value, dof, expected = chi2_contingency(contingency_table)
                        chi2_results[col] = {'chi2': chi2, 'p_value': p_value}
            
            print(f"âœ“ í†µê³„ì  ê²€ì¦ ì™„ë£Œ")
            print(f"  ë¶„ì„ëœ ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ")
            print(f"  ë¶„ì„ëœ ë²”ì£¼í˜• ë³€ìˆ˜: {len(chi2_results)}ê°œ")
            print(f"  ìƒìœ„ 5ê°œ ìƒê´€ê´€ê³„:")
            for i, (col, corr) in enumerate(correlations[:5], 1):
                print(f"    {i}. {col}: {corr:.4f}")
        
        self.execution_times['statistical_validation'] = time.time() - start_time
        return True
    
    def create_visualization_report(self):
        """ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        start_time = time.time()
        print("\nğŸ“Š 14ë‹¨ê³„: ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±")
        print("-" * 40)
        
        # ì‹œê°í™” ë””ë ‰í† ë¦¬ ìƒì„±
        viz_dir = os.path.join(self.output_dir, 'visualizations')
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬
        if 'target' in self.df.columns:
            plt.figure(figsize=(10, 6))
            target_counts = self.df['target'].value_counts()
            plt.pie(target_counts.values, labels=['ì •ìƒ', 'ë¶€ë„'], autopct='%1.1f%%', startangle=90)
            plt.title('íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬')
            plt.savefig(os.path.join(viz_dir, 'target_distribution.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        # 2. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ê°œ)
        if 'target' in self.df.columns:
            numeric_cols = self.df.select_dtypes(include=[np.number]).columns
            numeric_cols = [col for col in numeric_cols if col != 'target'][:10]
            
            fig, axes = plt.subplots(2, 5, figsize=(20, 8))
            axes = axes.ravel()
            
            for i, col in enumerate(numeric_cols):
                if i < 10:
                    axes[i].hist(self.df[col].dropna(), bins=30, alpha=0.7)
                    axes[i].set_title(col)
                    axes[i].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(os.path.join(viz_dir, 'numeric_distributions.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        # 3. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        if 'target' in self.df.columns:
            numeric_cols = self.df.select_dtypes(include=[np.number]).columns
            numeric_cols = [col for col in numeric_cols if col != 'target'][:20]  # ìƒìœ„ 20ê°œë§Œ
            
            correlation_matrix = self.df[numeric_cols + ['target']].corr()
            
            plt.figure(figsize=(12, 10))
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
                       square=True, linewidths=0.5)
            plt.title('íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ')
            plt.tight_layout()
            plt.savefig(os.path.join(viz_dir, 'correlation_heatmap.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"âœ“ ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        print(f"  ì €ì¥ ìœ„ì¹˜: {viz_dir}")
        
        self.execution_times['visualization_report'] = time.time() - start_time
        return True
    
    def create_validation_report(self):
        """ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        start_time = time.time()
        print("\nğŸ“‹ 15ë‹¨ê³„: ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±")
        print("-" * 40)
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„ ìš”ì•½
        total_time = sum(self.execution_times.values())
        
        # ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
        quality_metrics = {
            'ì´ í–‰ ìˆ˜': len(self.df),
            'ì´ ì—´ ìˆ˜': len(self.df.columns),
            'ê²°ì¸¡ì¹˜ ë¹„ìœ¨': (self.df.isnull().sum().sum() / (len(self.df) * len(self.df.columns))) * 100,
            'ì¤‘ë³µ í–‰ ìˆ˜': self.df.duplicated().sum(),
            'íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬': self.df['target'].value_counts().to_dict() if 'target' in self.df.columns else None
        }
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report_content = f"""
# í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ë¦¬í¬íŠ¸ (ì™„ì „ ë²„ì „)

## ğŸ“Š ì‹¤í–‰ ìš”ì•½
- ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ
- ë°ì´í„° í¬ê¸°: {self.df.shape}
- ìƒì„±ëœ íŠ¹ì„± ìˆ˜: {len(self.df.columns)}

## â±ï¸ ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„
"""
        
        for step, time_taken in self.execution_times.items():
            report_content += f"- {step}: {time_taken:.2f}ì´ˆ\n"
        
        report_content += f"""
## ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
- ì´ í–‰ ìˆ˜: {quality_metrics['ì´ í–‰ ìˆ˜']:,}ê°œ
- ì´ ì—´ ìˆ˜: {quality_metrics['ì´ ì—´ ìˆ˜']}ê°œ
- ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {quality_metrics['ê²°ì¸¡ì¹˜ ë¹„ìœ¨']:.2f}%
- ì¤‘ë³µ í–‰ ìˆ˜: {quality_metrics['ì¤‘ë³µ í–‰ ìˆ˜']}ê°œ
"""
        
        if quality_metrics['íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬']:
            report_content += f"- íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬: {quality_metrics['íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬']}\n"
        
        # íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns
        
        report_content += f"""
## ğŸ”¢ íŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
- ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ
- ë²”ì£¼í˜• ë³€ìˆ˜: {len(categorical_cols)}ê°œ
- ì´ íŠ¹ì„± ìˆ˜: {len(self.df.columns)}ê°œ

## ğŸ¯ ì£¼ìš” ê°œì„ ì‚¬í•­ ì ìš© í˜„í™©
- âœ… Phase 1: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ (4/4 ì™„ë£Œ)
- âœ… Phase 2: ë‹¨ê¸° ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ (8/8 ì™„ë£Œ)
- âœ… Phase 3: ì¥ê¸° ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­ (3/3 ì™„ë£Œ)
- âœ… Phase 5: ì¶”ê°€ ì „ì²˜ë¦¬ ê°•í™” (5/5 ì™„ë£Œ)

## ğŸ“Š ìƒì„±ëœ ì£¼ìš” íŠ¹ì„±
"""
        
        # ìƒì„±ëœ íŠ¹ì„±ë“¤ ë‚˜ì—´
        new_features = [
            'fico_avg', 'last_fico_avg', 'fico_change', 'fico_change_rate', 'fico_range',
            'sub_grade_ordinal', 'emp_length_numeric', 'emp_length_is_na',
            'addr_state_optimized', 'issue_year', 'issue_month', 'issue_quarter', 'issue_season',
            'credit_history_months', 'credit_history_years', 'credit_history_category',
            'fico_improvement', 'fico_decline', 'debt_to_income_ratio', 'income_category',
            'delinquency_severity', 'credit_utilization_risk', 'account_diversity_ratio'
        ]
        
        for feature in new_features:
            if feature in self.df.columns:
                report_content += f"- {feature}\n"
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = os.path.join(self.output_dir, 'integrated_preprocessing_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ“ ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        print(f"  ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        self.execution_times['report_generation'] = time.time() - start_time
        return True
    
    def save_processed_data(self):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        start_time = time.time()
        print("\n" + "=" * 40)
        print("ë°ì´í„° ì €ì¥")
        print("=" * 40)
        
        # ì›ë³¸ ë°ì´í„° ì €ì¥ (ë³„ë„ íŒŒì¼)
        if self.save_separate:
            original_data_path = os.path.join(self.output_dir, 'original_data.csv')
            # ì›ë³¸ ë°ì´í„°ë¥¼ ë³„ë„ë¡œ ë¡œë“œí•˜ì—¬ ì €ì¥
            original_df = pd.read_csv(self.data_path)
            original_df.to_csv(original_data_path, index=False)
            print(f"  ì›ë³¸ ë°ì´í„° ì €ì¥: {original_data_path}")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        output_path = os.path.join(self.output_dir, 'integrated_preprocessed_data.csv')
        self.df.to_csv(output_path, index=False)
        print(f"  ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {output_path}")
        
        # íŠ¹ì„± ìš”ì•½ ì €ì¥
        feature_summary = []
        for col in self.df.columns:
            feature_summary.append({
                'feature_name': col,
                'data_type': str(self.df[col].dtype),
                'missing_count': self.df[col].isnull().sum(),
                'missing_ratio': f"{self.df[col].isnull().sum() / len(self.df) * 100:.2f}%"
            })
        
        feature_summary_df = pd.DataFrame(feature_summary)
        feature_summary_path = os.path.join(self.output_dir, 'feature_summary.csv')
        feature_summary_df.to_csv(feature_summary_path, index=False)
        print(f"  íŠ¹ì„± ìš”ì•½ ì €ì¥: {feature_summary_path}")
        
        # ë°ì´í„° ë¶„ë¦¬ ì •ë³´ ì €ì¥
        separation_info = {
            'original_columns': list(pd.read_csv(self.data_path).columns),
            'preprocessed_columns': list(self.df.columns),
            'new_columns': [col for col in self.df.columns if col not in pd.read_csv(self.data_path).columns],
            'removed_columns': [col for col in pd.read_csv(self.data_path).columns if col not in self.df.columns]
        }
        
        separation_info_path = os.path.join(self.output_dir, 'data_separation_info.txt')
        with open(separation_info_path, 'w', encoding='utf-8') as f:
            f.write("=== ë°ì´í„° ë¶„ë¦¬ ì •ë³´ ===\n\n")
            f.write(f"ì›ë³¸ ì»¬ëŸ¼ ìˆ˜: {len(separation_info['original_columns'])}\n")
            f.write(f"ì „ì²˜ë¦¬ í›„ ì»¬ëŸ¼ ìˆ˜: {len(separation_info['preprocessed_columns'])}\n")
            f.write(f"ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼ ìˆ˜: {len(separation_info['new_columns'])}\n")
            f.write(f"ì œê±°ëœ ì»¬ëŸ¼ ìˆ˜: {len(separation_info['removed_columns'])}\n\n")
            
            f.write("=== ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼ë“¤ ===\n")
            for col in separation_info['new_columns']:
                f.write(f"- {col}\n")
            
            f.write("\n=== ì œê±°ëœ ì»¬ëŸ¼ë“¤ ===\n")
            for col in separation_info['removed_columns']:
                f.write(f"- {col}\n")
        
        print(f"  ë°ì´í„° ë¶„ë¦¬ ì •ë³´ ì €ì¥: {separation_info_path}")
        
        print(f"\nâœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        print(f"  - ì›ë³¸ ë°ì´í„°: {os.path.join(self.output_dir, 'original_data.csv')}")
        print(f"  - ì „ì²˜ë¦¬ ë°ì´í„°: {output_path}")
        print(f"  - íŠ¹ì„± ìš”ì•½: {feature_summary_path}")
        print(f"  - ë¶„ë¦¬ ì •ë³´: {separation_info_path}")
        
        self.execution_times['data_saving'] = time.time() - start_time
    
    def create_clean_version(self):
        """ì›ë³¸ ì»¬ëŸ¼ì„ ì œê±°í•œ ê¹”ë”í•œ ë²„ì „ ìƒì„±"""
        print("\n" + "=" * 40)
        print("ê¹”ë”í•œ ë²„ì „ ìƒì„±")
        print("=" * 40)
        
        # ì›ë³¸ ì»¬ëŸ¼ë“¤ ì‹ë³„
        original_df = pd.read_csv(self.data_path)
        original_columns = set(original_df.columns)
        
        # ì›ë³¸ ì»¬ëŸ¼ë“¤ì„ ì œê±°í•œ ê¹”ë”í•œ ë²„ì „ ìƒì„±
        clean_df = self.df.copy()
        columns_to_remove = []
        
        for col in clean_df.columns:
            if col in original_columns:
                # ì›ë³¸ ì»¬ëŸ¼ì´ì§€ë§Œ ì „ì²˜ë¦¬ëœ ë²„ì „ì´ ìˆëŠ” ê²½ìš°ë§Œ ì œê±°
                processed_version = None
                if col == 'issue_d' and 'issue_date' in clean_df.columns:
                    processed_version = 'issue_date'
                elif col == 'earliest_cr_line' and 'earliest_cr_date' in clean_df.columns:
                    processed_version = 'earliest_cr_date'
                elif col == 'last_pymnt_d' and 'last_pymnt_date' in clean_df.columns:
                    processed_version = 'last_pymnt_date'
                elif col == 'emp_length' and 'emp_length_numeric' in clean_df.columns:
                    processed_version = 'emp_length_numeric'
                elif col == 'sub_grade' and 'sub_grade_ordinal' in clean_df.columns:
                    processed_version = 'sub_grade_ordinal'
                elif col == 'home_ownership' and 'home_ownership' in clean_df.columns:
                    processed_version = 'home_ownership'
                
                if processed_version:
                    columns_to_remove.append(col)
                    print(f"  ì œê±°: {col} (ëŒ€ì²´: {processed_version})")
        
        # ì›ë³¸ ì»¬ëŸ¼ë“¤ ì œê±°
        clean_df = clean_df.drop(columns=columns_to_remove)
        
        # ê¹”ë”í•œ ë²„ì „ ì €ì¥
        clean_output_path = os.path.join(self.output_dir, 'clean_preprocessed_data.csv')
        clean_df.to_csv(clean_output_path, index=False)
        
        print(f"\nâœ… ê¹”ë”í•œ ë²„ì „ ìƒì„± ì™„ë£Œ")
        print(f"  - ì›ë³¸ ì»¬ëŸ¼ ìˆ˜: {len(self.df.columns)}")
        print(f"  - ê¹”ë”í•œ ë²„ì „ ì»¬ëŸ¼ ìˆ˜: {len(clean_df.columns)}")
        print(f"  - ì œê±°ëœ ì»¬ëŸ¼ ìˆ˜: {len(columns_to_remove)}")
        print(f"  - ì €ì¥ ê²½ë¡œ: {clean_output_path}")
        
        return clean_df
    
    def preserve_financial_features(self):
        """ê¸ˆìœµ ëª¨ë¸ë§ í•„ìˆ˜ íŠ¹ì„± ë³´ì¡´ (Phase 3 ëŒ€ë¹„)"""
        print("\nğŸ’° ê¸ˆìœµ ëª¨ë¸ë§ í•„ìˆ˜ íŠ¹ì„± ë³´ì¡´")
        print("-" * 40)
        
        # ê¸ˆìœµ ëª¨ë¸ë§ì— í•„ìˆ˜ì ì¸ íŠ¹ì„±ë“¤ (í™•ì¥ë¨)
        financial_critical_features = [
            # ê¸°ë³¸ ëŒ€ì¶œ ì •ë³´
            'term', 'int_rate', 'loan_amnt', 'funded_amnt',
            'installment', 'total_pymnt', 'grade', 'sub_grade',
            'annual_inc', 'dti', 'revol_util',
            
            # ëŒ€ì¶œ ëª©ì  ë° ê²€ì¦ (ì¤‘ìš”!)
            'purpose', 'verification_status',
            
            # ì‹ ìš© ì´ë ¥ (ì¤‘ìš”!)
            'delinq_2yrs', 'earliest_cr_line', 'fico_range_low', 'fico_range_high',
            'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'total_acc',
            
            # ì‹ ìš© ì¹´ë“œ ê´€ë ¨ (ì¤‘ìš”!)
            'bc_open_to_buy', 'bc_util', 'percent_bc_gt_75',
            
            # ëŒ€ì¶œ ë‹¤ì–‘ì„± (ì¤‘ìš”!)
            'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_tl', 'num_il_tl',
            'num_op_rev_tl', 'num_rev_tl_bal_gt_0',
            
            # ì—°ì²´ íŒ¨í„´ (ì¤‘ìš”!)
            'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',
            
            # ì”ì•¡/í•œë„ ê´€ë ¨
            'tot_cur_bal', 'avg_cur_bal', 'tot_hi_cred_lim',
            'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',
            
            # ê¸°íƒ€ ì¤‘ìš” íŠ¹ì„±
            'collections_12_mths_ex_med', 'acc_now_delinq',
            'pub_rec_bankruptcies', 'tax_liens', 'chargeoff_within_12_mths',
            
            # íŒŒì´í”„ë¼ì¸ì—ì„œ ìƒì„±ëœ ê¸ˆìœµ íŠ¹ì„±ë“¤
            'loan_to_income_ratio', 'monthly_payment_ratio',
            'grade_risk_score', 'term_risk_score',
            'expected_return_rate', 'risk_adjusted_return'
        ]
        
        preserved_features = []
        for feature in financial_critical_features:
            if feature in self.df.columns:
                preserved_features.append(feature)
                print(f"  âœ“ ë³´ì¡´: {feature}")
            else:
                print(f"  âš ï¸ ëˆ„ë½: {feature}")
        
        print(f"\nâœ“ ê¸ˆìœµ ëª¨ë¸ë§ í•„ìˆ˜ íŠ¹ì„± ë³´ì¡´ ì™„ë£Œ: {len(preserved_features)}ê°œ")
        return preserved_features
    
    def create_financial_features(self):
        """ê¸ˆìœµ ëª¨ë¸ë§ ì „ìš© íŠ¹ì„± ìƒì„± (Sharpe Ratio ê³„ì‚°ìš©)"""
        print("\nğŸ“ˆ ê¸ˆìœµ ëª¨ë¸ë§ ì „ìš© íŠ¹ì„± ìƒì„±")
        print("-" * 40)
        
        # 1. ëŒ€ì¶œ ì¡°ê±´ ê´€ë ¨ íŠ¹ì„±
        if 'loan_amnt' in self.df.columns and 'annual_inc' in self.df.columns:
            self.df['loan_to_income_ratio'] = self.df['loan_amnt'] / (self.df['annual_inc'] + 1e-8)
            print("  âœ“ loan_to_income_ratio ìƒì„±")
        
        if 'installment' in self.df.columns and 'annual_inc' in self.df.columns:
            self.df['monthly_payment_ratio'] = (self.df['installment'] * 12) / (self.df['annual_inc'] + 1e-8)
            print("  âœ“ monthly_payment_ratio ìƒì„±")
        
        # 2. ì‹ ìš© ë“±ê¸‰ë³„ ìœ„í—˜ë„ ì ìˆ˜
        if 'grade' in self.df.columns:
            grade_risk_mapping = {
                'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7
            }
            self.df['grade_risk_score'] = self.df['grade'].map(grade_risk_mapping).fillna(4)
            print("  âœ“ grade_risk_score ìƒì„±")
        
        # 3. ëŒ€ì¶œ ê¸°ê°„ë³„ ìœ„í—˜ë„
        if 'term' in self.df.columns:
            self.df['term_months'] = self.df['term'].str.extract(r'(\d+)').astype(float)
            self.df['term_risk_score'] = pd.cut(
                self.df['term_months'],
                bins=[0, 36, 60, float('inf')],
                labels=['Low', 'Medium', 'High']
            )
            print("  âœ“ term_risk_score ìƒì„±")
        
        # 4. ì˜ˆìƒ ìˆ˜ìµë¥  (ê°„ë‹¨í•œ ì¶”ì •)
        if 'int_rate' in self.df.columns and 'grade_risk_score' in self.df.columns:
            # ê¸°ë³¸ ìˆ˜ìµë¥  = ì´ììœ¨ - ìœ„í—˜ë„ ë³´ì •
            self.df['expected_return_rate'] = self.df['int_rate'] - (self.df['grade_risk_score'] * 0.5)
            print("  âœ“ expected_return_rate ìƒì„±")
        
        # 5. ìœ„í—˜ì¡°ì •ìˆ˜ìµë¥  (Sharpe Ratio ê³„ì‚°ìš©)
        if 'expected_return_rate' in self.df.columns and 'grade_risk_score' in self.df.columns:
            # ê°„ë‹¨í•œ ìœ„í—˜ì¡°ì •ìˆ˜ìµë¥  (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê³„ì‚° í•„ìš”)
            self.df['risk_adjusted_return'] = self.df['expected_return_rate'] / (self.df['grade_risk_score'] + 1e-8)
            print("  âœ“ risk_adjusted_return ìƒì„±")
        
        print(f"\nâœ“ ê¸ˆìœµ ëª¨ë¸ë§ ì „ìš© íŠ¹ì„± ìƒì„± ì™„ë£Œ")
        return True
    
    def remove_unnecessary_features(self):
        start_time = time.time()
        print("\nğŸ—‘ï¸ ëª¨ë¸ë§ì— ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±°")
        print("-" * 40)
        
        # ì œê±°í•  íŠ¹ì„±ë“¤ ì •ì˜ (ìˆ˜ì •ë¨ - ì¤‘ìš” íŠ¹ì„± ë³´ì¡´)
        unnecessary_features = [
            # ì‹ë³„ì/ë©”íƒ€ë°ì´í„°
            'id', 'url', 'title', 'zip_code',
            
            # í…ìŠ¤íŠ¸ ë°ì´í„° (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¼)
            'emp_title', 'desc',
            
            # ì¤‘ë³µë˜ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ (ì¼ë¶€ ë³´ì¡´)
            'verification_status_joint',
            'hardship_reason', 'hardship_type', 'hardship_status',
            
            # ì›ë³¸ ë‚ ì§œ ë¬¸ìì—´ (ì´ë¯¸ ì²˜ë¦¬ëœ ë‚ ì§œ íŠ¹ì„±ìœ¼ë¡œ ëŒ€ì²´)
            'issue_d', 'earliest_cr_line', 'last_pymnt_d',
            'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date',
            
            # ì¤‘ë³µë˜ëŠ” FICO íŠ¹ì„±ë“¤ (fico_avgë¡œ ëŒ€ì²´)
            'fico_range_high', 'sec_app_fico_range_low',
            
            # ê³µë™ì‹ ì²­ì¸ ê´€ë ¨ (ëŒ€ë¶€ë¶„ ê²°ì¸¡ì¹˜)
            'sec_app_fico_range_high', 'sec_app_earliest_cr_line',
            'sec_app_inq_last_6mths', 'sec_app_mort_acc',
            'sec_app_open_acc', 'sec_app_revol_util',
            'sec_app_open_act_il', 'sec_app_num_rev_accts',
            'sec_app_chargeoff_within_12_mths', 'sec_app_collections_12_mths_ex_med',
            
            # ì–´ë ¤ì›€ ëŒ€ì¶œ ê´€ë ¨ (ëŒ€ë¶€ë¶„ ê²°ì¸¡ì¹˜)
            'deferral_term', 'hardship_amount', 'hardship_length',
            'hardship_dpd', 'hardship_loan_status',
            'orig_projected_additional_accrued_interest',
            'hardship_payoff_balance_amount', 'hardship_last_payment_amount'
        ]
        
        # ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” íŠ¹ì„±ë“¤ë§Œ í•„í„°ë§
        existing_unnecessary = [feature for feature in unnecessary_features if feature in self.df.columns]
        
        if existing_unnecessary:
            print(f"ì œê±°í•  íŠ¹ì„± ({len(existing_unnecessary)}ê°œ):")
            for feature in existing_unnecessary:
                print(f"  - {feature}")
            
            # íŠ¹ì„± ì œê±°
            self.df = self.df.drop(columns=existing_unnecessary)
            print(f"âœ“ ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±° ì™„ë£Œ: {len(existing_unnecessary)}ê°œ")
        else:
            print("âœ“ ì œê±°í•  ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤")
        
        print(f"âœ“ ë‚¨ì€ íŠ¹ì„± ìˆ˜: {len(self.df.columns)}ê°œ")
        
        self.execution_times['remove_unnecessary_features'] = time.time() - start_time
        return True
    
    def run_pipeline(self, create_clean=True):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        print("=" * 80)
        print("í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        print("=" * 80)
        
        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë“¤
        pipeline_steps = [
            ("ë°ì´í„° ë¡œë”©", self.load_data),
            ("ì´ìƒì¹˜ í–‰ ì œê±°", self.remove_anomalous_rows),
            ("íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±", self.create_target_variable),
            ("ë¬¸ìì—´ ë°ì´í„° ì •ë¦¬", self.clean_percentage_columns),
            ("ë†’ì€ ê²°ì¸¡ì¹˜ íŠ¹ì„± ì²˜ë¦¬", self.handle_high_missing_features),
            ("FICO íŠ¹ì„± ìƒì„±", self.create_fico_features),
            ("ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©", self.enhanced_categorical_encoding),
            ("ì´ìƒì¹˜ ì²˜ë¦¬", self.handle_outliers),
            ("ì£¼ ë°ì´í„° ìµœì í™”", self.optimize_state_encoding),
            ("ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ê°•í™”", self.enhance_time_based_features),
            ("ê³ ê¸‰ ë³µí•© íŠ¹ì„± ìƒì„±", self.create_advanced_composite_features),
            ("ê¸ˆìœµ íŠ¹ì„± ë³´ì¡´", self.preserve_financial_features),
            ("ê¸ˆìœµ ëª¨ë¸ë§ íŠ¹ì„± ìƒì„±", self.create_financial_features),
            ("ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±°", self.remove_unnecessary_features),
            ("íŠ¹ì„± ì„ íƒ ê°œì„ ", self.improve_feature_selection),
            ("í†µê³„ì  ê²€ì¦", self.statistical_validation),
            ("ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±", self.create_visualization_report),
            ("ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±", self.create_validation_report),
            ("ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥", self.save_processed_data)
        ]
        
        # ê¹”ë”í•œ ë²„ì „ ìƒì„± ì˜µì…˜
        if create_clean:
            pipeline_steps.append(("ê¹”ë”í•œ ë²„ì „ ìƒì„±", self.create_clean_version))
        
        # ê° ë‹¨ê³„ ì‹¤í–‰
        for step_name, step_func in pipeline_steps:
            step_start = time.time()
            print(f"\nğŸ”„ {step_name}...")
            
            try:
                step_func()
                step_time = time.time() - step_start
                self.execution_times[step_name] = step_time
                print(f"âœ… {step_name} ì™„ë£Œ ({step_time:.2f}ì´ˆ)")
                
            except Exception as e:
                print(f"âŒ {step_name} ì‹¤íŒ¨: {str(e)}")
                return False
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! (ì´ {total_time:.2f}ì´ˆ)")
        
        return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ì™„ì „ ë²„ì „)")
    
    # íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    pipeline = IntegratedPreprocessingPipeline()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    success = pipeline.run_pipeline()
    
    if success:
        print("\nâœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“Š ëª¨ë“  ê°œì„ ì‚¬í•­ì´ ì ìš©ëœ ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 