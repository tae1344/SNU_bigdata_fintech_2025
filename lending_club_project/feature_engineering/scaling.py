#!/usr/bin/env python3
"""
ìŠ¤ì¼€ì¼ë§ ëª¨ë“ˆ
ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ í‘œì¤€í™”ì™€ ì •ê·œí™”ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import sys
import os
import warnings
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from config.file_paths import (
    ENCODED_DATA_PATH,
    NEW_FEATURES_DATA_PATH,
    SCALED_STANDARD_DATA_PATH,
    SCALED_MINMAX_DATA_PATH,
    REPORTS_DIR,
    ensure_directory_exists,
    file_exists,
    get_reports_file_path
)

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS ê¸°ì¤€)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

def identify_numeric_columns(df):
    """
    ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
    
    Returns:
    --------
    list
        ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    """
    print("\n[ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì‹ë³„]")
    print("-" * 40)
    
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì‹ë³„
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    
    print(f"ì´ ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ")
    print("ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ë³€ìˆ˜:")
    for i, col in enumerate(numeric_cols, 1):
        print(f"  {i:2d}. {col}")
    
    return numeric_cols

def apply_standard_scaling(X_train, X_val, numeric_cols):
    """
    í‘œì¤€í™”(StandardScaler)ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame
        í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val : pandas.DataFrame
        ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    numeric_cols : list
        ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
    --------
    tuple: (pandas.DataFrame, pandas.DataFrame, StandardScaler)
        í‘œì¤€í™”ëœ í›ˆë ¨/ê²€ì¦ ë°ì´í„°í”„ë ˆì„ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
    """
    print("\n[í‘œì¤€í™”(StandardScaler) ì ìš©]")
    print("-" * 40)
    
    if len(numeric_cols) == 0:
        print("âš ï¸ ê²½ê³ : ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return X_train.copy(), X_val.copy(), None
    
    try:
        scaler_std = StandardScaler()
        
        # í›ˆë ¨ ë°ì´í„°ë¡œë§Œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        X_train_std = X_train.copy()
        X_train_std[numeric_cols] = scaler_std.fit_transform(X_train[numeric_cols])
        
        # ê²€ì¦ ë°ì´í„°ì— í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
        X_val_std = X_val.copy()
        X_val_std[numeric_cols] = scaler_std.transform(X_val[numeric_cols])
        
        print("âœ“ í‘œì¤€í™” ì™„ë£Œ")
        print(f"  í›ˆë ¨ ë°ì´í„°: {X_train_std.shape}")
        print(f"  ê²€ì¦ ë°ì´í„°: {X_val_std.shape}")
        
        # í‘œì¤€í™” ê²°ê³¼ ê²€ì¦ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€)
        print("\ní‘œì¤€í™” ê²°ê³¼ ê²€ì¦ (í›ˆë ¨ ë°ì´í„°):")
        for col in numeric_cols:
            mean_val = X_train_std[col].mean()
            std_val = X_train_std[col].std()
            print(f"  {col}: í‰ê· ={mean_val:.6f}, í‘œì¤€í¸ì°¨={std_val:.6f}")
            
        return X_train_std, X_val_std, scaler_std
        
    except Exception as e:
        print(f"âœ— í‘œì¤€í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return X_train.copy(), X_val.copy(), None

def apply_minmax_scaling(X_train, X_val, numeric_cols):
    """
    ì •ê·œí™”(MinMaxScaler)ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    X_train : pandas.DataFrame
        í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val : pandas.DataFrame
        ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    numeric_cols : list
        ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
    --------
    tuple: (pandas.DataFrame, pandas.DataFrame, MinMaxScaler)
        ì •ê·œí™”ëœ í›ˆë ¨/ê²€ì¦ ë°ì´í„°í”„ë ˆì„ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
    """
    print("\n[ì •ê·œí™”(MinMaxScaler) ì ìš©]")
    print("-" * 40)
    
    if len(numeric_cols) == 0:
        print("âš ï¸ ê²½ê³ : ìŠ¤ì¼€ì¼ë§í•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return X_train.copy(), X_val.copy(), None
    
    try:
        scaler_minmax = MinMaxScaler()
        
        # í›ˆë ¨ ë°ì´í„°ë¡œë§Œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
        X_train_minmax = X_train.copy()
        X_train_minmax[numeric_cols] = scaler_minmax.fit_transform(X_train[numeric_cols])
        
        # ê²€ì¦ ë°ì´í„°ì— í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
        X_val_minmax = X_val.copy()
        X_val_minmax[numeric_cols] = scaler_minmax.transform(X_val[numeric_cols])
        
        print("âœ“ ì •ê·œí™” ì™„ë£Œ")
        print(f"  í›ˆë ¨ ë°ì´í„°: {X_train_minmax.shape}")
        print(f"  ê²€ì¦ ë°ì´í„°: {X_val_minmax.shape}")
        
        # ì •ê·œí™” ê²°ê³¼ ê²€ì¦ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€)
        print("\nì •ê·œí™” ê²°ê³¼ ê²€ì¦ (í›ˆë ¨ ë°ì´í„°):")
        for col in numeric_cols:
            min_val = X_train_minmax[col].min()
            max_val = X_train_minmax[col].max()
            print(f"  {col}: ìµœì†Œê°’={min_val:.6f}, ìµœëŒ€ê°’={max_val:.6f}")
            
        return X_train_minmax, X_val_minmax, scaler_minmax
        
    except Exception as e:
        print(f"âœ— ì •ê·œí™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return X_train.copy(), X_val.copy(), None

def save_scaled_data(X_train_std, X_val_std, X_train_minmax, X_val_minmax, scaler_std, scaler_minmax):
    """
    ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    X_train_std : pandas.DataFrame
        í‘œì¤€í™”ëœ í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val_std : pandas.DataFrame
        í‘œì¤€í™”ëœ ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    X_train_minmax : pandas.DataFrame
        ì •ê·œí™”ëœ í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val_minmax : pandas.DataFrame
        ì •ê·œí™”ëœ ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    scaler_std : StandardScaler
        í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
    scaler_minmax : MinMaxScaler
        ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
    """
    print("\n[ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì €ì¥]")
    print("-" * 40)
    
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        ensure_directory_exists(SCALED_STANDARD_DATA_PATH.parent)
        ensure_directory_exists(SCALED_MINMAX_DATA_PATH.parent)
        
        # í‘œì¤€í™” ë°ì´í„° ì €ì¥
        X_train_std.to_csv(SCALED_STANDARD_DATA_PATH, index=False)
        X_val_std.to_csv(SCALED_STANDARD_DATA_PATH.parent / "validation_scaled_standard.csv", index=False)
        
        # ì •ê·œí™” ë°ì´í„° ì €ì¥
        X_train_minmax.to_csv(SCALED_MINMAX_DATA_PATH, index=False)
        X_val_minmax.to_csv(SCALED_MINMAX_DATA_PATH.parent / "validation_scaled_minmax.csv", index=False)
        
        print(f"âœ“ í‘œì¤€í™” í›ˆë ¨ ë°ì´í„° ì €ì¥: {SCALED_STANDARD_DATA_PATH}")
        print(f"âœ“ í‘œì¤€í™” ê²€ì¦ ë°ì´í„° ì €ì¥: validation_scaled_standard.csv")
        print(f"âœ“ ì •ê·œí™” í›ˆë ¨ ë°ì´í„° ì €ì¥: {SCALED_MINMAX_DATA_PATH}")
        print(f"âœ“ ì •ê·œí™” ê²€ì¦ ë°ì´í„° ì €ì¥: validation_scaled_minmax.csv")
        
        # ì €ì¥ëœ íŒŒì¼ í¬ê¸° í™•ì¸
        import os
        train_std_size = os.path.getsize(SCALED_STANDARD_DATA_PATH) / (1024 * 1024)  # MB
        val_std_size = os.path.getsize(SCALED_STANDARD_DATA_PATH.parent / "validation_scaled_standard.csv") / (1024 * 1024)
        train_minmax_size = os.path.getsize(SCALED_MINMAX_DATA_PATH) / (1024 * 1024)  # MB
        val_minmax_size = os.path.getsize(SCALED_MINMAX_DATA_PATH.parent / "validation_scaled_minmax.csv") / (1024 * 1024)
        
        print(f"  í‘œì¤€í™” í›ˆë ¨ íŒŒì¼ í¬ê¸°: {train_std_size:.2f} MB")
        print(f"  í‘œì¤€í™” ê²€ì¦ íŒŒì¼ í¬ê¸°: {val_std_size:.2f} MB")
        print(f"  ì •ê·œí™” í›ˆë ¨ íŒŒì¼ í¬ê¸°: {train_minmax_size:.2f} MB")
        print(f"  ì •ê·œí™” ê²€ì¦ íŒŒì¼ í¬ê¸°: {val_minmax_size:.2f} MB")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ëŠ” save_scaling_info í•¨ìˆ˜ì—ì„œ ì €ì¥ë¨
        
    except Exception as e:
        print(f"âœ— íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def save_scaling_info(scaler_std, scaler_minmax, numeric_cols, df_original, X_train_std, X_val_std, X_train_minmax, X_val_minmax):
    """
    ìŠ¤ì¼€ì¼ë§ ì •ë³´ë¥¼ ìƒì„¸íˆ ì €ì¥í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    scaler_std : StandardScaler
        í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
    scaler_minmax : MinMaxScaler
        ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
    numeric_cols : list
        ìŠ¤ì¼€ì¼ë§ëœ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    df_original : pandas.DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    X_train_std : pandas.DataFrame
        í‘œì¤€í™”ëœ í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val_std : pandas.DataFrame
        í‘œì¤€í™”ëœ ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    X_train_minmax : pandas.DataFrame
        ì •ê·œí™”ëœ í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val_minmax : pandas.DataFrame
        ì •ê·œí™”ëœ ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    """
    print("\n[ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì €ì¥]")
    print("-" * 40)
    
    try:
        import json
        import pickle
        from datetime import datetime
        
        # ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì €ì¥ ë””ë ‰í† ë¦¬
        scaling_info_dir = SCALED_STANDARD_DATA_PATH.parent / "scaling_info"
        ensure_directory_exists(scaling_info_dir)
        
        # 1. í‘œì¤€í™” ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì €ì¥
        if scaler_std is not None:
            std_info = {
                "scaler_type": "StandardScaler",
                "created_at": datetime.now().isoformat(),
                "variables": numeric_cols,
                "scaler_params": {
                    "mean_": scaler_std.mean_.tolist(),
                    "scale_": scaler_std.scale_.tolist(),
                    "var_": scaler_std.var_.tolist(),
                    "n_samples_seen_": int(scaler_std.n_samples_seen_)
                },
                "original_statistics": {},
                "scaled_statistics": {}
            }
            
            # ì›ë³¸ í†µê³„ ì •ë³´
            for i, col in enumerate(numeric_cols):
                if col in df_original.columns:
                    std_info["original_statistics"][col] = {
                        "mean": float(df_original[col].mean()),
                        "std": float(df_original[col].std()),
                        "min": float(df_original[col].min()),
                        "max": float(df_original[col].max()),
                        "median": float(df_original[col].median()),
                        "q25": float(df_original[col].quantile(0.25)),
                        "q75": float(df_original[col].quantile(0.75))
                    }
            
            # ìŠ¤ì¼€ì¼ë§ëœ í†µê³„ ì •ë³´ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€)
            for col in numeric_cols:
                if col in X_train_std.columns:
                    std_info["scaled_statistics"][col] = {
                        "mean": float(X_train_std[col].mean()),
                        "std": float(X_train_std[col].std()),
                        "min": float(X_train_std[col].min()),
                        "max": float(X_train_std[col].max()),
                        "median": float(X_train_std[col].median()),
                        "q25": float(X_train_std[col].quantile(0.25)),
                        "q75": float(X_train_std[col].quantile(0.75))
                    }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            std_info_path = scaling_info_dir / "standard_scaling_info.json"
            with open(std_info_path, 'w', encoding='utf-8') as f:
                json.dump(std_info, f, indent=2, ensure_ascii=False)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ ì €ì¥
            std_scaler_path = scaling_info_dir / "standard_scaler.pkl"
            with open(std_scaler_path, 'wb') as f:
                pickle.dump(scaler_std, f)
            
            print(f"âœ“ í‘œì¤€í™” ì •ë³´ ì €ì¥: {std_info_path}")
            print(f"âœ“ í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {std_scaler_path}")
        
        # 2. ì •ê·œí™” ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì €ì¥
        if scaler_minmax is not None:
            minmax_info = {
                "scaler_type": "MinMaxScaler",
                "created_at": datetime.now().isoformat(),
                "variables": numeric_cols,
                "scaler_params": {
                    "min_": scaler_minmax.min_.tolist(),
                    "scale_": scaler_minmax.scale_.tolist(),
                    "data_min_": scaler_minmax.data_min_.tolist(),
                    "data_max_": scaler_minmax.data_max_.tolist(),
                    "data_range_": scaler_minmax.data_range_.tolist(),
                    "n_samples_seen_": int(scaler_minmax.n_samples_seen_)
                },
                "original_statistics": {},
                "scaled_statistics": {}
            }
            
            # ì›ë³¸ í†µê³„ ì •ë³´
            for col in numeric_cols:
                if col in df_original.columns:
                    minmax_info["original_statistics"][col] = {
                        "mean": float(df_original[col].mean()),
                        "std": float(df_original[col].std()),
                        "min": float(df_original[col].min()),
                        "max": float(df_original[col].max()),
                        "median": float(df_original[col].median()),
                        "q25": float(df_original[col].quantile(0.25)),
                        "q75": float(df_original[col].quantile(0.75))
                    }
            
            # ìŠ¤ì¼€ì¼ë§ëœ í†µê³„ ì •ë³´ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€)
            for col in numeric_cols:
                if col in X_train_minmax.columns:
                    minmax_info["scaled_statistics"][col] = {
                        "mean": float(X_train_minmax[col].mean()),
                        "std": float(X_train_minmax[col].std()),
                        "min": float(X_train_minmax[col].min()),
                        "max": float(X_train_minmax[col].max()),
                        "median": float(X_train_minmax[col].median()),
                        "q25": float(X_train_minmax[col].quantile(0.25)),
                        "q75": float(X_train_minmax[col].quantile(0.75))
                    }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            minmax_info_path = scaling_info_dir / "minmax_scaling_info.json"
            with open(minmax_info_path, 'w', encoding='utf-8') as f:
                json.dump(minmax_info, f, indent=2, ensure_ascii=False)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ ì €ì¥
            minmax_scaler_path = scaling_info_dir / "minmax_scaler.pkl"
            with open(minmax_scaler_path, 'wb') as f:
                pickle.dump(scaler_minmax, f)
            
            print(f"âœ“ ì •ê·œí™” ì •ë³´ ì €ì¥: {minmax_info_path}")
            print(f"âœ“ ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {minmax_scaler_path}")
        
        # 3. í†µí•© ìŠ¤ì¼€ì¼ë§ ìš”ì•½ ì •ë³´ ì €ì¥
        summary_info = {
            "scaling_summary": {
                "created_at": datetime.now().isoformat(),
                "total_variables": len(numeric_cols),
                "variables_scaled": numeric_cols,
                "train_data_shape": X_train_std.shape if scaler_std else None,
                "val_data_shape": X_val_std.shape if scaler_std else None,
                "scaling_methods": []
            },
            "file_paths": {
                "standard_scaled_train": str(SCALED_STANDARD_DATA_PATH),
                "standard_scaled_val": str(SCALED_STANDARD_DATA_PATH.parent / "validation_scaled_standard.csv"),
                "minmax_scaled_train": str(SCALED_MINMAX_DATA_PATH),
                "minmax_scaled_val": str(SCALED_MINMAX_DATA_PATH.parent / "validation_scaled_minmax.csv"),
                "scaling_info_directory": str(scaling_info_dir)
            }
        }
        
        if scaler_std:
            summary_info["scaling_summary"]["scaling_methods"].append("StandardScaler")
        if scaler_minmax:
            summary_info["scaling_summary"]["scaling_methods"].append("MinMaxScaler")
        
        # ìš”ì•½ ì •ë³´ ì €ì¥
        summary_path = scaling_info_dir / "scaling_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ ìŠ¤ì¼€ì¼ë§ ìš”ì•½ ì •ë³´ ì €ì¥: {summary_path}")
        
        # 4. ìŠ¤ì¼€ì¼ë§ ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
        validation_report = {
            "scaling_validation": {
                "created_at": datetime.now().isoformat(),
                "standard_scaling_validation": {},
                "minmax_scaling_validation": {}
            }
        }
        
        # í‘œì¤€í™” ê²€ì¦
        if scaler_std:
            for col in numeric_cols:
                if col in X_train_std.columns:
                    validation_report["scaling_validation"]["standard_scaling_validation"][col] = {
                        "original_mean": float(df_original[col].mean()),
                        "original_std": float(df_original[col].std()),
                        "scaled_mean": float(X_train_std[col].mean()),
                        "scaled_std": float(X_train_std[col].std()),
                        "mean_difference": abs(float(X_train_std[col].mean())),
                        "std_difference": abs(float(X_train_std[col].std()) - 1.0)
                    }
        
        # ì •ê·œí™” ê²€ì¦
        if scaler_minmax:
            for col in numeric_cols:
                if col in X_train_minmax.columns:
                    validation_report["scaling_validation"]["minmax_scaling_validation"][col] = {
                        "original_min": float(df_original[col].min()),
                        "original_max": float(df_original[col].max()),
                        "scaled_min": float(X_train_minmax[col].min()),
                        "scaled_max": float(X_train_minmax[col].max()),
                        "min_difference": abs(float(X_train_minmax[col].min())),
                        "max_difference": abs(float(X_train_minmax[col].max()) - 1.0)
                    }
        
        # ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥
        validation_path = scaling_info_dir / "scaling_validation.json"
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation_report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ ìŠ¤ì¼€ì¼ë§ ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥: {validation_path}")
        
        # 5. ì‚¬ìš©ë²• ê°€ì´ë“œ ìƒì„±
        usage_guide = f"""
# ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì‚¬ìš©ë²•

## ì €ì¥ëœ íŒŒì¼ë“¤
- `standard_scaling_info.json`: í‘œì¤€í™” ìƒì„¸ ì •ë³´
- `minmax_scaling_info.json`: ì •ê·œí™” ìƒì„¸ ì •ë³´
- `scaling_summary.json`: ì „ì²´ ìš”ì•½ ì •ë³´
- `scaling_validation.json`: ê²€ì¦ ê²°ê³¼
- `standard_scaler.pkl`: í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
- `minmax_scaler.pkl`: ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´

## ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë°©ë²•
```python
import pickle
import json

# ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ ë¡œë“œ
with open('scaling_info/standard_scaler.pkl', 'rb') as f:
    scaler_std = pickle.load(f)

# ìŠ¤ì¼€ì¼ë§ ì •ë³´ ë¡œë“œ
with open('scaling_info/standard_scaling_info.json', 'r') as f:
    scaling_info = json.load(f)

# ìƒˆë¡œìš´ ë°ì´í„°ì— ìŠ¤ì¼€ì¼ë§ ì ìš©
new_data_scaled = scaler_std.transform(new_data)
```

## ê²€ì¦ ë°©ë²•
- í‘œì¤€í™”ëœ ë°ì´í„°ì˜ í‰ê· ì€ 0ì— ê°€ê¹Œì›Œì•¼ í•¨
- í‘œì¤€í™”ëœ ë°ì´í„°ì˜ í‘œì¤€í¸ì°¨ëŠ” 1ì— ê°€ê¹Œì›Œì•¼ í•¨
- ì •ê·œí™”ëœ ë°ì´í„°ì˜ ìµœì†Œê°’ì€ 0, ìµœëŒ€ê°’ì€ 1ì´ì–´ì•¼ í•¨
"""
        
        guide_path = scaling_info_dir / "README.md"
        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write(usage_guide)
        
        print(f"âœ“ ì‚¬ìš©ë²• ê°€ì´ë“œ ì €ì¥: {guide_path}")
        
    except Exception as e:
        print(f"âœ— ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def create_scaling_report(df_original, X_train_std, X_val_std, X_train_minmax, X_val_minmax, numeric_cols):
    """
    ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    df_original : pandas.DataFrame
        ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    X_train_std : pandas.DataFrame
        í‘œì¤€í™”ëœ í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val_std : pandas.DataFrame
        í‘œì¤€í™”ëœ ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    X_train_minmax : pandas.DataFrame
        ì •ê·œí™”ëœ í›ˆë ¨ ë°ì´í„°í”„ë ˆì„
    X_val_minmax : pandas.DataFrame
        ì •ê·œí™”ëœ ê²€ì¦ ë°ì´í„°í”„ë ˆì„
    numeric_cols : list
        ìŠ¤ì¼€ì¼ë§ëœ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    """
    print("\n[ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±]")
    print("-" * 40)
    
    report_path = get_reports_file_path("scaling_report.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
        f.write("=" * 50 + "\n\n")
        
        # ê¸°ë³¸ ì •ë³´
        f.write("1. ìŠ¤ì¼€ì¼ë§ ê¸°ë³¸ ì •ë³´\n")
        f.write("-" * 20 + "\n")
        f.write(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df_original.shape}\n")
        f.write(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train_std.shape}\n")
        f.write(f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_val_std.shape}\n")
        f.write(f"ìŠ¤ì¼€ì¼ë§ëœ ë³€ìˆ˜ ìˆ˜: {len(numeric_cols)}\n")
        f.write(f"ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ë³€ìˆ˜: {numeric_cols}\n\n")
        
        # í‘œì¤€í™” ê²°ê³¼
        f.write("2. í‘œì¤€í™”(StandardScaler) ê²°ê³¼\n")
        f.write("-" * 30 + "\n")
        for col in numeric_cols:
            if col in df_original.columns and col in X_train_std.columns:
                original_mean = df_original[col].mean()
                original_std = df_original[col].std()
                train_scaled_mean = X_train_std[col].mean()
                train_scaled_std = X_train_std[col].std()
                val_scaled_mean = X_val_std[col].mean()
                val_scaled_std = X_val_std[col].std()
                
                f.write(f"{col}:\n")
                f.write(f"  ì›ë³¸ - í‰ê· : {original_mean:.4f}, í‘œì¤€í¸ì°¨: {original_std:.4f}\n")
                f.write(f"  í›ˆë ¨ í‘œì¤€í™” - í‰ê· : {train_scaled_mean:.6f}, í‘œì¤€í¸ì°¨: {train_scaled_std:.6f}\n")
                f.write(f"  ê²€ì¦ í‘œì¤€í™” - í‰ê· : {val_scaled_mean:.6f}, í‘œì¤€í¸ì°¨: {val_scaled_std:.6f}\n\n")
        
        # ì •ê·œí™” ê²°ê³¼
        f.write("3. ì •ê·œí™”(MinMaxScaler) ê²°ê³¼\n")
        f.write("-" * 30 + "\n")
        for col in numeric_cols:
            if col in df_original.columns and col in X_train_minmax.columns:
                original_min = df_original[col].min()
                original_max = df_original[col].max()
                train_scaled_min = X_train_minmax[col].min()
                train_scaled_max = X_train_minmax[col].max()
                val_scaled_min = X_val_minmax[col].min()
                val_scaled_max = X_val_minmax[col].max()
                
                f.write(f"{col}:\n")
                f.write(f"  ì›ë³¸ - ìµœì†Œê°’: {original_min:.4f}, ìµœëŒ€ê°’: {original_max:.4f}\n")
                f.write(f"  í›ˆë ¨ ì •ê·œí™” - ìµœì†Œê°’: {train_scaled_min:.6f}, ìµœëŒ€ê°’: {train_scaled_max:.6f}\n")
                f.write(f"  ê²€ì¦ ì •ê·œí™” - ìµœì†Œê°’: {val_scaled_min:.6f}, ìµœëŒ€ê°’: {val_scaled_max:.6f}\n\n")
        
        # íŒŒì¼ ì •ë³´
        f.write("4. ì €ì¥ëœ íŒŒì¼ ì •ë³´\n")
        f.write("-" * 20 + "\n")
        f.write(f"í‘œì¤€í™” í›ˆë ¨ ë°ì´í„°: {SCALED_STANDARD_DATA_PATH}\n")
        f.write(f"í‘œì¤€í™” ê²€ì¦ ë°ì´í„°: validation_scaled_standard.csv\n")
        f.write(f"ì •ê·œí™” í›ˆë ¨ ë°ì´í„°: {SCALED_MINMAX_DATA_PATH}\n")
        f.write(f"ì •ê·œí™” ê²€ì¦ ë°ì´í„°: validation_scaled_minmax.csv\n")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        import os
        if os.path.exists(SCALED_STANDARD_DATA_PATH):
            train_std_size = os.path.getsize(SCALED_STANDARD_DATA_PATH) / (1024 * 1024)
            f.write(f"í‘œì¤€í™” í›ˆë ¨ íŒŒì¼ í¬ê¸°: {train_std_size:.2f} MB\n")
        
        if os.path.exists(SCALED_STANDARD_DATA_PATH.parent / "validation_scaled_standard.csv"):
            val_std_size = os.path.getsize(SCALED_STANDARD_DATA_PATH.parent / "validation_scaled_standard.csv") / (1024 * 1024)
            f.write(f"í‘œì¤€í™” ê²€ì¦ íŒŒì¼ í¬ê¸°: {val_std_size:.2f} MB\n")
        
        if os.path.exists(SCALED_MINMAX_DATA_PATH):
            train_minmax_size = os.path.getsize(SCALED_MINMAX_DATA_PATH) / (1024 * 1024)
            f.write(f"ì •ê·œí™” í›ˆë ¨ íŒŒì¼ í¬ê¸°: {train_minmax_size:.2f} MB\n")
        
        if os.path.exists(SCALED_MINMAX_DATA_PATH.parent / "validation_scaled_minmax.csv"):
            val_minmax_size = os.path.getsize(SCALED_MINMAX_DATA_PATH.parent / "validation_scaled_minmax.csv") / (1024 * 1024)
            f.write(f"ì •ê·œí™” ê²€ì¦ íŒŒì¼ í¬ê¸°: {val_minmax_size:.2f} MB\n")
    
    print(f"âœ“ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ“Š ìŠ¤ì¼€ì¼ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[ë°ì´í„° ë¡œë“œ]")
    print("-" * 40)

    # ************* ë°ì´í„° ê²½ë¡œ ì„¤ì • *************
    DATA_PATH = ENCODED_DATA_PATH  # ì •ì œ + ìƒˆ íŠ¹ì„± ìƒì„± + ì¸ì½”ë”© ëœ ë°ì´í„° ê²½ë¡œ
    
    if file_exists(DATA_PATH):
        data_path = DATA_PATH
        print("ì •ì œ + ìƒˆ íŠ¹ì„± ìƒì„±ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    if not file_exists(data_path):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        print("ë¨¼ì € ë°ì´í„° ì •ì œ ë˜ëŠ” ì¸ì½”ë”©ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return False
    
    try:
        df = pd.read_csv(data_path)
        print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    # ì›ë³¸ ë°ì´í„° ë°±ì—…
    df_original = df.copy()

    # Train/Validation ë¶„í•  (ë°ì´í„° ë¶„í•  í›„ ìŠ¤ì¼€ì¼ë§ ì ìš©)
    print("\n[Train/Validation ë¶„í• ]")
    print("-" * 40)
    
    from sklearn.model_selection import train_test_split
    
    X = df.drop('target', axis=1)
    y = df['target']
    
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    print(f"âœ“ ë°ì´í„° ë¶„í•  ì™„ë£Œ")
    print(f"  í›ˆë ¨ ë°ì´í„°: {X_train.shape}")
    print(f"  ê²€ì¦ ë°ì´í„°: {X_val.shape}")
    print(f"  í›ˆë ¨ íƒ€ê²Ÿ ë¶„í¬: {y_train.value_counts().to_dict()}")
    print(f"  ê²€ì¦ íƒ€ê²Ÿ ë¶„í¬: {y_val.value_counts().to_dict()}")
    
    # 2. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì‹ë³„
    numeric_cols = identify_numeric_columns(X_train)
    
    # 3. í‘œì¤€í™” ì ìš©
    X_train_std, X_val_std, scaler_std = apply_standard_scaling(X_train, X_val, numeric_cols)
    
    # 4. ì •ê·œí™” ì ìš©
    X_train_minmax, X_val_minmax, scaler_minmax = apply_minmax_scaling(X_train, X_val, numeric_cols)
    
    # 5. ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì €ì¥
    save_scaled_data(X_train_std, X_val_std, X_train_minmax, X_val_minmax, scaler_std, scaler_minmax)
    
    # 6. ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì €ì¥
    save_scaling_info(scaler_std, scaler_minmax, numeric_cols, df_original, X_train_std, X_val_std, X_train_minmax, X_val_minmax)

    # 7. ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
    create_scaling_report(df_original, X_train_std, X_val_std, X_train_minmax, X_val_minmax, numeric_cols)
    
    print(f"\nâœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ!")
    print(f"ğŸ“ í‘œì¤€í™” ë°ì´í„°: {SCALED_STANDARD_DATA_PATH}")
    print(f"ğŸ“ ì •ê·œí™” ë°ì´í„°: {SCALED_MINMAX_DATA_PATH}")
    print(f"ğŸ“ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼ ë¦¬í¬íŠ¸: {REPORTS_DIR}")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 