"""
ê¸°ë³¸ ëª¨ë¸ êµ¬í˜„ ìŠ¤í¬ë¦½íŠ¸
ë¡œì§€ìŠ¤í‹± íšŒê·€, ëœë¤í¬ë ˆìŠ¤íŠ¸, XGBoost, LightGBM ëª¨ë¸ì„ êµ¬í˜„
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI ì°½ì´ ì—´ë¦¬ì§€ ì•Šë„ë¡ ì„¤ì •
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
# XGBoostì™€ LightGBM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. XGBoost ëª¨ë¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LightGBM ëª¨ë¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

import warnings
import sys
import os
from pathlib import Path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.file_paths import (
    SELECTED_FEATURES_PATH,
    SCALED_STANDARD_DATA_PATH,
    BASIC_MODELS_REPORT_PATH,
    ensure_directory_exists,
    file_exists
)
from config.settings import settings

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class BasicModels:
    """ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.results = {}
        self.feature_importance = {}
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì„ íƒëœ íŠ¹ì„± ë¡œë“œ
        if not file_exists(SELECTED_FEATURES_PATH):
            print(f"âœ— ì„ íƒëœ íŠ¹ì„± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SELECTED_FEATURES_PATH}")
            print("ë¨¼ì € feature_selection_analysis.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
            
        selected_features_df = pd.read_csv(SELECTED_FEATURES_PATH)
        selected_features = selected_features_df['selected_feature'].tolist()
        
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ë¡œë“œ
        if not file_exists(SCALED_STANDARD_DATA_PATH):
            print(f"âœ— ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SCALED_STANDARD_DATA_PATH}")
            print("ë¨¼ì € feature_engineering_step2_scaling.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
            
        df = pd.read_csv(SCALED_STANDARD_DATA_PATH)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        df['loan_status_binary'] = df['loan_status'].apply(
            lambda x: 1 if x in ['Default', 'Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'] else 0
        )
        
        # ì„ íƒëœ íŠ¹ì„±ë§Œ ì‚¬ìš©
        available_features = [f for f in selected_features if f in df.columns]
        print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {len(available_features)}ê°œ")
        
        X = df[available_features]
        y = df['loan_status_binary']
        
        # ê²°ì¸¡ì¹˜ í™•ì¸ (ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨)
        total_missing = X.isnull().sum().sum()
        if total_missing > 0:
            print(f"âš ï¸ ê²½ê³ : {total_missing}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("   feature_engineering_step2_scaling.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return None
        else:
            print("âœ“ ê²°ì¸¡ì¹˜ ì—†ìŒ - ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©")
        
        # Train/Test Split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state, stratify=y
        )
        
        print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"  - í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ")
        print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ")
        print(f"  - íŠ¹ì„± ìˆ˜: {X_train.shape[1]}ê°œ")
        
        return X_train, X_test, y_train, y_test, available_features
    
    def train_logistic_regression(self, X_train, y_train, X_test, y_test):
        """ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # ëª¨ë¸ ì •ì˜
        lr_model = LogisticRegression(
            random_state=self.random_state,
            max_iter=1000,
            class_weight='balanced'
        )
        
        # ëª¨ë¸ í›ˆë ¨
        lr_model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = lr_model.predict(X_test)
        y_pred_proba = lr_model.predict_proba(X_test)[:, 1]
        
        # ì„±ëŠ¥ í‰ê°€
        accuracy = lr_model.score(X_test, y_test)
        auc = roc_auc_score(y_test, y_pred_proba)
        
        # íŠ¹ì„± ì¤‘ìš”ë„ (ê³„ìˆ˜ ì ˆëŒ“ê°’)
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'coefficient': np.abs(lr_model.coef_[0])
        }).sort_values('coefficient', ascending=False)
        
        # ê²°ê³¼ ì €ì¥
        self.models['logistic_regression'] = lr_model
        self.results['logistic_regression'] = {
            'accuracy': accuracy,
            'auc': auc,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        self.feature_importance['logistic_regression'] = feature_importance
        
        print(f"âœ“ ë¡œì§€ìŠ¤í‹± íšŒê·€ í›ˆë ¨ ì™„ë£Œ")
        print(f"  - ì •í™•ë„: {accuracy:.4f}")
        print(f"  - AUC: {auc:.4f}")
        
        return lr_model
    
    def train_random_forest(self, X_train, y_train, X_test, y_test):
        """ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸŒ² ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # ëª¨ë¸ ì •ì˜
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=self.random_state,
            class_weight='balanced',
            n_jobs=-1
        )
        
        # ëª¨ë¸ í›ˆë ¨
        rf_model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = rf_model.predict(X_test)
        y_pred_proba = rf_model.predict_proba(X_test)[:, 1]
        
        # ì„±ëŠ¥ í‰ê°€
        accuracy = rf_model.score(X_test, y_test)
        auc = roc_auc_score(y_test, y_pred_proba)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': rf_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # ê²°ê³¼ ì €ì¥
        self.models['random_forest'] = rf_model
        self.results['random_forest'] = {
            'accuracy': accuracy,
            'auc': auc,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        self.feature_importance['random_forest'] = feature_importance
        
        print(f"âœ“ ëœë¤í¬ë ˆìŠ¤íŠ¸ í›ˆë ¨ ì™„ë£Œ")
        print(f"  - ì •í™•ë„: {accuracy:.4f}")
        print(f"  - AUC: {auc:.4f}")
        
        return rf_model
    
    def train_xgboost(self, X_train, y_train, X_test, y_test):
        """XGBoost ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸš€ XGBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # ëª¨ë¸ ì •ì˜
        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=self.random_state,
            scale_pos_weight=6.62,  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨
            eval_metric='auc',
            use_label_encoder=False
        )
        
        # ëª¨ë¸ í›ˆë ¨
        xgb_model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = xgb_model.predict(X_test)
        y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]
        
        # ì„±ëŠ¥ í‰ê°€
        accuracy = xgb_model.score(X_test, y_test)
        auc = roc_auc_score(y_test, y_pred_proba)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': xgb_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # ê²°ê³¼ ì €ì¥
        self.models['xgboost'] = xgb_model
        self.results['xgboost'] = {
            'accuracy': accuracy,
            'auc': auc,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        self.feature_importance['xgboost'] = feature_importance
        
        print(f"âœ“ XGBoost í›ˆë ¨ ì™„ë£Œ")
        print(f"  - ì •í™•ë„: {accuracy:.4f}")
        print(f"  - AUC: {auc:.4f}")
        
        return xgb_model
    
    def train_lightgbm(self, X_train, y_train, X_test, y_test):
        """LightGBM ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸ’¡ LightGBM ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # ëª¨ë¸ ì •ì˜
        lgb_model = lgb.LGBMClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=self.random_state,
            class_weight='balanced',
            verbose=-1
        )
        
        # ëª¨ë¸ í›ˆë ¨
        lgb_model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = lgb_model.predict(X_test)
        y_pred_proba = lgb_model.predict_proba(X_test)[:, 1]
        
        # ì„±ëŠ¥ í‰ê°€
        accuracy = lgb_model.score(X_test, y_test)
        auc = roc_auc_score(y_test, y_pred_proba)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': lgb_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # ê²°ê³¼ ì €ì¥
        self.models['lightgbm'] = lgb_model
        self.results['lightgbm'] = {
            'accuracy': accuracy,
            'auc': auc,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        self.feature_importance['lightgbm'] = feature_importance
        
        print(f"âœ“ LightGBM í›ˆë ¨ ì™„ë£Œ")
        print(f"  - ì •í™•ë„: {accuracy:.4f}")
        print(f"  - AUC: {auc:.4f}")
        
        return lgb_model
    
    def compare_models(self):
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print("=" * 60)
        
        # ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not self.results:
            print("âš ï¸ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        print(f"âœ“ í›ˆë ¨ëœ ëª¨ë¸ ìˆ˜: {len(self.results)}ê°œ")
        print(f"âœ“ ëª¨ë¸ ëª©ë¡: {list(self.results.keys())}")
        
        comparison = []
        for model_name, results in self.results.items():
            comparison.append({
                'Model': model_name,
                'Accuracy': results['accuracy'],
                'AUC': results['auc']
            })
        
        comparison_df = pd.DataFrame(comparison)
        print(comparison_df.to_string(index=False))
        
        return comparison_df
    
    def plot_roc_curves(self, y_test):
        """ROC ê³¡ì„  ì‹œê°í™”"""
        plt.figure(figsize=(10, 8))
        
        for model_name, results in self.results.items():
            fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'])
            auc = results['auc']
            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')
        
        plt.plot([0, 1], [0, 1], 'k--', label='Random')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves Comparison')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ì €ì¥
        roc_plot_path = BASIC_MODELS_REPORT_PATH.parent / 'roc_curves_comparison.png'
        ensure_directory_exists(roc_plot_path.parent)
        plt.savefig(roc_plot_path, dpi=300, bbox_inches='tight')
        plt.close()  # ì°½ì„ ë‹«ì•„ì„œ ë©”ëª¨ë¦¬ í•´ì œ
        
        print(f"âœ“ ROC ê³¡ì„ ì´ '{roc_plot_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def plot_feature_importance(self, top_n=10):
        """íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ', fontsize=16, fontweight='bold')
        
        models = ['logistic_regression', 'random_forest', 'xgboost', 'lightgbm']
        titles = ['ë¡œì§€ìŠ¤í‹± íšŒê·€', 'ëœë¤í¬ë ˆìŠ¤íŠ¸', 'XGBoost', 'LightGBM']
        
        for i, (model, title) in enumerate(zip(models, titles)):
            if model in self.feature_importance:
                importance_df = self.feature_importance[model].head(top_n)
                
                ax = axes[i//2, i%2]
                bars = ax.barh(range(len(importance_df)), importance_df.iloc[:, 1])
                ax.set_yticks(range(len(importance_df)))
                ax.set_yticklabels(importance_df.iloc[:, 0], fontsize=8)
                ax.set_title(title)
                ax.set_xlabel('ì¤‘ìš”ë„')
                
                # ìƒ‰ìƒ êµ¬ë¶„
                colors = ['red' if 'fico' in str(feature).lower() else 
                         'blue' if 'income' in str(feature).lower() else 
                         'green' if 'debt' in str(feature).lower() else 
                         'orange' for feature in importance_df.iloc[:, 0]]
                
                for bar, color in zip(bars, colors):
                    bar.set_color(color)
        
        plt.tight_layout()
        
        # ì €ì¥
        feature_plot_path = BASIC_MODELS_REPORT_PATH.parent / 'feature_importance_comparison.png'
        ensure_directory_exists(feature_plot_path.parent)
        plt.savefig(feature_plot_path, dpi=300, bbox_inches='tight')
        plt.close()  # ì°½ì„ ë‹«ì•„ì„œ ë©”ëª¨ë¦¬ í•´ì œ
        
        print(f"âœ“ íŠ¹ì„± ì¤‘ìš”ë„ê°€ '{feature_plot_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def generate_model_report(self):
        """ëª¨ë¸ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“ ëª¨ë¸ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        report_content = []
        report_content.append("=" * 80)
        report_content.append("ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë³´ê³ ì„œ")
        report_content.append("=" * 80)
        report_content.append("")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½
        report_content.append("ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
        report_content.append("-" * 50)
        
        for model_name, results in self.results.items():
            report_content.append(f"\nğŸ”¸ {model_name.upper()}")
            report_content.append(f"  - ì •í™•ë„: {results['accuracy']:.4f}")
            report_content.append(f"  - AUC: {results['auc']:.4f}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        report_content.append("\n\nğŸ” íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
        report_content.append("-" * 50)
        
        for model_name, importance_df in self.feature_importance.items():
            report_content.append(f"\nğŸ“ˆ {model_name.upper()} - ìƒìœ„ 10ê°œ íŠ¹ì„±")
            for i, row in importance_df.head(10).iterrows():
                feature = row.iloc[0]
                importance = row.iloc[1]
                report_content.append(f"  {i+1:2d}. {feature:<25} | ì¤‘ìš”ë„: {importance:.4f}")
        
        # ëª¨ë¸ë³„ ì¥ë‹¨ì 
        report_content.append("\n\nğŸ’¡ ëª¨ë¸ë³„ ì¥ë‹¨ì ")
        report_content.append("-" * 50)
        
        model_analysis = {
            'logistic_regression': {
                'ì¥ì ': ['í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ', 'ì•ˆì •ì„± ë†’ìŒ', 'ê³„ì‚° íš¨ìœ¨ì„± ë†’ìŒ'],
                'ë‹¨ì ': ['ë¹„ì„ í˜• ê´€ê³„ í¬ì°© ì–´ë ¤ì›€', 'íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš© ê³ ë ¤ ì•ˆí•¨'],
                'ì í•©ì„±': 'ê·œì œ í™˜ê²½, í•´ì„ì´ ì¤‘ìš”í•œ ê²½ìš°'
            },
            'random_forest': {
                'ì¥ì ': ['ë¹„ì„ í˜• ê´€ê³„ í¬ì°©', 'íŠ¹ì„± ì¤‘ìš”ë„ ì œê³µ', 'ê³¼ì í•©ì— ê°•í•¨'],
                'ë‹¨ì ': ['í•´ì„ ë³µì¡í•¨', 'ê³„ì‚° ë¹„ìš© ë†’ìŒ'],
                'ì í•©ì„±': 'ê· í˜•ì¡íŒ ì„±ëŠ¥ê³¼ í•´ì„ì´ í•„ìš”í•œ ê²½ìš°'
            },
            'xgboost': {
                'ì¥ì ': ['ë§¤ìš° ë†’ì€ ì„±ëŠ¥', 'ì •ê·œí™” íš¨ê³¼', 'ë¹ ë¥¸ í•™ìŠµ'],
                'ë‹¨ì ': ['í•´ì„ ì–´ë ¤ì›€', 'í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë³µì¡'],
                'ì í•©ì„±': 'ìµœê³  ì„±ëŠ¥ì´ ì¤‘ìš”í•œ ê²½ìš°'
            },
            'lightgbm': {
                'ì¥ì ': ['ë§¤ìš° ë¹ ë¥¸ í•™ìŠµ', 'ë©”ëª¨ë¦¬ íš¨ìœ¨ì ', 'ë²”ì£¼í˜• ë³€ìˆ˜ ìë™ ì²˜ë¦¬'],
                'ë‹¨ì ': ['í•´ì„ ì–´ë ¤ì›€', 'ê³¼ì í•© ìœ„í—˜'],
                'ì í•©ì„±': 'ëŒ€ìš©ëŸ‰ ë°ì´í„°, ë¹ ë¥¸ í•™ìŠµì´ í•„ìš”í•œ ê²½ìš°'
            }
        }
        
        for model_name, analysis in model_analysis.items():
            report_content.append(f"\nğŸ”¸ {model_name.upper()}")
            report_content.append(f"  ì¥ì : {', '.join(analysis['ì¥ì '])}")
            report_content.append(f"  ë‹¨ì : {', '.join(analysis['ë‹¨ì '])}")
            report_content.append(f"  ì í•©ì„±: {analysis['ì í•©ì„±']}")
        
        # ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­
        report_content.append("\n\nğŸš€ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­")
        report_content.append("-" * 50)
        report_content.append("1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ")
        report_content.append("2. ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•")
        report_content.append("3. Sharpe Ratio ê¸°ë°˜ í‰ê°€ êµ¬í˜„")
        report_content.append("4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€ ì‹¤í—˜")
        
        # ë³´ê³ ì„œ ì €ì¥
        ensure_directory_exists(BASIC_MODELS_REPORT_PATH.parent)
        
        with open(BASIC_MODELS_REPORT_PATH, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"âœ“ ëª¨ë¸ ì„±ëŠ¥ ë³´ê³ ì„œê°€ '{BASIC_MODELS_REPORT_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return report_content

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê¸°ë³¸ ëª¨ë¸ êµ¬í˜„ ì‹œì‘")
    print("=" * 60)
    
    # ëª¨ë¸ í´ë˜ìŠ¤ ì´ˆê¸°í™”
    models = BasicModels(random_state=settings.random_seed)
    
    # ë°ì´í„° ë¡œë“œ
    data = models.load_data()
    if data is None:
        return
    
    X_train, X_test, y_train, y_test, features = data
    
    # ëª¨ë¸ í›ˆë ¨
    print("\nğŸ”§ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    
    print("1. ë¡œì§€ìŠ¤í‹± íšŒê·€ í›ˆë ¨ ì¤‘...")
    models.train_logistic_regression(X_train, y_train, X_test, y_test)
    
    print("2. ëœë¤í¬ë ˆìŠ¤íŠ¸ í›ˆë ¨ ì¤‘...")
    models.train_random_forest(X_train, y_train, X_test, y_test)
    
    # XGBoostì™€ LightGBMì€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    if XGBOOST_AVAILABLE:
        print("3. XGBoost í›ˆë ¨ ì¤‘...")
        models.train_xgboost(X_train, y_train, X_test, y_test)
    else:
        print("\nâš ï¸ XGBoostë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    if LIGHTGBM_AVAILABLE:
        print("4. LightGBM í›ˆë ¨ ì¤‘...")
        models.train_lightgbm(X_train, y_train, X_test, y_test)
    else:
        print("\nâš ï¸ LightGBMì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print(f"\nâœ“ í›ˆë ¨ ì™„ë£Œëœ ëª¨ë¸ ìˆ˜: {len(models.results)}ê°œ")
    
    # ì„±ëŠ¥ ë¹„êµ
    comparison_df = models.compare_models()
    
    # ì‹œê°í™” (ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
    if len(models.results) > 0:
        print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
        models.plot_roc_curves(y_test)
        models.plot_feature_importance()
        
        # ë³´ê³ ì„œ ìƒì„±
        models.generate_model_report()
    else:
        print("\nâš ï¸ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ì–´ ì‹œê°í™”ì™€ ë³´ê³ ì„œ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print("\nğŸ‰ ê¸°ë³¸ ëª¨ë¸ êµ¬í˜„ ì™„ë£Œ!")
    print("ë‹¤ìŒ ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•")

if __name__ == "__main__":
    main() 