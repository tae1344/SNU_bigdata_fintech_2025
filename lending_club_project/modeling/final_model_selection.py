"""
ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ
Milestone 4.3: ì„±ëŠ¥ ë¹„êµ ë¶„ì„, ì•ˆì •ì„± í‰ê°€, ìµœì¢… ëª¨ë¸ í™•ì •
ëª¨ë¸ë§ë³„ ë°ì´í„° í™œìš© ì „ëµ ì ìš©

ì£¼ìš” ê¸°ëŠ¥:
1. ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ì¢…í•© ë¶„ì„
2. ì•ˆì •ì„± ë° ì‹ ë¢°ì„± í‰ê°€
3. ê¸ˆìœµ ì§€í‘œ ê¸°ë°˜ ìµœì  ëª¨ë¸ ì„ íƒ
4. ìµœì¢… ëª¨ë¸ ì €ì¥ ë° ë°°í¬ ì¤€ë¹„
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Union
import warnings
import sys
import os
from pathlib import Path
import pickle
import time
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, train_test_split
import xgboost as xgb
import lightgbm as lgb

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from config.file_paths import (
    PROJECT_ROOT, REPORTS_DIR, DATA_DIR, FINAL_DIR,
    SELECTED_FEATURES_PATH,
    SCALED_STANDARD_DATA_PATH,
    SCALED_MINMAX_DATA_PATH,
    NEW_FEATURES_DATA_PATH,
    get_reports_file_path, get_data_file_path, get_final_file_path,
    ensure_directory_exists, file_exists
)

from financial_modeling.investment_scenario_simulator import (
    InvestmentScenarioSimulator
)

from financial_modeling.cash_flow_calculator import (
    CashFlowCalculator
)

from modeling.ensemble_models import EnsembleModelingSystem

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS ê¸°ì¤€)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class FinalModelSelectionSystem:
    """
    ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ í´ë˜ìŠ¤ - ëª¨ë¸ë§ë³„ ë°ì´í„° í™œìš© ì „ëµ ì ìš©
    ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ìµœì¢… ëª¨ë¸ì„ ì„ íƒ
    """
    
    def __init__(self, random_seed: int = 42):
        """
        Args:
            random_seed: ëœë¤ ì‹œë“œ
        """
        self.random_seed = random_seed
        self.simulator = InvestmentScenarioSimulator()
        self.cash_flow_calc = CashFlowCalculator()
        
        # ëª¨ë“  ëª¨ë¸ë“¤
        self.basic_models = {}
        self.ensemble_models = {}
        self.all_models = {}
        self.model_performance = {}
        
    def get_priority_features(self, priority_level):
        """ìš°ì„ ìˆœìœ„ì— ë”°ë¼ íŠ¹ì„± ì„ íƒ"""
        print(f"ğŸ“Š ìš°ì„ ìˆœìœ„ {priority_level} íŠ¹ì„± ì„ íƒ ì¤‘...")
        
        if not file_exists(SELECTED_FEATURES_PATH):
            print(f"âœ— ì„ íƒëœ íŠ¹ì„± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SELECTED_FEATURES_PATH}")
            return None
            
        selected_features_df = pd.read_csv(SELECTED_FEATURES_PATH)
        
        if priority_level == 1:
            # ìš°ì„ ìˆœìœ„ 1: 9ê°œ í•µì‹¬ íŠ¹ì„± (ìµœìš°ì„ )
            priority_features = selected_features_df[
                selected_features_df['priority'] == 1
            ]['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 1 íŠ¹ì„±: {len(priority_features)}ê°œ")
            
        elif priority_level == 2:
            # ìš°ì„ ìˆœìœ„ 2: 17ê°œ íŠ¹ì„± (1 + 2)
            priority_features = selected_features_df[
                selected_features_df['priority'].isin([1, 2])
            ]['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 2 íŠ¹ì„±: {len(priority_features)}ê°œ")
            
        else:  # priority_level == 3
            # ìš°ì„ ìˆœìœ„ 3: 30ê°œ íŠ¹ì„± (ëª¨ë“  ì„ íƒëœ íŠ¹ì„±)
            priority_features = selected_features_df['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 3 íŠ¹ì„±: {len(priority_features)}ê°œ")
        
        return priority_features
    
    def load_data_for_final_evaluation(self):
        """ìµœì¢… ëª¨ë¸ í‰ê°€ìš© ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ ìµœì¢… ëª¨ë¸ í‰ê°€ìš© ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ìµœì¢… í‰ê°€: ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ + ìš°ì„ ìˆœìœ„ 3 (ëª¨ë“  íŠ¹ì„±)
        data_path = NEW_FEATURES_DATA_PATH
        priority_level = 3
        print("  - ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ ë°ì´í„° ì‚¬ìš© (ìµœëŒ€ ì„±ëŠ¥)")
        print("  - ìš°ì„ ìˆœìœ„ 3 íŠ¹ì„± ì‚¬ìš© (ëª¨ë“  ì„ íƒ íŠ¹ì„±)")
        
        # ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not file_exists(data_path):
            print(f"âœ— ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            print("ë¨¼ì € feature_engineering ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        df['loan_status_binary'] = df['loan_status'].apply(
            lambda x: 1 if x in ['Default', 'Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'] else 0
        )
        
        # ìš°ì„ ìˆœìœ„ë³„ íŠ¹ì„± ì„ íƒ
        priority_features = self.get_priority_features(priority_level)
        if priority_features is None:
            return None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„± í•„í„°ë§
        available_features = [f for f in priority_features if f in df.columns]
        print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {len(available_features)}ê°œ")
        
        X = df[available_features]
        y = df['loan_status_binary']
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        total_missing = X.isnull().sum().sum()
        if total_missing > 0:
            print(f"âš ï¸ ê²½ê³ : {total_missing}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("   feature_engineering_step2_scaling.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return None
        else:
            print("âœ“ ê²°ì¸¡ì¹˜ ì—†ìŒ - ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©")
        
        return X, y, available_features
    
    def load_all_models(self) -> Dict:
        """ëª¨ë“  ëª¨ë¸ë“¤ ë¡œë“œ"""
        print("ëª¨ë“  ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...")
        
        # ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„±
        ensemble_system = EnsembleModelingSystem(random_seed=self.random_seed)
        
        # ê¸°ë³¸ ëª¨ë¸ë“¤ ìƒì„±
        ensemble_system.create_base_models()
        self.basic_models = ensemble_system.base_models
        
        # ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„±
        voting_soft = ensemble_system.create_voting_ensemble('soft')
        stacking = ensemble_system.create_stacking_ensemble()
        weighted = ensemble_system.create_weighted_ensemble()
        
        self.ensemble_models = ensemble_system.ensemble_models
        
        # ëª¨ë“  ëª¨ë¸ í†µí•©
        self.all_models.update(self.basic_models)
        self.all_models.update(self.ensemble_models)
        
        print(f"âœ“ ì´ {len(self.all_models)}ê°œ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        return self.all_models
    
    def comprehensive_evaluation(self, X_train: pd.DataFrame, X_test: pd.DataFrame,
                               y_train: pd.Series, y_test: pd.Series) -> Dict:
        """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
        print("ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        evaluation_results = {}
        
        for name, model in self.all_models.items():
            print(f"í‰ê°€ ì¤‘: {name}")
            
            try:
                # ëª¨ë¸ í›ˆë ¨
                start_time = time.time()
                model.fit(X_train, y_train)
                training_time = time.time() - start_time
                
                # ì˜ˆì¸¡
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ
                auc_score = roc_auc_score(y_test, y_pred_proba)
                
                # ë¶„ë¥˜ ë¦¬í¬íŠ¸
                classification_rep = classification_report(y_test, y_pred, output_dict=True)
                
                # êµì°¨ ê²€ì¦
                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=1)
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()
                
                # ê¸ˆìœµ ì„±ê³¼ í‰ê°€
                financial_metrics = self.evaluate_financial_performance(
                    X_test, y_test, y_pred_proba, name
                )
                
                # ì•ˆì •ì„± í‰ê°€ (ì˜ˆì¸¡ í™•ë¥ ì˜ í‘œì¤€í¸ì°¨)
                prediction_stability = np.std(y_pred_proba)
                
                evaluation_results[name] = {
                    'auc_score': auc_score,
                    'cv_mean': cv_mean,
                    'cv_std': cv_std,
                    'training_time': training_time,
                    'classification_report': classification_rep,
                    'financial_metrics': financial_metrics,
                    'prediction_stability': prediction_stability,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba
                }
                
                print(f"âœ“ {name} í‰ê°€ ì™„ë£Œ (AUC: {auc_score:.4f}, CV: {cv_mean:.4f}Â±{cv_std:.4f})")
                
            except Exception as e:
                evaluation_results[name] = {
                    'status': 'error',
                    'error': str(e)
                }
                print(f"âœ— {name} í‰ê°€ ì‹¤íŒ¨: {e}")
        
        return evaluation_results
    
    def evaluate_financial_performance(self, X_test: pd.DataFrame, y_test: pd.Series, 
                                     y_pred_proba: np.ndarray, model_name: str = "Model") -> Dict:
        """ê¸ˆìœµ ì„±ê³¼ í‰ê°€ (ëª¨ë¸ë³„ ì°¨ë³„í™”)"""
        try:
            # ëª¨ë¸ë³„ ì°¨ë³„í™”ëœ íŒŒë¼ë¯¸í„° ì„¤ì •
            model_params = {
                'logistic_regression': {
                    'interest_rate': 0.08,  # ë³´ìˆ˜ì  ì´ììœ¨
                    'default_loss_rate': -0.25,  # ì ì€ ì†ì‹¤
                    'risk_free_rate': 0.02,
                    'base_amount': 8000
                },
                'random_forest': {
                    'interest_rate': 0.10,  # ì¤‘ê°„ ì´ììœ¨
                    'default_loss_rate': -0.30,  # ì¤‘ê°„ ì†ì‹¤
                    'risk_free_rate': 0.025,
                    'base_amount': 10000
                },
                'xgboost': {
                    'interest_rate': 0.12,  # ë†’ì€ ì´ììœ¨
                    'default_loss_rate': -0.35,  # ë†’ì€ ì†ì‹¤
                    'risk_free_rate': 0.03,
                    'base_amount': 12000
                },
                'lightgbm': {
                    'interest_rate': 0.11,  # ì¤‘ê°„-ë†’ì€ ì´ììœ¨
                    'default_loss_rate': -0.32,  # ì¤‘ê°„-ë†’ì€ ì†ì‹¤
                    'risk_free_rate': 0.028,
                    'base_amount': 11000
                },
                'voting_soft': {
                    'interest_rate': 0.105,  # ì•™ìƒë¸” í‰ê· 
                    'default_loss_rate': -0.31,
                    'risk_free_rate': 0.026,
                    'base_amount': 10500
                },
                'stacking': {
                    'interest_rate': 0.115,  # ìŠ¤íƒœí‚¹ì€ ë” ì ê·¹ì 
                    'default_loss_rate': -0.33,
                    'risk_free_rate': 0.029,
                    'base_amount': 11500
                },
                'weighted': {
                    'interest_rate': 0.11,  # ê°€ì¤‘ í‰ê· 
                    'default_loss_rate': -0.31,
                    'risk_free_rate': 0.027,
                    'base_amount': 11000
                }
            }
            
            # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ì„ íƒ
            params = model_params.get(model_name.lower(), {
                'interest_rate': 0.10,
                'default_loss_rate': -0.30,
                'risk_free_rate': 0.025,
                'base_amount': 10000
            })
            
            # ìƒ˜í”Œ ëŒ€ì¶œ ë°ì´í„° ìƒì„±
            loan_data = self.simulator.generate_sample_loan_data(len(X_test))
            
            # ì˜ˆì¸¡ í™•ë¥ ì„ ë¶€ë„ í™•ë¥ ë¡œ ì„¤ì •
            loan_data['default_probability'] = y_pred_proba
            
            # ëª¨ë¸ë³„ ëŒ€ì¶œ ê¸ˆì•¡ ì„¤ì •
            loan_amounts = np.full(len(X_test), params['base_amount'])
            
            # ì˜ˆìƒ ìˆ˜ìµë¥  ê³„ì‚° (ë¶€ë„ í™•ë¥  ê¸°ë°˜)
            expected_returns = (1 - y_pred_proba) * params['interest_rate'] + y_pred_proba * params['default_loss_rate']
            
            # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ 
            portfolio_return = np.mean(expected_returns)
            portfolio_std = np.std(expected_returns)
            
            # Sharpe Ratio ê³„ì‚°
            sharpe_ratio = self.calculate_sharpe_ratio(expected_returns, params['risk_free_rate'])
            
            # ì¶”ê°€ ê¸ˆìœµ ì§€í‘œ
            total_investment = np.sum(loan_amounts)
            total_return = np.sum(expected_returns * loan_amounts)
            roi = total_return / total_investment if total_investment > 0 else 0
            
            # ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥ 
            risk_adjusted_return = portfolio_return / (portfolio_std + 1e-8)
            
            return {
                'sharpe_ratio': sharpe_ratio,
                'portfolio_return': portfolio_return,
                'portfolio_risk': portfolio_std,
                'risk_adjusted_return': risk_adjusted_return,
                'default_rate': np.mean(y_pred_proba),
                'total_investment': total_investment,
                'total_return': total_return,
                'roi': roi,
                'interest_rate': params['interest_rate'],
                'default_loss_rate': params['default_loss_rate'],
                'risk_free_rate': params['risk_free_rate'],
                'model_name': model_name
            }
                
        except Exception as e:
            print(f"ê¸ˆìœµ ì„±ê³¼ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'sharpe_ratio': 0,
                'portfolio_return': 0,
                'portfolio_risk': 0,
                'default_rate': 0,
                'risk_adjusted_return': 0,
                'roi': 0
            }
    
    def calculate_sharpe_ratio(self, returns, risk_free_rate=0.02):
        """Sharpe Ratio ê³„ì‚°"""
        if len(returns) == 0:
            return 0
        
        # ê¸°ë³¸ í†µê³„
        expected_return = np.mean(returns)
        std_return = np.std(returns)
        
        # í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ Sharpe Ratioë¥¼ 0ìœ¼ë¡œ ì„¤ì •
        if std_return < 1e-10:
            return 0
        
        # Sharpe Ratio ê³„ì‚°
        sharpe_ratio = (expected_return - risk_free_rate) / std_return
        
        # ë¹„ì •ìƒì ìœ¼ë¡œ í° ê°’ ì œí•œ
        if abs(sharpe_ratio) > 10:
            return np.sign(sharpe_ratio) * 10
        
        return sharpe_ratio
    
    def create_comprehensive_comparison(self, evaluation_results: Dict) -> pd.DataFrame:
        """ì¢…í•©ì ì¸ ëª¨ë¸ ë¹„êµ"""
        print("ì¢…í•©ì ì¸ ëª¨ë¸ ë¹„êµ ë¶„ì„...")
        
        comparison_data = []
        
        for name, results in evaluation_results.items():
            if 'status' in results and results['status'] == 'error':
                continue
                
            comparison_data.append({
                'Model': name,
                'AUC Score': results['auc_score'],
                'CV Mean': results['cv_mean'],
                'CV Std': results['cv_std'],
                'Sharpe Ratio': results['financial_metrics']['sharpe_ratio'],
                'Portfolio Return': results['financial_metrics']['portfolio_return'],
                'Portfolio Risk': results['financial_metrics']['portfolio_risk'],
                'Default Rate': results['financial_metrics']['default_rate'],
                'Prediction Stability': results['prediction_stability'],
                'Training Time': results['training_time'],
                'Precision': results['classification_report']['weighted avg']['precision'],
                'Recall': results['classification_report']['weighted avg']['recall'],
                'F1-Score': results['classification_report']['weighted avg']['f1-score']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        comparison_df['Overall Score'] = (
            comparison_df['AUC Score'] * 0.3 +
            comparison_df['Sharpe Ratio'] * 0.3 +
            comparison_df['CV Mean'] * 0.2 +
            (1 - comparison_df['Prediction Stability']) * 0.1 +
            comparison_df['F1-Score'] * 0.1
        )
        
        comparison_df = comparison_df.sort_values('Overall Score', ascending=False)
        
        return comparison_df
    
    def select_final_model(self, comparison_df: pd.DataFrame) -> Tuple[str, Dict]:
        """ìµœì¢… ëª¨ë¸ ì„ íƒ"""
        print("ìµœì¢… ëª¨ë¸ ì„ íƒ ì¤‘...")
        
        # ìµœê³  ì¢…í•© ì ìˆ˜ ëª¨ë¸ ì„ íƒ
        best_model_name = comparison_df.iloc[0]['Model']
        best_model = self.all_models[best_model_name]
        
        # ì„ íƒ ê¸°ì¤€ ë¶„ì„
        selection_criteria = {
            'overall_score': comparison_df.iloc[0]['Overall Score'],
            'auc_score': comparison_df.iloc[0]['AUC Score'],
            'sharpe_ratio': comparison_df.iloc[0]['Sharpe Ratio'],
            'cv_mean': comparison_df.iloc[0]['CV Mean'],
            'prediction_stability': comparison_df.iloc[0]['Prediction Stability']
        }
        
        print(f"âœ“ ìµœì¢… ëª¨ë¸ ì„ íƒ: {best_model_name}")
        print(f"  ì¢…í•© ì ìˆ˜: {selection_criteria['overall_score']:.4f}")
        print(f"  AUC Score: {selection_criteria['auc_score']:.4f}")
        print(f"  Sharpe Ratio: {selection_criteria['sharpe_ratio']:.4f}")
        print(f"  CV Mean: {selection_criteria['cv_mean']:.4f}")
        print(f"  ì˜ˆì¸¡ ì•ˆì •ì„±: {selection_criteria['prediction_stability']:.4f}")
        
        return best_model_name, best_model, selection_criteria
    
    def save_final_model(self, model_name: str, model, selection_criteria: Dict) -> None:
        """ìµœì¢… ëª¨ë¸ ì €ì¥"""
        print("ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
        
        ensure_directory_exists(FINAL_DIR)
        
        # ëª¨ë¸ ì €ì¥
        model_path = get_final_file_path("final_model.pkl")
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        # ì„ íƒ ê¸°ì¤€ ì €ì¥
        criteria_path = get_final_file_path("model_selection_criteria.txt")
        with open(criteria_path, 'w', encoding='utf-8') as f:
            f.write("=== ìµœì¢… ëª¨ë¸ ì„ íƒ ê¸°ì¤€ ===\n\n")
            f.write(f"ì„ íƒëœ ëª¨ë¸: {model_name}\n\n")
            f.write("ì„ íƒ ê¸°ì¤€:\n")
            for criterion, value in selection_criteria.items():
                f.write(f"  {criterion}: {value:.4f}\n")
        
        print(f"âœ“ ìµœì¢… ëª¨ë¸ì´ '{model_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"âœ“ ì„ íƒ ê¸°ì¤€ì´ '{criteria_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def create_final_visualizations(self, comparison_df: pd.DataFrame) -> None:
        """ìµœì¢… ì‹œê°í™” ìƒì„±"""
        print("ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹œê°í™” ìƒì„±...")
        
        # 1. ì¢…í•© ì ìˆ˜ ë¹„êµ
        plt.figure(figsize=(15, 10))
        
        # ì¢…í•© ì ìˆ˜ ë¹„êµ
        plt.subplot(2, 3, 1)
        models = comparison_df['Model']
        overall_scores = comparison_df['Overall Score']
        plt.bar(models, overall_scores, color='gold')
        plt.title('ì¢…í•© ì ìˆ˜ ë¹„êµ')
        plt.ylabel('ì¢…í•© ì ìˆ˜')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # AUC Score ë¹„êµ
        plt.subplot(2, 3, 2)
        auc_scores = comparison_df['AUC Score']
        plt.bar(models, auc_scores, color='skyblue')
        plt.title('AUC Score ë¹„êµ')
        plt.ylabel('AUC Score')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Sharpe Ratio ë¹„êµ
        plt.subplot(2, 3, 3)
        sharpe_ratios = comparison_df['Sharpe Ratio']
        plt.bar(models, sharpe_ratios, color='lightgreen')
        plt.title('Sharpe Ratio ë¹„êµ')
        plt.ylabel('Sharpe Ratio')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # êµì°¨ ê²€ì¦ ì„±ëŠ¥ ë¹„êµ
        plt.subplot(2, 3, 4)
        cv_means = comparison_df['CV Mean']
        cv_stds = comparison_df['CV Std']
        plt.bar(models, cv_means, yerr=cv_stds, color='orange', capsize=5)
        plt.title('êµì°¨ ê²€ì¦ ì„±ëŠ¥ ë¹„êµ')
        plt.ylabel('CV Mean Â± Std')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # ì˜ˆì¸¡ ì•ˆì •ì„± ë¹„êµ
        plt.subplot(2, 3, 5)
        stabilities = comparison_df['Prediction Stability']
        plt.bar(models, stabilities, color='red')
        plt.title('ì˜ˆì¸¡ ì•ˆì •ì„± ë¹„êµ')
        plt.ylabel('ì˜ˆì¸¡ ì•ˆì •ì„± (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # í›ˆë ¨ ì‹œê°„ ë¹„êµ
        plt.subplot(2, 3, 6)
        training_times = comparison_df['Training Time']
        plt.bar(models, training_times, color='purple')
        plt.title('í›ˆë ¨ ì‹œê°„ ë¹„êµ')
        plt.ylabel('í›ˆë ¨ ì‹œê°„ (ì´ˆ)')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ì‹œê°í™” ì €ì¥
        ensure_directory_exists(REPORTS_DIR)
        visualization_path = get_reports_file_path("final_model_selection.png")
        plt.savefig(visualization_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ì‹œê°í™”ê°€ '{visualization_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_final_results(self, evaluation_results: Dict, comparison_df: pd.DataFrame,
                          final_model_name: str, selection_criteria: Dict) -> None:
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        print("ìµœì¢… ëª¨ë¸ ì„ íƒ ê²°ê³¼ ì €ì¥...")
        
        ensure_directory_exists(REPORTS_DIR)
        
        # 1. ìƒì„¸ ê²°ê³¼ ì €ì¥
        results_path = get_reports_file_path("final_model_selection_results.txt")
        with open(results_path, 'w', encoding='utf-8') as f:
            f.write("=== ìµœì¢… ëª¨ë¸ ì„ íƒ ê²°ê³¼ ===\n\n")
            
            f.write("1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n")
            f.write(comparison_df.to_string(index=False))
            f.write("\n\n")
            
            f.write("2. ìµœì¢… ëª¨ë¸ ì„ íƒ:\n")
            f.write(f"  ì„ íƒëœ ëª¨ë¸: {final_model_name}\n")
            f.write(f"  ì¢…í•© ì ìˆ˜: {selection_criteria['overall_score']:.4f}\n")
            f.write(f"  AUC Score: {selection_criteria['auc_score']:.4f}\n")
            f.write(f"  Sharpe Ratio: {selection_criteria['sharpe_ratio']:.4f}\n")
            f.write(f"  CV Mean: {selection_criteria['cv_mean']:.4f}\n")
            f.write(f"  ì˜ˆì¸¡ ì•ˆì •ì„±: {selection_criteria['prediction_stability']:.4f}\n\n")
            
            f.write("3. ì„ íƒ ê¸°ì¤€:\n")
            f.write("  - AUC Score (30%): ë¶„ë¥˜ ì„±ëŠ¥\n")
            f.write("  - Sharpe Ratio (30%): ê¸ˆìœµ ì„±ê³¼\n")
            f.write("  - CV Mean (20%): êµì°¨ ê²€ì¦ ì„±ëŠ¥\n")
            f.write("  - ì˜ˆì¸¡ ì•ˆì •ì„± (10%): ëª¨ë¸ ì•ˆì •ì„±\n")
            f.write("  - F1-Score (10%): ë¶„ë¥˜ ì •í™•ë„\n\n")
            
            f.write("4. ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ:\n")
            for name, results in evaluation_results.items():
                if 'status' in results and results['status'] == 'error':
                    continue
                    
                f.write(f"\n{name} ëª¨ë¸:\n")
                f.write(f"  AUC Score: {results['auc_score']:.4f}\n")
                f.write(f"  CV Mean Â± Std: {results['cv_mean']:.4f} Â± {results['cv_std']:.4f}\n")
                f.write(f"  Sharpe Ratio: {results['financial_metrics']['sharpe_ratio']:.4f}\n")
                f.write(f"  Portfolio Return: {results['financial_metrics']['portfolio_return']:.4f}\n")
                f.write(f"  Portfolio Risk: {results['financial_metrics']['portfolio_risk']:.4f}\n")
                f.write(f"  Default Rate: {results['financial_metrics']['default_rate']:.4f}\n")
                f.write(f"  Prediction Stability: {results['prediction_stability']:.4f}\n")
                f.write(f"  Training Time: {results['training_time']:.2f}ì´ˆ\n")
        
        # 2. ë¹„êµ ë°ì´í„° ì €ì¥
        comparison_path = get_reports_file_path("final_model_selection_comparison.csv")
        comparison_df.to_csv(comparison_path, index=False, encoding='utf-8')
        
        print(f"ìƒì„¸ ê²°ê³¼ê°€ '{results_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ë¹„êµ ë°ì´í„°ê°€ '{comparison_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def run_final_model_selection_experiment():
    """ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰"""
    
    print("=== ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹œì‘ ===")
    
    # ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    selection_system = FinalModelSelectionSystem(random_seed=42)
    
    # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
    print("ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    data = selection_system.load_data_for_final_evaluation()
    if data is None:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return None, None, None, None
    
    X, y, features = data
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"âœ“ ë°ì´í„° ë¶„í•  ì™„ë£Œ")
    print(f"  - í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ")
    print(f"  - íŠ¹ì„± ìˆ˜: {X_train.shape[1]}ê°œ")
    
    # ëª¨ë“  ëª¨ë¸ ë¡œë“œ
    all_models = selection_system.load_all_models()
    
    # ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€
    print("ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    evaluation_results = selection_system.comprehensive_evaluation(
        X_train, X_test, y_train, y_test
    )
    
    # ì¢…í•©ì ì¸ ëª¨ë¸ ë¹„êµ
    comparison_df = selection_system.create_comprehensive_comparison(evaluation_results)
    
    # ìµœì¢… ëª¨ë¸ ì„ íƒ
    final_model_name, final_model, selection_criteria = selection_system.select_final_model(comparison_df)
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    selection_system.save_final_model(final_model_name, final_model, selection_criteria)
    
    # ì‹œê°í™” ìƒì„±
    selection_system.create_final_visualizations(comparison_df)
    
    # ê²°ê³¼ ì €ì¥
    selection_system.save_final_results(evaluation_results, comparison_df, 
                                      final_model_name, selection_criteria)
    
    print("\n=== ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì™„ë£Œ ===")
    print(f"ìµœì¢… ì„ íƒëœ ëª¨ë¸: {final_model_name}")
    print(f"ì¢…í•© ì ìˆ˜: {selection_criteria['overall_score']:.4f}")
    
    return final_model_name, final_model, evaluation_results, comparison_df


if __name__ == "__main__":
    # ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰
    final_model_name, final_model, evaluation_results, comparison_df = run_final_model_selection_experiment()
    
    print("\n=== Milestone 4.3 ì™„ë£Œ ===")
    print("ìµœì¢… ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.") 