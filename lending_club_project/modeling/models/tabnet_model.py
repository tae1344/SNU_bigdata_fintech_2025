"""
TabNet ëª¨ë¸ í´ë˜ìŠ¤
ì‹ ìš© ìœ„í—˜ í‰ê°€ì™€ íˆ¬ì ìˆ˜ìµë¥  ìµœì í™”ë¥¼ ìœ„í•œ TabNet êµ¬í˜„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report
from .base_model import BaseModel

# TabNet ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    from pytorch_tabnet.tab_model import TabNetClassifier
    from pytorch_tabnet.metrics import Metric
    TABNET_AVAILABLE = True
except ImportError:
    TABNET_AVAILABLE = False
    print("âš ï¸ TabNetì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install pytorch-tabnet'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

class TabNetDataset(Dataset):
    """TabNetìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, X_data, y_data):
        self.X_data = torch.FloatTensor(X_data)
        self.y_data = torch.LongTensor(y_data)
        
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]
        
    def __len__(self):
        return len(self.X_data)

class TabNetModel(BaseModel):
    """TabNet ëª¨ë¸ í´ë˜ìŠ¤ - ì‹ ìš© ìœ„í—˜ í‰ê°€ íŠ¹í™”"""
    
    def __init__(self, random_state=42, **kwargs):
        super().__init__(random_state)
        
        if not TABNET_AVAILABLE:
            raise ImportError("TabNetì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install pytorch-tabnet'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ì‹ ìš© ìœ„í—˜ í‰ê°€ì— ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° (Cobra í™˜ê²½ìš©)
        self.model_params = {
            'n_d': 8,  # ì˜ì‚¬ê²°ì • ì˜ˆì¸¡ ë ˆì´ì–´ ë„ˆë¹„ (ê¸°ë³¸ê°’)
            'n_a': 8,  # ì£¼ì˜ ì„ë² ë”© ë„ˆë¹„ (ê¸°ë³¸ê°’)
            'n_steps': 3,   # ì˜ì‚¬ê²°ì • ë‹¨ê³„ ìˆ˜ (ê¸°ë³¸ê°’)
            'gamma': 1.3,   # íŠ¹ì„± ì„ íƒ ì •ê·œí™” (ê¸°ë³¸ê°’)
            'n_independent': 2,  # ë…ë¦½ íŠ¹ì„± ë³€í™˜ê¸° ìˆ˜ (ê¸°ë³¸ê°’)
            'n_shared': 2,  # ê³µìœ  íŠ¹ì„± ë³€í™˜ê¸° ìˆ˜ (ê¸°ë³¸ê°’)
            'epsilon': 1e-15,  # ìˆ˜ì¹˜ ì•ˆì •ì„± (ê¸°ë³¸ê°’)
            'seed': self.random_state,  # ëœë¤ ì‹œë“œ
            'momentum': 0.02,  # ë°°ì¹˜ ì •ê·œí™” ëª¨ë©˜í…€ (ê¸°ë³¸ê°’)
            'clip_value': None,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ê¸°ë³¸ê°’)
            'lambda_sparse': 1e-3,  # í¬ì†Œì„± ì†ì‹¤ ê³„ìˆ˜ (ê¸°ë³¸ê°’)
            'optimizer_fn': torch.optim.Adam,
            'optimizer_params': {
                'lr': 0.02,  # ê³µì‹ ê¶Œì¥ê°’
                'weight_decay': 1e-5
            },
            'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,
            'scheduler_params': {
                'mode': 'min',
                'factor': 0.5,
                'patience': 10,
                'min_lr': 1e-6
            },
            'mask_type': 'sparsemax',  # ê¸°ë³¸ ë§ˆìŠ¤í¬ íƒ€ì…
            'verbose': 1,   # ê¸°ë³¸ verbose
            'device_name': 'auto',  # GPU ìë™ ê°ì§€
            **kwargs
        }
        
        # fit ë©”ì„œë“œì—ì„œ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„° (ê³µì‹ ë¬¸ì„œ ê¸°ì¤€)
        self.fit_params = {
            'patience': 10,  # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬ (ê¸°ë³¸ê°’)
            'max_epochs': 200,  # ìµœëŒ€ ì—í¬í¬ (ê¸°ë³¸ê°’)
            'batch_size': 1024,  # ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’)
            'virtual_batch_size': 128,  # ê°€ìƒ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’)
            'num_workers': 0,  # ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’)
            'drop_last': False,  # ë§ˆì§€ë§‰ ë°°ì¹˜ ë“œë¡­ ì—¬ë¶€ (ê¸°ë³¸ê°’)
        }
        
        # ì‹ ìš© ìœ„í—˜ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
        self.class_weights = None
        self.scaler = MinMaxScaler()
        
    def _calculate_class_weights(self, y_train):
        """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° - ì‹ ìš© ìœ„í—˜ í‰ê°€ì— ìµœì í™”"""
        from sklearn.utils.class_weight import compute_class_weight
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶€ë„ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(y_train),
            y=y_train
        )
        
        # ì‹ ìš© ìœ„í—˜ í‰ê°€ë¥¼ ìœ„í•´ ë¶€ë„ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ë” ë†’ê²Œ ì¡°ì •
        if len(class_weights) == 2:
            # ë¶€ë„ í´ë˜ìŠ¤(1)ì˜ ê°€ì¤‘ì¹˜ë¥¼ 1.5ë°° ì¦ê°€
            class_weights[1] *= 1.5
        
        self.class_weights = dict(zip(np.unique(y_train), class_weights))
        return self.class_weights
    
    def _prepare_data(self, X_train, y_train, X_test, y_test):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¤€ë¹„"""
        # ë°ì´í„° ì •ê·œí™”
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = TabNetDataset(X_train_scaled, y_train)
        test_dataset = TabNetDataset(X_test_scaled, y_test)
        
        return train_dataset, test_dataset, X_train_scaled, X_test_scaled
    
    def train(self, X_train, y_train, X_test, y_test):
        """TabNet ëª¨ë¸ í›ˆë ¨ - ì‹ ìš© ìœ„í—˜ í‰ê°€ íŠ¹í™”"""
        print("ğŸ§  TabNet ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights = self._calculate_class_weights(y_train)
        print(f"  - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights}")
        
        # ë°ì´í„° ì¤€ë¹„
        train_dataset, test_dataset, X_train_scaled, X_test_scaled = self._prepare_data(
            X_train, y_train, X_test, y_test
        )
        
        # TabNet ëª¨ë¸ ìƒì„±
        self.model = TabNetClassifier(
            n_d=self.model_params['n_d'],
            n_a=self.model_params['n_a'],
            n_steps=self.model_params['n_steps'],
            gamma=self.model_params['gamma'],
            n_independent=self.model_params['n_independent'],
            n_shared=self.model_params['n_shared'],
            epsilon=self.model_params['epsilon'],
            seed=self.model_params['seed'],
            momentum=self.model_params['momentum'],
            lambda_sparse=self.model_params['lambda_sparse'],
            clip_value=self.model_params['clip_value'],
            verbose=self.model_params['verbose'],
            optimizer_fn=self.model_params['optimizer_fn'],
            optimizer_params=self.model_params['optimizer_params'],
            scheduler_fn=self.model_params['scheduler_fn'],
            scheduler_params=self.model_params['scheduler_params'],
            mask_type=self.model_params['mask_type']
        )
        
        # ëª¨ë¸ í›ˆë ¨
        self.model.fit(
            X_train=X_train_scaled,
            y_train=y_train,
            eval_set=[(X_test_scaled, y_test)],
            eval_name=['test'],
            eval_metric=['auc'],
            max_epochs=self.fit_params['max_epochs'],
            patience=self.fit_params['patience'],
            batch_size=self.fit_params['batch_size'],
            virtual_batch_size=self.fit_params['virtual_batch_size'],
            num_workers=self.fit_params['num_workers'],
            drop_last=self.fit_params['drop_last'],
            weights=class_weights
        )
        
        # ì„±ëŠ¥ í‰ê°€
        results = self.evaluate(X_test, y_test)
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° (TabNetì˜ íŠ¹ì„± ì„ íƒ ë§ˆìŠ¤í¬ ì‚¬ìš©)
        self.feature_importance = self._calculate_feature_importance(X_train_scaled)
        
        print(f"âœ“ TabNet í›ˆë ¨ ì™„ë£Œ")
        print(f"  - ì •í™•ë„: {results['accuracy']:.4f}")
        print(f"  - AUC: {results['auc']:.4f}")
        print(f"  - íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ì™„ë£Œ")
        
        return self.model
    
    def _calculate_feature_importance(self, X_train):
        """TabNet íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° - ë§ˆìŠ¤í¬ ê¸°ë°˜"""
        if self.model is None:
            return None
        
        try:
            # TabNetì˜ íŠ¹ì„± ì„ íƒ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•œ ì¤‘ìš”ë„ ê³„ì‚°
            feature_importances = self.model.feature_importances_
            
            # ë§ˆìŠ¤í¬ ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
            mask_importances = np.mean(feature_importances, axis=0)
            
            importance_df = pd.DataFrame({
                'feature': range(len(mask_importances)),
                'importance': mask_importances
            }).sort_values('importance', ascending=False)
            
            return importance_df
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def get_feature_importance(self, feature_names=None):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë°˜í™˜"""
        if self.feature_importance is None:
            return None
        
        if feature_names is not None:
            self.feature_importance['feature'] = feature_names
            self.feature_importance = self.feature_importance.reindex(
                self.feature_importance['feature'].index
            )
        
        return self.feature_importance
    
    def get_attention_masks(self):
        """TabNetì˜ ì£¼ì˜ ë§ˆìŠ¤í¬ ë°˜í™˜"""
        if self.model is None:
            return None
        
        try:
            # TabNetì˜ ì£¼ì˜ ë§ˆìŠ¤í¬ ë°˜í™˜
            attention_masks = self.model.feature_importances_
            return attention_masks
        except Exception as e:
            print(f"âš ï¸ ì£¼ì˜ ë§ˆìŠ¤í¬ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.model is None:
            return "ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        info = {
            'model_type': 'TabNet',
            'n_d': self.model_params['n_d'],
            'n_a': self.model_params['n_a'],
            'n_steps': self.model_params['n_steps'],
            'gamma': self.model_params['gamma'],
            'mask_type': self.model_params['mask_type'],
            'class_weights': self.class_weights
        }
        
        return info
    
    def get_model_summary(self):
        """ëª¨ë¸ ìš”ì•½ ì •ë³´"""
        if self.model is None:
            return "ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        summary = {
            'model_type': 'TabNet',
            'parameters': self.model_params,
            'n_steps': self.model_params['n_steps'],
            'n_shared': self.model_params['n_shared'],
            'n_independent': self.model_params['n_independent'],
            'gamma': self.model_params['gamma'],
            'mask_type': self.model_params['mask_type'],
            'class_weights': self.class_weights
        }
        
        if self.results:
            summary.update({
                'accuracy': self.results['accuracy'],
                'auc': self.results['auc']
            })
        
        return summary
    
    def plot_attention_masks(self, save_path=None):
        """TabNet ì£¼ì˜ ë§ˆìŠ¤í¬ ì‹œê°í™”"""
        attention_masks = self.get_attention_masks()
        
        if attention_masks is None:
            print("âš ï¸ ì£¼ì˜ ë§ˆìŠ¤í¬ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(1, len(attention_masks), figsize=(5*len(attention_masks), 6))
        if len(attention_masks) == 1:
            axes = [axes]
        
        for i, mask in enumerate(attention_masks):
            ax = axes[i]
            im = ax.imshow(mask.reshape(1, -1), cmap='viridis', aspect='auto')
            ax.set_title(f'Step {i+1} Attention Mask')
            ax.set_xlabel('Features')
            ax.set_ylabel('Attention')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def get_interpretability_report(self):
        """TabNet í•´ì„ ê°€ëŠ¥ì„± ë³´ê³ ì„œ"""
        if self.model is None:
            return "ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        report = {
            'model_type': 'TabNet',
            'interpretability_features': {
                'attention_masks': 'ê° ì˜ì‚¬ê²°ì • ë‹¨ê³„ë³„ íŠ¹ì„± ì„ íƒ',
                'feature_importance': 'ì „ì²´ íŠ¹ì„± ì¤‘ìš”ë„',
                'step_importance': 'ê° ë‹¨ê³„ë³„ ì¤‘ìš”ë„'
            },
            'advantages': [
                'ì˜ì‚¬ê²°ì • ê³¼ì •ì˜ í•´ì„ ê°€ëŠ¥ì„±',
                'íŠ¹ì„± ì„ íƒì˜ ìë™í™”',
                'ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ í¬ì°©',
                'ì‹ ìš© ìœ„í—˜ í‰ê°€ì— ì í•©í•œ êµ¬ì¡°'
            ]
        }
        
        return report
    
    def predict_with_confidence(self, X):
        """ì‹ ë¢°ë„ì™€ í•¨ê»˜ ì˜ˆì¸¡ ìˆ˜í–‰"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ì •ê·œí™”
        X_scaled = self.scaler.transform(X)
        
        # ì˜ˆì¸¡ í™•ë¥ 
        probabilities = self.predict_proba(X_scaled)
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ì˜ˆì¸¡ í™•ë¥ ì˜ ìµœëŒ€ê°’)
        confidence = np.max(probabilities, axis=1)
        
        # ì˜ˆì¸¡ í´ë˜ìŠ¤
        predictions = self.predict(X_scaled)
        
        return predictions, probabilities, confidence
    
    def get_risk_score(self, X):
        """ì‹ ìš© ìœ„í—˜ ì ìˆ˜ ê³„ì‚°"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ì •ê·œí™”
        X_scaled = self.scaler.transform(X)
        
        # ë¶€ë„ í™•ë¥  (ìœ„í—˜ ì ìˆ˜)
        risk_scores = self.predict_proba(X_scaled)[:, 1]
        
        return risk_scores 