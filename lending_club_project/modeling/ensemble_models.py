"""
ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œìŠ¤í…œ
Milestone 4.2: ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” êµ¬í˜„, ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”, Stacking ì•™ìƒë¸”
ëª¨ë¸ë§ë³„ ë°ì´í„° í™œìš© ì „ëµ ì ìš©

ì£¼ìš” ê¸°ëŠ¥:
1. Voting Classifier (Hard/Soft Voting)
2. Stacking Classifier
3. ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
4. ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
5. ê¸ˆìœµ ì§€í‘œ ê¸°ë°˜ í‰ê°€
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Union
import warnings
import sys
import os
from pathlib import Path
import pickle
import time
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, train_test_split
import xgboost as xgb
import lightgbm as lgb

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from config.file_paths import (
    PROJECT_ROOT, REPORTS_DIR, DATA_DIR, FINAL_DIR,
    SELECTED_FEATURES_PATH,
    SCALED_STANDARD_DATA_PATH,
    SCALED_MINMAX_DATA_PATH,
    NEW_FEATURES_DATA_PATH,
    get_reports_file_path, get_data_file_path, get_final_file_path,
    ensure_directory_exists, file_exists
)

from financial_modeling.investment_scenario_simulator import (
    InvestmentScenarioSimulator
)

from financial_modeling.cash_flow_calculator import (
    CashFlowCalculator
)

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS ê¸°ì¤€)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class EnsembleModelingSystem:
    """
    ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œìŠ¤í…œ í´ë˜ìŠ¤ - ëª¨ë¸ë§ë³„ ë°ì´í„° í™œìš© ì „ëµ ì ìš©
    ë‹¤ì–‘í•œ ì•™ìƒë¸” ê¸°ë²•ì„ êµ¬í˜„í•˜ê³  ì„±ëŠ¥ì„ í‰ê°€
    """
    
    def __init__(self, random_seed: int = 42):
        """
        Args:
            random_seed: ëœë¤ ì‹œë“œ
        """
        self.random_seed = random_seed
        self.simulator = InvestmentScenarioSimulator()
        self.cash_flow_calc = CashFlowCalculator()
        
        # ê¸°ë³¸ ëª¨ë¸ë“¤
        self.base_models = {}
        self.ensemble_models = {}
        self.performance_results = {}
        
    def get_priority_features(self, priority_level):
        """ìš°ì„ ìˆœìœ„ì— ë”°ë¼ íŠ¹ì„± ì„ íƒ"""
        print(f"ğŸ“Š ìš°ì„ ìˆœìœ„ {priority_level} íŠ¹ì„± ì„ íƒ ì¤‘...")
        
        if not file_exists(SELECTED_FEATURES_PATH):
            print(f"âœ— ì„ íƒëœ íŠ¹ì„± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SELECTED_FEATURES_PATH}")
            return None
            
        selected_features_df = pd.read_csv(SELECTED_FEATURES_PATH)
        
        if priority_level == 1:
            # ìš°ì„ ìˆœìœ„ 1: 9ê°œ í•µì‹¬ íŠ¹ì„± (ìµœìš°ì„ )
            priority_features = selected_features_df[
                selected_features_df['priority'] == 1
            ]['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 1 íŠ¹ì„±: {len(priority_features)}ê°œ")
            
        elif priority_level == 2:
            # ìš°ì„ ìˆœìœ„ 2: 17ê°œ íŠ¹ì„± (1 + 2)
            priority_features = selected_features_df[
                selected_features_df['priority'].isin([1, 2])
            ]['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 2 íŠ¹ì„±: {len(priority_features)}ê°œ")
            
        else:  # priority_level == 3
            # ìš°ì„ ìˆœìœ„ 3: 30ê°œ íŠ¹ì„± (ëª¨ë“  ì„ íƒëœ íŠ¹ì„±)
            priority_features = selected_features_df['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 3 íŠ¹ì„±: {len(priority_features)}ê°œ")
        
        return priority_features
    
    def load_data_for_ensemble(self):
        """ì•™ìƒë¸” ëª¨ë¸ìš© ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ ì•™ìƒë¸” ëª¨ë¸ìš© ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì•™ìƒë¸”: ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ + ìš°ì„ ìˆœìœ„ 3
        data_path = NEW_FEATURES_DATA_PATH
        priority_level = 3
        print("  - ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ ë°ì´í„° ì‚¬ìš© (ìµœëŒ€ ì„±ëŠ¥)")
        print("  - ìš°ì„ ìˆœìœ„ 3 íŠ¹ì„± ì‚¬ìš© (ëª¨ë“  ì„ íƒ íŠ¹ì„±)")
        
        # ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not file_exists(data_path):
            print(f"âœ— ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            print("ë¨¼ì € feature_engineering ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        df['loan_status_binary'] = df['loan_status'].apply(
            lambda x: 1 if x in ['Default', 'Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'] else 0
        )
        
        # ìš°ì„ ìˆœìœ„ë³„ íŠ¹ì„± ì„ íƒ
        priority_features = self.get_priority_features(priority_level)
        if priority_features is None:
            return None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„± í•„í„°ë§
        available_features = [f for f in priority_features if f in df.columns]
        print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {len(available_features)}ê°œ")
        
        X = df[available_features]
        y = df['loan_status_binary']
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        total_missing = X.isnull().sum().sum()
        if total_missing > 0:
            print(f"âš ï¸ ê²½ê³ : {total_missing}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("   feature_engineering_step2_scaling.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return None
        else:
            print("âœ“ ê²°ì¸¡ì¹˜ ì—†ìŒ - ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©")
        
        return X, y, available_features
    
    def load_tuned_models(self) -> Dict:
        """íŠœë‹ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        print("íŠœë‹ëœ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...")
        
        models_dir = project_root / "models"
        
        # ëª¨ë¸ íŒŒì¼ë“¤ ë¡œë“œ
        model_files = {
            'logistic_regression': 'logisticregression_tuned.pkl',
            'random_forest': 'randomforest_tuned.pkl',
            'xgboost': 'xgboost_tuned.pkl'
        }
        
        for name, filename in model_files.items():
            filepath = models_dir / filename
            if filepath.exists():
                with open(filepath, 'rb') as f:
                    self.base_models[name] = pickle.load(f)
                print(f"âœ“ {name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                print(f"âš  {name} ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filename}")
        
        return self.base_models
    
    def create_base_models(self) -> Dict:
        """ê¸°ë³¸ ëª¨ë¸ë“¤ ìƒì„± (íŠœë‹ëœ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)"""
        print("ê¸°ë³¸ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")
        
        # Logistic Regression
        self.base_models['logistic_regression'] = LogisticRegression(
            C=1.0, penalty='l1', solver='liblinear', random_state=self.random_seed
        )
        
        # Random Forest
        self.base_models['random_forest'] = RandomForestClassifier(
            n_estimators=50, max_depth=None, max_features='log2',
            min_samples_split=10, min_samples_leaf=1, random_state=self.random_seed
        )
        
        # XGBoost
        self.base_models['xgboost'] = xgb.XGBClassifier(
            n_estimators=200, learning_rate=0.2, max_depth=3,
            colsample_bytree=0.8, subsample=0.8, reg_alpha=0.1, reg_lambda=0.1,
            random_state=self.random_seed
        )
        
        # LightGBM (ì„ íƒì )
        try:
            self.base_models['lightgbm'] = lgb.LGBMClassifier(
                n_estimators=200, learning_rate=0.2, max_depth=3,
                colsample_bytree=0.8, subsample=0.8, reg_alpha=0.1, reg_lambda=0.1,
                random_state=self.random_seed, verbose=-1
            )
            print("âœ“ LightGBM ëª¨ë¸ ì¶”ê°€")
        except ImportError:
            print("âš  LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì œì™¸í•©ë‹ˆë‹¤")
        
        print(f"âœ“ {len(self.base_models)}ê°œ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        return self.base_models
    
    def create_voting_ensemble(self, voting_type: str = 'soft') -> VotingClassifier:
        """Voting ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        print(f"Voting ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ({voting_type} voting)")
        
        estimators = []
        for name, model in self.base_models.items():
            # ëª¨ë¸ ì´ë¦„ì„ ê°„ë‹¨í•˜ê²Œ ë³€ê²½
            short_name = name.replace('_', '').upper()
            estimators.append((short_name, model))
        
        voting_clf = VotingClassifier(
            estimators=estimators,
            voting=voting_type,
            n_jobs=-1
        )
        
        self.ensemble_models[f'voting_{voting_type}'] = voting_clf
        return voting_clf
    
    def create_stacking_ensemble(self, meta_classifier=None) -> StackingClassifier:
        """Stacking ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        print("Stacking ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
        
        if meta_classifier is None:
            meta_classifier = LogisticRegression(random_state=self.random_seed)
        
        estimators = []
        for name, model in self.base_models.items():
            short_name = name.replace('_', '').upper()
            estimators.append((short_name, model))
        
        stacking_clf = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_classifier,
            cv=5,
            n_jobs=-1
        )
        
        self.ensemble_models['stacking'] = stacking_clf
        return stacking_clf
    
    def create_weighted_ensemble(self, weights: Dict[str, float] = None) -> 'WeightedEnsemble':
        """ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        print("ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
        
        if weights is None:
            # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ëª¨ë¸ ì„±ëŠ¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜)
            weights = {
                'logistic_regression': 0.25,
                'random_forest': 0.30,
                'xgboost': 0.25,
                'lightgbm': 0.20
            }
        
        weighted_ensemble = WeightedEnsemble(
            models=self.base_models,
            weights=weights,
            random_state=self.random_seed
        )
        
        self.ensemble_models['weighted'] = weighted_ensemble
        return weighted_ensemble
    
    def train_ensemble_models(self, X_train: pd.DataFrame, y_train: pd.Series) -> Dict:
        """ì•™ìƒë¸” ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ì•™ìƒë¸” ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")
        
        training_results = {}
        
        for name, ensemble_model in self.ensemble_models.items():
            print(f"í›ˆë ¨ ì¤‘: {name}")
            start_time = time.time()
            
            try:
                ensemble_model.fit(X_train, y_train)
                training_time = time.time() - start_time
                training_results[name] = {
                    'status': 'success',
                    'training_time': training_time
                }
                print(f"âœ“ {name} í›ˆë ¨ ì™„ë£Œ ({training_time:.2f}ì´ˆ)")
            except Exception as e:
                training_results[name] = {
                    'status': 'error',
                    'error': str(e)
                }
                print(f"âœ— {name} í›ˆë ¨ ì‹¤íŒ¨: {e}")
        
        return training_results
    
    def evaluate_ensemble_models(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """ì•™ìƒë¸” ëª¨ë¸ë“¤ í‰ê°€"""
        print("ì•™ìƒë¸” ëª¨ë¸ë“¤ í‰ê°€ ì‹œì‘...")
        
        evaluation_results = {}
        
        for name, ensemble_model in self.ensemble_models.items():
            print(f"í‰ê°€ ì¤‘: {name}")
            
            try:
                # ì˜ˆì¸¡ í™•ë¥ 
                y_pred_proba = ensemble_model.predict_proba(X_test)[:, 1]
                
                # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ
                auc_score = roc_auc_score(y_test, y_pred_proba)
                
                # ë¶„ë¥˜ ë¦¬í¬íŠ¸
                y_pred = ensemble_model.predict(X_test)
                classification_rep = classification_report(y_test, y_pred, output_dict=True)
                
                # ê¸ˆìœµ ì„±ê³¼ í‰ê°€
                financial_metrics = self.evaluate_financial_performance(
                    X_test, y_test, y_pred_proba
                )
                
                evaluation_results[name] = {
                    'auc_score': auc_score,
                    'classification_report': classification_rep,
                    'financial_metrics': financial_metrics,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba
                }
                
                print(f"âœ“ {name} í‰ê°€ ì™„ë£Œ (AUC: {auc_score:.4f})")
                
            except Exception as e:
                evaluation_results[name] = {
                    'status': 'error',
                    'error': str(e)
                }
                print(f"âœ— {name} í‰ê°€ ì‹¤íŒ¨: {e}")
        
        return evaluation_results
    
    def evaluate_financial_performance(self, X_test: pd.DataFrame, y_test: pd.Series, 
                                     y_pred_proba: np.ndarray) -> Dict:
        """ê¸ˆìœµ ì„±ê³¼ í‰ê°€"""
        try:
            # ìƒ˜í”Œ ëŒ€ì¶œ ë°ì´í„° ìƒì„±
            loan_data = self.simulator.generate_sample_loan_data(len(X_test))
            
            # ì˜ˆì¸¡ í™•ë¥ ì„ ë¶€ë„ í™•ë¥ ë¡œ ì„¤ì •
            loan_data['default_probability'] = y_pred_proba
            
            # íˆ¬ì ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
            scenario = self.simulator.simulate_loan_approval_scenario(
                loan_data, approval_threshold=0.5, investment_amount=1000
            )
            
            if scenario.get('portfolio_metrics'):
                return scenario['portfolio_metrics']
            else:
                return {
                    'sharpe_ratio': 0,
                    'portfolio_return': 0,
                    'portfolio_risk': 0,
                    'default_rate': 0
                }
                
        except Exception as e:
            print(f"ê¸ˆìœµ ì„±ê³¼ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'sharpe_ratio': 0,
                'portfolio_return': 0,
                'portfolio_risk': 0,
                'default_rate': 0
            }
    
    def compare_models(self, evaluation_results: Dict) -> pd.DataFrame:
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„...")
        
        comparison_data = []
        
        for name, results in evaluation_results.items():
            if 'status' in results and results['status'] == 'error':
                continue
                
            comparison_data.append({
                'Model': name,
                'AUC Score': results['auc_score'],
                'Sharpe Ratio': results['financial_metrics']['sharpe_ratio'],
                'Portfolio Return': results['financial_metrics']['portfolio_return'],
                'Portfolio Risk': results['financial_metrics']['portfolio_risk'],
                'Default Rate': results['financial_metrics']['default_rate'],
                'Precision': results['classification_report']['weighted avg']['precision'],
                'Recall': results['classification_report']['weighted avg']['recall'],
                'F1-Score': results['classification_report']['weighted avg']['f1-score']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('AUC Score', ascending=False)
        
        return comparison_df
    
    def create_visualizations(self, evaluation_results: Dict, comparison_df: pd.DataFrame) -> None:
        """ì‹œê°í™” ìƒì„±"""
        print("ì•™ìƒë¸” ëª¨ë¸ ì‹œê°í™” ìƒì„±...")
        
        # 1. ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
        plt.figure(figsize=(15, 10))
        
        # AUC Score ë¹„êµ
        plt.subplot(2, 3, 1)
        models = comparison_df['Model']
        auc_scores = comparison_df['AUC Score']
        plt.bar(models, auc_scores, color='skyblue')
        plt.title('AUC Score ë¹„êµ')
        plt.ylabel('AUC Score')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # Sharpe Ratio ë¹„êµ
        plt.subplot(2, 3, 2)
        sharpe_ratios = comparison_df['Sharpe Ratio']
        plt.bar(models, sharpe_ratios, color='lightgreen')
        plt.title('Sharpe Ratio ë¹„êµ')
        plt.ylabel('Sharpe Ratio')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # ìˆ˜ìµë¥  ë¹„êµ
        plt.subplot(2, 3, 3)
        returns = comparison_df['Portfolio Return']
        plt.bar(models, returns, color='orange')
        plt.title('í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ë¹„êµ')
        plt.ylabel('ìˆ˜ìµë¥ ')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # ìœ„í—˜ë„ ë¹„êµ
        plt.subplot(2, 3, 4)
        risks = comparison_df['Portfolio Risk']
        plt.bar(models, risks, color='red')
        plt.title('í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜ë„ ë¹„êµ')
        plt.ylabel('ìœ„í—˜ë„')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # ë¶€ë„ìœ¨ ë¹„êµ
        plt.subplot(2, 3, 5)
        default_rates = comparison_df['Default Rate']
        plt.bar(models, default_rates, color='purple')
        plt.title('ë¶€ë„ìœ¨ ë¹„êµ')
        plt.ylabel('ë¶€ë„ìœ¨')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        # F1-Score ë¹„êµ
        plt.subplot(2, 3, 6)
        f1_scores = comparison_df['F1-Score']
        plt.bar(models, f1_scores, color='gold')
        plt.title('F1-Score ë¹„êµ')
        plt.ylabel('F1-Score')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ì‹œê°í™” ì €ì¥
        ensure_directory_exists(REPORTS_DIR)
        visualization_path = get_reports_file_path("ensemble_models_comparison.png")
        plt.savefig(visualization_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ì‹œê°í™”ê°€ '{visualization_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_results(self, evaluation_results: Dict, comparison_df: pd.DataFrame) -> None:
        """ê²°ê³¼ ì €ì¥"""
        print("ì•™ìƒë¸” ëª¨ë¸ ê²°ê³¼ ì €ì¥...")
        
        ensure_directory_exists(REPORTS_DIR)
        
        # 1. ìƒì„¸ ê²°ê³¼ ì €ì¥
        results_path = get_reports_file_path("ensemble_models_results.txt")
        with open(results_path, 'w', encoding='utf-8') as f:
            f.write("=== ì•™ìƒë¸” ëª¨ë¸ ê²°ê³¼ ===\n\n")
            
            f.write("1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n")
            f.write(comparison_df.to_string(index=False))
            f.write("\n\n")
            
            f.write("2. ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ:\n")
            for name, results in evaluation_results.items():
                if 'status' in results and results['status'] == 'error':
                    continue
                    
                f.write(f"\n{name} ëª¨ë¸:\n")
                f.write(f"  AUC Score: {results['auc_score']:.4f}\n")
                f.write(f"  Sharpe Ratio: {results['financial_metrics']['sharpe_ratio']:.4f}\n")
                f.write(f"  Portfolio Return: {results['financial_metrics']['portfolio_return']:.4f}\n")
                f.write(f"  Portfolio Risk: {results['financial_metrics']['portfolio_risk']:.4f}\n")
                f.write(f"  Default Rate: {results['financial_metrics']['default_rate']:.4f}\n")
                
                # ë¶„ë¥˜ ë¦¬í¬íŠ¸
                f.write("  Classification Report:\n")
                for class_name, metrics in results['classification_report'].items():
                    if isinstance(metrics, dict):
                        f.write(f"    {class_name}:\n")
                        for metric, value in metrics.items():
                            if isinstance(value, (int, float)):
                                f.write(f"      {metric}: {value:.4f}\n")
        
        # 2. ë¹„êµ ë°ì´í„° ì €ì¥
        comparison_path = get_reports_file_path("ensemble_models_comparison.csv")
        comparison_df.to_csv(comparison_path, index=False, encoding='utf-8')
        
        print(f"ìƒì„¸ ê²°ê³¼ê°€ '{results_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ë¹„êµ ë°ì´í„°ê°€ '{comparison_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


class WeightedEnsemble:
    """ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” í´ë˜ìŠ¤"""
    
    def __init__(self, models: Dict, weights: Dict[str, float], random_state: int = 42):
        """
        Args:
            models: ê¸°ë³¸ ëª¨ë¸ë“¤
            weights: ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜
            random_state: ëœë¤ ì‹œë“œ
        """
        self.models = models
        self.weights = weights
        self.random_state = random_state
        self.is_fitted = False
    
    def fit(self, X, y):
        """ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ê°€ì¤‘ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        for name, model in self.models.items():
            if name in self.weights:
                print(f"  í›ˆë ¨ ì¤‘: {name} (ê°€ì¤‘ì¹˜: {self.weights[name]:.2f})")
                model.fit(X, y)
        
        self.is_fitted = True
        return self
    
    def predict_proba(self, X):
        """ê°€ì¤‘ í‰ê·  í™•ë¥  ì˜ˆì¸¡"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥  ìˆ˜ì§‘
        probas = []
        total_weight = 0
        
        for name, model in self.models.items():
            if name in self.weights:
                weight = self.weights[name]
                proba = model.predict_proba(X)[:, 1]  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ 
                probas.append(proba * weight)
                total_weight += weight
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_proba = np.sum(probas, axis=0) / total_weight
        
        # 2D ë°°ì—´ë¡œ ë³€í™˜ (ìŒì„±, ì–‘ì„± í´ë˜ìŠ¤)
        result = np.column_stack([1 - weighted_proba, weighted_proba])
        
        return result
    
    def predict(self, X):
        """í´ë˜ìŠ¤ ì˜ˆì¸¡"""
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)


def run_ensemble_modeling_experiment():
    """ì•™ìƒë¸” ëª¨ë¸ë§ ì‹¤í—˜ ì‹¤í–‰"""
    
    print("=== ì•™ìƒë¸” ëª¨ë¸ë§ ì‹¤í—˜ ì‹œì‘ ===")
    
    # ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ensemble_system = EnsembleModelingSystem(random_seed=42)
    
    # ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
    try:
        ensemble_system.load_tuned_models()
    except:
        print("íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ìƒì„±")
        ensemble_system.create_base_models()
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    print("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    loan_data = ensemble_system.simulator.generate_sample_loan_data(1000)
    loan_data['target'] = (np.random.random(len(loan_data)) < loan_data['default_probability']).astype(int)
    
    # íŠ¹ì„± ì„ íƒ
    feature_columns = ['loan_amnt', 'int_rate', 'term', 'fico_score']
    available_features = [col for col in feature_columns if col in loan_data.columns]
    
    if len(available_features) < 2:
        # ê°€ìƒ íŠ¹ì„± ìƒì„±
        X = np.random.rand(len(loan_data), 4)
    else:
        X = loan_data[available_features].values
    
    y = loan_data['target']
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„±
    print("ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")
    
    # 1. Voting ì•™ìƒë¸” (Soft)
    voting_soft = ensemble_system.create_voting_ensemble('soft')
    
    # 2. Voting ì•™ìƒë¸” (Hard)
    voting_hard = ensemble_system.create_voting_ensemble('hard')
    
    # 3. Stacking ì•™ìƒë¸”
    stacking = ensemble_system.create_stacking_ensemble()
    
    # 4. ê°€ì¤‘ ì•™ìƒë¸”
    weighted = ensemble_system.create_weighted_ensemble()
    
    # ì•™ìƒë¸” ëª¨ë¸ë“¤ í›ˆë ¨
    training_results = ensemble_system.train_ensemble_models(X_train, y_train)
    
    # ì•™ìƒë¸” ëª¨ë¸ë“¤ í‰ê°€
    evaluation_results = ensemble_system.evaluate_ensemble_models(X_test, y_test)
    
    # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    comparison_df = ensemble_system.compare_models(evaluation_results)
    
    # ì‹œê°í™” ìƒì„±
    ensemble_system.create_visualizations(evaluation_results, comparison_df)
    
    # ê²°ê³¼ ì €ì¥
    ensemble_system.save_results(evaluation_results, comparison_df)
    
    print("\n=== ì•™ìƒë¸” ëª¨ë¸ë§ ì‹¤í—˜ ì™„ë£Œ ===")
    print("ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
    best_model = comparison_df.iloc[0]
    print(f"  ëª¨ë¸: {best_model['Model']}")
    print(f"  AUC Score: {best_model['AUC Score']:.4f}")
    print(f"  Sharpe Ratio: {best_model['Sharpe Ratio']:.4f}")
    
    return evaluation_results, comparison_df


if __name__ == "__main__":
    # ì•™ìƒë¸” ëª¨ë¸ë§ ì‹¤í—˜ ì‹¤í–‰
    evaluation_results, comparison_df = run_ensemble_modeling_experiment()
    
    print("\n=== Milestone 4.2 ì™„ë£Œ ===")
    print("ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.") 