"""
ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ êµ¬í˜„
Train/Validation/Test Split, Cross Validation, ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ë“¤ì„ êµ¬í˜„
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI ì°½ì´ ì—´ë¦¬ì§€ ì•Šë„ë¡ ì„¤ì •
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import (
    train_test_split, 
    cross_val_score, 
    StratifiedKFold, 
    KFold,
    GridSearchCV,
    RandomizedSearchCV
)
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_auc_score, 
    roc_curve,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
    precision_recall_curve,
    average_precision_score
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import warnings
import sys
import os
from pathlib import Path
import time
from datetime import datetime
import json

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.file_paths import (
    SELECTED_FEATURES_PATH,
    SCALED_STANDARD_DATA_PATH,
    BASIC_MODELS_REPORT_PATH,
    ensure_directory_exists,
    file_exists
)
from config.settings import settings

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class ModelEvaluationFramework:
    """ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ í´ë˜ìŠ¤"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        self.feature_names = None
        self.split_info = {}
        self.evaluation_results = {}
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì„ íƒëœ íŠ¹ì„± ë¡œë“œ
        if not file_exists(SELECTED_FEATURES_PATH):
            print(f"âœ— ì„ íƒëœ íŠ¹ì„± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SELECTED_FEATURES_PATH}")
            print("ë¨¼ì € feature_selection_analysis.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
            
        selected_features_df = pd.read_csv(SELECTED_FEATURES_PATH)
        selected_features = selected_features_df['selected_feature'].tolist()
        
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ë¡œë“œ
        if not file_exists(SCALED_STANDARD_DATA_PATH):
            print(f"âœ— ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SCALED_STANDARD_DATA_PATH}")
            print("ë¨¼ì € feature_engineering_step2_scaling.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
            
        df = pd.read_csv(SCALED_STANDARD_DATA_PATH)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        df['loan_status_binary'] = df['loan_status'].apply(
            lambda x: 1 if x in ['Default', 'Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'] else 0
        )
        
        # ì„ íƒëœ íŠ¹ì„±ë§Œ ì‚¬ìš©
        available_features = [f for f in selected_features if f in df.columns]
        print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {len(available_features)}ê°œ")
        
        X = df[available_features]
        y = df['loan_status_binary']
        
        # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ìˆ˜ì •
        print("ğŸ” ë°ì´í„° íƒ€ì… í™•ì¸ ì¤‘...")
        non_numeric_columns = []
        for col in X.columns:
            if X[col].dtype == 'object':
                non_numeric_columns.append(col)
                print(f"  âš ï¸ ë¬¸ìì—´ ì»¬ëŸ¼ ë°œê²¬: {col}")
        
        if non_numeric_columns:
            print(f"ğŸ“ {len(non_numeric_columns)}ê°œ ë¬¸ìì—´ ì»¬ëŸ¼ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
            for col in non_numeric_columns:
                # ë¬¸ìì—´ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ ì‹œë„
                try:
                    X[col] = pd.to_numeric(X[col], errors='coerce')
                    # ê²°ì¸¡ì¹˜ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
                    median_val = X[col].median()
                    if pd.isna(median_val):
                        # ì¤‘ì•™ê°’ì´ NaNì¸ ê²½ìš° 0ìœ¼ë¡œ ëŒ€ì²´
                        median_val = 0
                    X[col].fillna(median_val, inplace=True)
                    print(f"  âœ“ {col}: ë¬¸ìì—´ â†’ ìˆ˜ì¹˜í˜• ë³€í™˜ ì™„ë£Œ")
                except Exception as e:
                    print(f"  âœ— {col}: ë³€í™˜ ì‹¤íŒ¨ - {e}")
                    # ë¼ë²¨ ì¸ì½”ë”© ì‹œë„
                    try:
                        X[col] = X[col].astype('category').cat.codes
                        print(f"  âœ“ {col}: ë¼ë²¨ ì¸ì½”ë”© ì™„ë£Œ")
                    except Exception as e2:
                        print(f"  âœ— {col}: ë¼ë²¨ ì¸ì½”ë”©ë„ ì‹¤íŒ¨ - {e2}")
                        # í•´ë‹¹ ì»¬ëŸ¼ ì œê±°
                        X = X.drop(columns=[col])
                        available_features.remove(col)
                        print(f"  âœ— {col}: ì»¬ëŸ¼ ì œê±°ë¨")
        
        # ìµœì¢… NaN ê°’ ì²˜ë¦¬
        final_nan_count = X.isnull().sum().sum()
        if final_nan_count > 0:
            print(f"âš ï¸ ê²½ê³ : {final_nan_count}ê°œì˜ ìµœì¢… NaN ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("   NaN ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            for col in X.columns:
                nan_count = X[col].isnull().sum()
                if nan_count > 0:
                    if X[col].dtype in ['float64', 'int64']:
                        median_val = X[col].median()
                        if pd.isna(median_val):
                            # ì¤‘ì•™ê°’ì´ NaNì¸ ê²½ìš° 0ìœ¼ë¡œ ëŒ€ì²´
                            median_val = 0
                        X[col].fillna(median_val, inplace=True)
                        print(f"  âœ“ {col}: {nan_count}ê°œ NaN â†’ ì¤‘ì•™ê°’({median_val:.4f})")
                    else:
                        # ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ê²½ìš° ìµœë¹ˆê°’ ì‚¬ìš©
                        mode_val = X[col].mode().iloc[0] if not X[col].mode().empty else 0
                        X[col].fillna(mode_val, inplace=True)
                        print(f"  âœ“ {col}: {nan_count}ê°œ NaN â†’ ìµœë¹ˆê°’({mode_val})")
        else:
            print("âœ“ ìµœì¢… NaN ê°’ ì—†ìŒ")
        
        # ë°ì´í„° ê²€ì¦
        print("ğŸ” ìµœì¢… ë°ì´í„° ê²€ì¦ ì¤‘...")
        print(f"  ë°ì´í„° í˜•íƒœ: {X.shape}")
        print(f"  ë°ì´í„° íƒ€ì…: {X.dtypes.value_counts().to_dict()}")
        print(f"  ê²°ì¸¡ì¹˜: {X.isnull().sum().sum()}ê°œ")
        
        # ë¬´í•œê°’ í™•ì¸ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ)
        numeric_cols = X.select_dtypes(include=[np.number])
        inf_count = np.isinf(numeric_cols).sum().sum()
        print(f"  ë¬´í•œê°’: {inf_count}ê°œ")
        
        # ëª¨ë“  ê°’ì´ ìœ í•œí•œì§€ í™•ì¸ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ)
        if inf_count > 0:
            print("âŒ ì˜¤ë¥˜: ì—¬ì „íˆ ë¬´í•œê°’ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return None
        
        # ëª¨ë“  ê°’ì´ ìˆ«ìì¸ì§€ í™•ì¸
        if not X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]:
            print("âŒ ì˜¤ë¥˜: ë¬¸ìì—´ ê°’ì´ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return None
        
        print("âœ“ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
        
        self.feature_names = available_features
        return X, y
    
    def train_validation_test_split(self, X, y, train_size=0.6, val_size=0.2, test_size=0.2, 
                                   stratify=True, random_state=None):
        """
        Train/Validation/Test Split í•¨ìˆ˜
        
        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            train_size: í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.6)
            val_size: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2)
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2)
            stratify: ê³„ì¸µí™” ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            random_state: ëœë¤ ì‹œë“œ
            
        Returns:
            dict: ë¶„í• ëœ ë°ì´í„°ì…‹ ì •ë³´
        """
        if random_state is None:
            random_state = self.random_state
            
        print(f"ğŸ”„ Train/Validation/Test Split ì§„í–‰ ì¤‘...")
        print(f"   í›ˆë ¨: {train_size:.1%}, ê²€ì¦: {val_size:.1%}, í…ŒìŠ¤íŠ¸: {test_size:.1%}")
        
        # ë¹„ìœ¨ ê²€ì¦
        total_ratio = train_size + val_size + test_size
        if abs(total_ratio - 1.0) > 1e-6:
            print(f"âš ï¸ ê²½ê³ : ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤ ({total_ratio:.3f})")
            print("   ë¹„ìœ¨ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.")
            train_size = train_size / total_ratio
            val_size = val_size / total_ratio
            test_size = test_size / total_ratio
        
        # 1ë‹¨ê³„: Train + (Validation + Test) ë¶„í• 
        if stratify:
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, 
                test_size=test_size, 
                random_state=random_state,
                stratify=y
            )
        else:
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, 
                test_size=test_size, 
                random_state=random_state
            )
        
        # 2ë‹¨ê³„: Train + Validation ë¶„í• 
        val_ratio = val_size / (train_size + val_size)
        if stratify:
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp,
                test_size=val_ratio,
                random_state=random_state,
                stratify=y_temp
            )
        else:
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp,
                test_size=val_ratio,
                random_state=random_state
            )
        
        # ê²°ê³¼ ì €ì¥
        self.X_train = X_train
        self.X_val = X_val
        self.X_test = X_test
        self.y_train = y_train
        self.y_val = y_val
        self.y_test = y_test
        
        # ë¶„í•  ì •ë³´ ì €ì¥
        self.split_info = {
            'train_size': len(X_train),
            'val_size': len(X_val),
            'test_size': len(X_test),
            'total_size': len(X),
            'train_ratio': len(X_train) / len(X),
            'val_ratio': len(X_val) / len(X),
            'test_ratio': len(X_test) / len(X),
            'train_class_distribution': y_train.value_counts().to_dict(),
            'val_class_distribution': y_val.value_counts().to_dict(),
            'test_class_distribution': y_test.value_counts().to_dict(),
            'stratify': stratify,
            'random_state': random_state
        }
        
        # ë¶„í•  ê²°ê³¼ ì¶œë ¥
        self._print_split_summary()
        
        return {
            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
            'y_train': y_train, 'y_val': y_val, 'y_test': y_test,
            'split_info': self.split_info
        }
    
    def _print_split_summary(self):
        """ë¶„í•  ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼:")
        print(f"   ì „ì²´ ë°ì´í„°: {self.split_info['total_size']:,}ê°œ")
        print(f"   í›ˆë ¨ ë°ì´í„°: {self.split_info['train_size']:,}ê°œ ({self.split_info['train_ratio']:.1%})")
        print(f"   ê²€ì¦ ë°ì´í„°: {self.split_info['val_size']:,}ê°œ ({self.split_info['val_ratio']:.1%})")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.split_info['test_size']:,}ê°œ ({self.split_info['test_ratio']:.1%})")
        
        print("\nğŸ“ˆ í´ë˜ìŠ¤ ë¶„í¬:")
        train_dist = self.split_info['train_class_distribution']
        val_dist = self.split_info['val_class_distribution']
        test_dist = self.split_info['test_class_distribution']
        
        print(f"   í›ˆë ¨ - ì •ìƒ: {train_dist.get(0, 0):,}ê°œ, ë¶€ë„: {train_dist.get(1, 0):,}ê°œ")
        print(f"   ê²€ì¦ - ì •ìƒ: {val_dist.get(0, 0):,}ê°œ, ë¶€ë„: {val_dist.get(1, 0):,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ - ì •ìƒ: {test_dist.get(0, 0):,}ê°œ, ë¶€ë„: {test_dist.get(1, 0):,}ê°œ")
        
        # í´ë˜ìŠ¤ ë¹„ìœ¨ ê³„ì‚°
        train_ratio = train_dist.get(1, 0) / (train_dist.get(0, 0) + train_dist.get(1, 0))
        val_ratio = val_dist.get(1, 0) / (val_dist.get(0, 0) + val_dist.get(1, 0))
        test_ratio = test_dist.get(1, 0) / (test_dist.get(0, 0) + test_dist.get(1, 0))
        
        print(f"   ë¶€ë„ìœ¨ - í›ˆë ¨: {train_ratio:.3f}, ê²€ì¦: {val_ratio:.3f}, í…ŒìŠ¤íŠ¸: {test_ratio:.3f}")
    
    def cross_validation(self, model, X, y, cv=5, scoring='roc_auc', n_jobs=-1):
        """
        Cross Validation í•¨ìˆ˜
        
        Args:
            model: í›ˆë ¨í•  ëª¨ë¸
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)
            scoring: í‰ê°€ ì§€í‘œ (ê¸°ë³¸ê°’: 'roc_auc')
            n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: -1, ëª¨ë“  CPU ì‚¬ìš©)
            
        Returns:
            dict: êµì°¨ ê²€ì¦ ê²°ê³¼
        """
        print(f"ğŸ”„ {cv}-Fold Cross Validation ì§„í–‰ ì¤‘...")
        print(f"   í‰ê°€ ì§€í‘œ: {scoring}")
        
        # StratifiedKFold ì‚¬ìš© (ë¶„ë¥˜ ë¬¸ì œ)
        if scoring in ['roc_auc', 'accuracy', 'precision', 'recall', 'f1']:
            kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        else:
            kfold = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # êµì°¨ ê²€ì¦ ìˆ˜í–‰
        cv_scores = cross_val_score(model, X, y, cv=kfold, scoring=scoring, n_jobs=n_jobs)
        
        # ê²°ê³¼ ê³„ì‚°
        mean_score = cv_scores.mean()
        std_score = cv_scores.std()
        
        # ê²°ê³¼ ì €ì¥
        cv_results = {
            'scores': cv_scores,
            'mean_score': mean_score,
            'std_score': std_score,
            'cv_folds': cv,
            'scoring': scoring,
            'fold_scores': cv_scores.tolist()
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ“ êµì°¨ ê²€ì¦ ì™„ë£Œ")
        print(f"   í‰ê·  ì ìˆ˜: {mean_score:.4f} Â± {std_score:.4f}")
        print(f"   ìµœì†Œ ì ìˆ˜: {cv_scores.min():.4f}")
        print(f"   ìµœëŒ€ ì ìˆ˜: {cv_scores.max():.4f}")
        
        return cv_results
    
    def calculate_performance_metrics(self, y_true, y_pred, y_pred_proba=None):
        """
        ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜
        
        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred: ì˜ˆì¸¡ ê°’
            y_pred_proba: ì˜ˆì¸¡ í™•ë¥  (ì„ íƒì‚¬í•­)
            
        Returns:
            dict: ì„±ëŠ¥ ì§€í‘œ
        """
        print("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        
        # ê¸°ë³¸ ë¶„ë¥˜ ì§€í‘œ
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # ì¶”ê°€ ì§€í‘œ
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # ROC-AUC (í™•ë¥ ì´ ì œê³µëœ ê²½ìš°)
        roc_auc = None
        if y_pred_proba is not None:
            roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        # Precision-Recall AUC
        pr_auc = None
        if y_pred_proba is not None:
            pr_auc = average_precision_score(y_true, y_pred_proba)
        
        # ê²°ê³¼ ì €ì¥
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'specificity': specificity,
            'sensitivity': sensitivity,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'confusion_matrix': cm.tolist(),
            'tn': int(tn),
            'fp': int(fp),
            'fn': int(fn),
            'tp': int(tp)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ“ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        print(f"   ì •í™•ë„: {accuracy:.4f}")
        print(f"   ì •ë°€ë„: {precision:.4f}")
        print(f"   ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"   F1 ì ìˆ˜: {f1:.4f}")
        if roc_auc is not None:
            print(f"   ROC-AUC: {roc_auc:.4f}")
        if pr_auc is not None:
            print(f"   PR-AUC: {pr_auc:.4f}")
        
        return metrics
    
    def evaluate_model(self, model, X_train, y_train, X_val, y_val, X_test, y_test, 
                      model_name="Model", cv_folds=5):
        """
        ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ (í†µí•©)
        
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            model_name: ëª¨ë¸ ì´ë¦„
            cv_folds: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            
        Returns:
            dict: í‰ê°€ ê²°ê³¼
        """
        print(f"\nğŸ” {model_name} ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # 1. ëª¨ë¸ í›ˆë ¨
        start_time = time.time()
        model.fit(X_train, y_train)
        training_time = time.time() - start_time
        
        # 2. ì˜ˆì¸¡
        y_train_pred = model.predict(X_train)
        y_val_pred = model.predict(X_val)
        y_test_pred = model.predict(X_test)
        
        # í™•ë¥  ì˜ˆì¸¡ (ê°€ëŠ¥í•œ ê²½ìš°)
        y_train_proba = None
        y_val_proba = None
        y_test_proba = None
        
        try:
            y_train_proba = model.predict_proba(X_train)[:, 1]
            y_val_proba = model.predict_proba(X_val)[:, 1]
            y_test_proba = model.predict_proba(X_test)[:, 1]
        except:
            print("âš ï¸ í™•ë¥  ì˜ˆì¸¡ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 3. êµì°¨ ê²€ì¦
        cv_results = self.cross_validation(model, X_train, y_train, cv=cv_folds)
        
        # 4. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        train_metrics = self.calculate_performance_metrics(y_train, y_train_pred, y_train_proba)
        val_metrics = self.calculate_performance_metrics(y_val, y_val_pred, y_val_proba)
        test_metrics = self.calculate_performance_metrics(y_test, y_test_pred, y_test_proba)
        
        # 5. ê²°ê³¼ í†µí•©
        evaluation_result = {
            'model_name': model_name,
            'training_time': training_time,
            'cv_results': cv_results,
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'predictions': {
                'train': y_train_pred.tolist(),
                'val': y_val_pred.tolist(),
                'test': y_test_pred.tolist()
            },
            'probabilities': {
                'train': y_train_proba.tolist() if y_train_proba is not None else None,
                'val': y_val_proba.tolist() if y_val_proba is not None else None,
                'test': y_test_proba.tolist() if y_test_proba is not None else None
            }
        }
        
        # ê²°ê³¼ ì €ì¥
        self.evaluation_results[model_name] = evaluation_result
        
        return evaluation_result
    
    def compare_models(self, models_dict):
        """
        ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í•¨ìˆ˜
        
        Args:
            models_dict: {ëª¨ë¸ëª…: ëª¨ë¸ê°ì²´} ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: ë¹„êµ ê²°ê³¼
        """
        print(f"\nğŸ”„ {len(models_dict)}ê°œ ëª¨ë¸ ë¹„êµ ì¤‘...")
        
        comparison_results = {}
        
        for model_name, model in models_dict.items():
            print(f"\n{'='*50}")
            print(f"ğŸ“Š {model_name} ëª¨ë¸ í‰ê°€")
            print(f"{'='*50}")
            
            result = self.evaluate_model(
                model, 
                self.X_train, self.y_train,
                self.X_val, self.y_val,
                self.X_test, self.y_test,
                model_name=model_name
            )
            
            comparison_results[model_name] = result
        
        return comparison_results
    
    def generate_evaluation_report(self, output_path=None):
        """
        í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        if not self.evaluation_results:
            print("âš ï¸ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í‰ê°€í•´ì£¼ì„¸ìš”.")
            return
        
        print("\nğŸ“ í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ ê²°ê³¼ ë³´ê³ ì„œ")
        report_lines.append("=" * 80)
        report_lines.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # ë°ì´í„° ë¶„í•  ì •ë³´
        report_lines.append("ğŸ“Š ë°ì´í„° ë¶„í•  ì •ë³´")
        report_lines.append("-" * 40)
        report_lines.append(f"ì „ì²´ ë°ì´í„°: {self.split_info['total_size']:,}ê°œ")
        report_lines.append(f"í›ˆë ¨ ë°ì´í„°: {self.split_info['train_size']:,}ê°œ ({self.split_info['train_ratio']:.1%})")
        report_lines.append(f"ê²€ì¦ ë°ì´í„°: {self.split_info['val_size']:,}ê°œ ({self.split_info['val_ratio']:.1%})")
        report_lines.append(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.split_info['test_size']:,}ê°œ ({self.split_info['test_ratio']:.1%})")
        report_lines.append("")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        report_lines.append("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        report_lines.append("-" * 40)
        
        # í…Œì´ë¸” í—¤ë”
        header = f"{'ëª¨ë¸ëª…':<15} {'ì •í™•ë„':<8} {'ì •ë°€ë„':<8} {'ì¬í˜„ìœ¨':<8} {'F1':<8} {'ROC-AUC':<8} {'í›ˆë ¨ì‹œê°„':<8}"
        report_lines.append(header)
        report_lines.append("-" * len(header))
        
        # ê° ëª¨ë¸ ê²°ê³¼
        for model_name, result in self.evaluation_results.items():
            test_metrics = result['test_metrics']
            accuracy = test_metrics['accuracy']
            precision = test_metrics['precision']
            recall = test_metrics['recall']
            f1 = test_metrics['f1_score']
            roc_auc = test_metrics['roc_auc'] or 0.0
            training_time = result['training_time']
            
            line = f"{model_name:<15} {accuracy:<8.4f} {precision:<8.4f} {recall:<8.4f} {f1:<8.4f} {roc_auc:<8.4f} {training_time:<8.2f}"
            report_lines.append(line)
        
        report_lines.append("")
        
        # êµì°¨ ê²€ì¦ ê²°ê³¼
        report_lines.append("ğŸ”„ êµì°¨ ê²€ì¦ ê²°ê³¼")
        report_lines.append("-" * 40)
        for model_name, result in self.evaluation_results.items():
            cv_results = result['cv_results']
            report_lines.append(f"{model_name}:")
            report_lines.append(f"  í‰ê·  ì ìˆ˜: {cv_results['mean_score']:.4f} Â± {cv_results['std_score']:.4f}")
            report_lines.append(f"  í´ë“œ ì ìˆ˜: {cv_results['fold_scores']}")
            report_lines.append("")
        
        # ë³´ê³ ì„œ ì €ì¥
        report_content = "\n".join(report_lines)
        
        if output_path is None:
            output_path = os.path.join(os.path.dirname(__file__), '..', 'reports', 'model_evaluation_report.txt')
        
        # ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜
        output_path = Path(output_path)
        ensure_directory_exists(output_path.parent)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ“ í‰ê°€ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return report_content

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ ì‹œì‘")
    
    # í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
    framework = ModelEvaluationFramework(random_state=42)
    
    # ë°ì´í„° ë¡œë“œ
    data_result = framework.load_data()
    if data_result is None:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return
    
    X, y = data_result
    
    # Train/Validation/Test Split
    split_result = framework.train_validation_test_split(
        X, y, 
        train_size=0.6, 
        val_size=0.2, 
        test_size=0.2,
        stratify=True
    )
    
    # ê¸°ë³¸ ëª¨ë¸ë“¤ ì •ì˜
    models = {
        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
        'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100)
    }
    
    # ëª¨ë¸ í‰ê°€
    comparison_results = framework.compare_models(models)
    
    # ë³´ê³ ì„œ ìƒì„±
    framework.generate_evaluation_report()
    
    print("\nâœ… ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ ì™„ë£Œ")

if __name__ == "__main__":
    main() 