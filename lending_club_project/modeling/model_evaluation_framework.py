"""
ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ êµ¬í˜„
Train/Validation/Test Split, Cross Validation, ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ë“¤ì„ êµ¬í˜„
ëª¨ë¸ë§ë³„ ë°ì´í„° í™œìš© ì „ëµ ì ìš©
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI ì°½ì´ ì—´ë¦¬ì§€ ì•Šë„ë¡ ì„¤ì •
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import (
    train_test_split, 
    cross_val_score, 
    StratifiedKFold, 
    KFold,
    GridSearchCV,
    RandomizedSearchCV
)
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_auc_score, 
    roc_curve,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
    precision_recall_curve,
    average_precision_score
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import warnings
import sys
import os
from pathlib import Path
import time
from datetime import datetime
import json

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.file_paths import (
    SELECTED_FEATURES_PATH,
    SCALED_STANDARD_DATA_PATH,
    SCALED_MINMAX_DATA_PATH,
    NEW_FEATURES_DATA_PATH,
    BASIC_MODELS_REPORT_PATH,
    ensure_directory_exists,
    file_exists
)
from config.settings import settings

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class ModelEvaluationFramework:
    """ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ í´ë˜ìŠ¤ - ëª¨ë¸ë§ë³„ ë°ì´í„° í™œìš© ì „ëµ ì ìš©"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        self.feature_names = None
        self.split_info = {}
        self.evaluation_results = {}
        
    def get_priority_features(self, priority_level):
        """ìš°ì„ ìˆœìœ„ì— ë”°ë¼ íŠ¹ì„± ì„ íƒ"""
        print(f"ğŸ“Š ìš°ì„ ìˆœìœ„ {priority_level} íŠ¹ì„± ì„ íƒ ì¤‘...")
        
        if not file_exists(SELECTED_FEATURES_PATH):
            print(f"âœ— ì„ íƒëœ íŠ¹ì„± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SELECTED_FEATURES_PATH}")
            return None
            
        selected_features_df = pd.read_csv(SELECTED_FEATURES_PATH)
        
        if priority_level == 1:
            # ìš°ì„ ìˆœìœ„ 1: 9ê°œ í•µì‹¬ íŠ¹ì„± (ìµœìš°ì„ )
            priority_features = selected_features_df[
                selected_features_df['priority'] == 1
            ]['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 1 íŠ¹ì„±: {len(priority_features)}ê°œ")
            
        elif priority_level == 2:
            # ìš°ì„ ìˆœìœ„ 2: 17ê°œ íŠ¹ì„± (1 + 2)
            priority_features = selected_features_df[
                selected_features_df['priority'].isin([1, 2])
            ]['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 2 íŠ¹ì„±: {len(priority_features)}ê°œ")
            
        else:  # priority_level == 3
            # ìš°ì„ ìˆœìœ„ 3: 30ê°œ íŠ¹ì„± (ëª¨ë“  ì„ íƒëœ íŠ¹ì„±)
            priority_features = selected_features_df['selected_feature'].tolist()
            print(f"âœ“ ìš°ì„ ìˆœìœ„ 3 íŠ¹ì„±: {len(priority_features)}ê°œ")
        
        return priority_features
    
    def load_data_for_model(self, model_type):
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ {model_type} ëª¨ë¸ìš© ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ëª¨ë¸ë³„ ë°ì´í„° ì „ëµ ì ìš©
        if model_type == "logistic_regression":
            # ë¡œì§€ìŠ¤í‹± íšŒê·€: StandardScaler + ìš°ì„ ìˆœìœ„ 1
            data_path = SCALED_STANDARD_DATA_PATH
            priority_level = 1
            print("  - StandardScaler ë°ì´í„° ì‚¬ìš© (ì„ í˜• ëª¨ë¸ ìµœì í™”)")
            print("  - ìš°ì„ ìˆœìœ„ 1 íŠ¹ì„± ì‚¬ìš© (í•´ì„ ê°€ëŠ¥ì„± ì¤‘ì‹œ)")
            
        elif model_type == "random_forest":
            # ëœë¤í¬ë ˆìŠ¤íŠ¸: MinMaxScaler + ìš°ì„ ìˆœìœ„ 1
            data_path = SCALED_MINMAX_DATA_PATH
            priority_level = 1
            print("  - MinMaxScaler ë°ì´í„° ì‚¬ìš© (íŠ¸ë¦¬ ëª¨ë¸ ìµœì í™”)")
            print("  - ìš°ì„ ìˆœìœ„ 1 íŠ¹ì„± ì‚¬ìš© (ì•ˆì •ì„± ì¤‘ì‹œ)")
            
        elif model_type in ["xgboost", "lightgbm"]:
            # XGBoost/LightGBM: ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ + ìš°ì„ ìˆœìœ„ 2
            data_path = NEW_FEATURES_DATA_PATH
            priority_level = 2
            print("  - ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ ë°ì´í„° ì‚¬ìš© (ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ)")
            print("  - ìš°ì„ ìˆœìœ„ 2 íŠ¹ì„± ì‚¬ìš© (ì„±ëŠ¥ê³¼ í•´ì„ì˜ ê· í˜•)")
            
        else:  # ensemble
            # ì•™ìƒë¸”: ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ + ìš°ì„ ìˆœìœ„ 3
            data_path = NEW_FEATURES_DATA_PATH
            priority_level = 3
            print("  - ìƒˆë¡œìš´ íŠ¹ì„± í¬í•¨ ë°ì´í„° ì‚¬ìš© (ìµœëŒ€ ì„±ëŠ¥)")
            print("  - ìš°ì„ ìˆœìœ„ 3 íŠ¹ì„± ì‚¬ìš© (ëª¨ë“  ì„ íƒ íŠ¹ì„±)")
        
        # ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not file_exists(data_path):
            print(f"âœ— ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            print("ë¨¼ì € feature_engineering ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        df['loan_status_binary'] = df['loan_status'].apply(
            lambda x: 1 if x in ['Default', 'Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'] else 0
        )
        
        # ìš°ì„ ìˆœìœ„ë³„ íŠ¹ì„± ì„ íƒ
        priority_features = self.get_priority_features(priority_level)
        if priority_features is None:
            return None
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„± í•„í„°ë§
        available_features = [f for f in priority_features if f in df.columns]
        print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {len(available_features)}ê°œ")
        
        X = df[available_features]
        y = df['loan_status_binary']
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        total_missing = X.isnull().sum().sum()
        if total_missing > 0:
            print(f"âš ï¸ ê²½ê³ : {total_missing}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("   feature_engineering_step2_scaling.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return None
        else:
            print("âœ“ ê²°ì¸¡ì¹˜ ì—†ìŒ - ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©")
        
        return X, y, available_features
    
    def load_data(self):
        """ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
        print("ğŸ“‚ ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì„ íƒëœ íŠ¹ì„± ë¡œë“œ
        if not file_exists(SELECTED_FEATURES_PATH):
            print(f"âœ— ì„ íƒëœ íŠ¹ì„± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SELECTED_FEATURES_PATH}")
            print("ë¨¼ì € feature_selection_analysis.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
            
        selected_features_df = pd.read_csv(SELECTED_FEATURES_PATH)
        selected_features = selected_features_df['selected_feature'].tolist()
        
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ë¡œë“œ
        if not file_exists(SCALED_STANDARD_DATA_PATH):
            print(f"âœ— ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SCALED_STANDARD_DATA_PATH}")
            print("ë¨¼ì € feature_engineering_step2_scaling.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return None
            
        df = pd.read_csv(SCALED_STANDARD_DATA_PATH)
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        df['loan_status_binary'] = df['loan_status'].apply(
            lambda x: 1 if x in ['Default', 'Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'] else 0
        )
        
        # ì„ íƒëœ íŠ¹ì„±ë§Œ ì‚¬ìš©
        available_features = [f for f in selected_features if f in df.columns]
        print(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±: {len(available_features)}ê°œ")
        
        X = df[available_features]
        y = df['loan_status_binary']
        
        # ê²°ì¸¡ì¹˜ í™•ì¸ (ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨)
        total_missing = X.isnull().sum().sum()
        if total_missing > 0:
            print(f"âš ï¸ ê²½ê³ : {total_missing}ê°œì˜ ê²°ì¸¡ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("   feature_engineering_step2_scaling.pyë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return None
        else:
            print("âœ“ ê²°ì¸¡ì¹˜ ì—†ìŒ - ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©")
        
        return X, y, available_features
    
    def train_validation_test_split(self, X, y, train_size=0.6, val_size=0.2, test_size=0.2, 
                                   stratify=True, random_state=None):
        """
        Train/Validation/Test Split í•¨ìˆ˜
        
        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            train_size: í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.6)
            val_size: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2)
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.2)
            stratify: ê³„ì¸µí™” ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            random_state: ëœë¤ ì‹œë“œ
            
        Returns:
            dict: ë¶„í• ëœ ë°ì´í„°ì…‹ ì •ë³´
        """
        if random_state is None:
            random_state = self.random_state
            
        print(f"ğŸ”„ Train/Validation/Test Split ì§„í–‰ ì¤‘...")
        print(f"   í›ˆë ¨: {train_size:.1%}, ê²€ì¦: {val_size:.1%}, í…ŒìŠ¤íŠ¸: {test_size:.1%}")
        
        # ë¹„ìœ¨ ê²€ì¦
        total_ratio = train_size + val_size + test_size
        if abs(total_ratio - 1.0) > 1e-6:
            print(f"âš ï¸ ê²½ê³ : ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤ ({total_ratio:.3f})")
            print("   ë¹„ìœ¨ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.")
            train_size = train_size / total_ratio
            val_size = val_size / total_ratio
            test_size = test_size / total_ratio
        
        # 1ë‹¨ê³„: Train + (Validation + Test) ë¶„í• 
        if stratify:
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, 
                test_size=test_size, 
                random_state=random_state,
                stratify=y
            )
        else:
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, 
                test_size=test_size, 
                random_state=random_state
            )
        
        # 2ë‹¨ê³„: Train + Validation ë¶„í• 
        val_ratio = val_size / (train_size + val_size)
        if stratify:
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp,
                test_size=val_ratio,
                random_state=random_state,
                stratify=y_temp
            )
        else:
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp,
                test_size=val_ratio,
                random_state=random_state
            )
        
        # ê²°ê³¼ ì €ì¥
        self.X_train = X_train
        self.X_val = X_val
        self.X_test = X_test
        self.y_train = y_train
        self.y_val = y_val
        self.y_test = y_test
        
        # ë¶„í•  ì •ë³´ ì €ì¥
        self.split_info = {
            'train_size': len(X_train),
            'val_size': len(X_val),
            'test_size': len(X_test),
            'total_size': len(X),
            'train_ratio': len(X_train) / len(X),
            'val_ratio': len(X_val) / len(X),
            'test_ratio': len(X_test) / len(X),
            'train_class_distribution': y_train.value_counts().to_dict(),
            'val_class_distribution': y_val.value_counts().to_dict(),
            'test_class_distribution': y_test.value_counts().to_dict(),
            'stratify': stratify,
            'random_state': random_state
        }
        
        # ë¶„í•  ê²°ê³¼ ì¶œë ¥
        self._print_split_summary()
        
        return {
            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
            'y_train': y_train, 'y_val': y_val, 'y_test': y_test,
            'split_info': self.split_info
        }
    
    def _print_split_summary(self):
        """ë¶„í•  ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼:")
        print(f"   ì „ì²´ ë°ì´í„°: {self.split_info['total_size']:,}ê°œ")
        print(f"   í›ˆë ¨ ë°ì´í„°: {self.split_info['train_size']:,}ê°œ ({self.split_info['train_ratio']:.1%})")
        print(f"   ê²€ì¦ ë°ì´í„°: {self.split_info['val_size']:,}ê°œ ({self.split_info['val_ratio']:.1%})")
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.split_info['test_size']:,}ê°œ ({self.split_info['test_ratio']:.1%})")
        
        print("\nğŸ“ˆ í´ë˜ìŠ¤ ë¶„í¬:")
        train_dist = self.split_info['train_class_distribution']
        val_dist = self.split_info['val_class_distribution']
        test_dist = self.split_info['test_class_distribution']
        
        print(f"   í›ˆë ¨ - ì •ìƒ: {train_dist.get(0, 0):,}ê°œ, ë¶€ë„: {train_dist.get(1, 0):,}ê°œ")
        print(f"   ê²€ì¦ - ì •ìƒ: {val_dist.get(0, 0):,}ê°œ, ë¶€ë„: {val_dist.get(1, 0):,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ - ì •ìƒ: {test_dist.get(0, 0):,}ê°œ, ë¶€ë„: {test_dist.get(1, 0):,}ê°œ")
        
        # í´ë˜ìŠ¤ ë¹„ìœ¨ ê³„ì‚°
        train_ratio = train_dist.get(1, 0) / (train_dist.get(0, 0) + train_dist.get(1, 0))
        val_ratio = val_dist.get(1, 0) / (val_dist.get(0, 0) + val_dist.get(1, 0))
        test_ratio = test_dist.get(1, 0) / (test_dist.get(0, 0) + test_dist.get(1, 0))
        
        print(f"   ë¶€ë„ìœ¨ - í›ˆë ¨: {train_ratio:.3f}, ê²€ì¦: {val_ratio:.3f}, í…ŒìŠ¤íŠ¸: {test_ratio:.3f}")
    
    def cross_validation(self, model, X, y, cv=5, scoring='roc_auc', n_jobs=-1):
        """
        Cross Validation í•¨ìˆ˜
        
        Args:
            model: í›ˆë ¨í•  ëª¨ë¸
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)
            scoring: í‰ê°€ ì§€í‘œ (ê¸°ë³¸ê°’: 'roc_auc')
            n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: -1, ëª¨ë“  CPU ì‚¬ìš©)
            
        Returns:
            dict: êµì°¨ ê²€ì¦ ê²°ê³¼
        """
        print(f"ğŸ”„ {cv}-Fold Cross Validation ì§„í–‰ ì¤‘...")
        print(f"   í‰ê°€ ì§€í‘œ: {scoring}")
        
        # StratifiedKFold ì‚¬ìš© (ë¶„ë¥˜ ë¬¸ì œ)
        if scoring in ['roc_auc', 'accuracy', 'precision', 'recall', 'f1']:
            kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        else:
            kfold = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)
        
        # êµì°¨ ê²€ì¦ ìˆ˜í–‰
        cv_scores = cross_val_score(model, X, y, cv=kfold, scoring=scoring, n_jobs=n_jobs)
        
        # ê²°ê³¼ ê³„ì‚°
        mean_score = cv_scores.mean()
        std_score = cv_scores.std()
        
        # ê²°ê³¼ ì €ì¥
        cv_results = {
            'scores': cv_scores,
            'mean_score': mean_score,
            'std_score': std_score,
            'cv_folds': cv,
            'scoring': scoring,
            'fold_scores': cv_scores.tolist()
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ“ êµì°¨ ê²€ì¦ ì™„ë£Œ")
        print(f"   í‰ê·  ì ìˆ˜: {mean_score:.4f} Â± {std_score:.4f}")
        print(f"   ìµœì†Œ ì ìˆ˜: {cv_scores.min():.4f}")
        print(f"   ìµœëŒ€ ì ìˆ˜: {cv_scores.max():.4f}")
        
        return cv_results
    
    def calculate_performance_metrics(self, y_true, y_pred, y_pred_proba=None):
        """
        ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜
        
        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred: ì˜ˆì¸¡ ê°’
            y_pred_proba: ì˜ˆì¸¡ í™•ë¥  (ì„ íƒì‚¬í•­)
            
        Returns:
            dict: ì„±ëŠ¥ ì§€í‘œ
        """
        print("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        
        # ê¸°ë³¸ ë¶„ë¥˜ ì§€í‘œ
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # ì¶”ê°€ ì§€í‘œ
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # ROC-AUC (í™•ë¥ ì´ ì œê³µëœ ê²½ìš°)
        roc_auc = None
        if y_pred_proba is not None:
            roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        # Precision-Recall AUC
        pr_auc = None
        if y_pred_proba is not None:
            pr_auc = average_precision_score(y_true, y_pred_proba)
        
        # ê²°ê³¼ ì €ì¥
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'specificity': specificity,
            'sensitivity': sensitivity,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'confusion_matrix': cm.tolist(),
            'tn': int(tn),
            'fp': int(fp),
            'fn': int(fn),
            'tp': int(tp)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ“ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        print(f"   ì •í™•ë„: {accuracy:.4f}")
        print(f"   ì •ë°€ë„: {precision:.4f}")
        print(f"   ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"   F1 ì ìˆ˜: {f1:.4f}")
        if roc_auc is not None:
            print(f"   ROC-AUC: {roc_auc:.4f}")
        if pr_auc is not None:
            print(f"   PR-AUC: {pr_auc:.4f}")
        
        return metrics
    
    def evaluate_model(self, model, X_train, y_train, X_val, y_val, X_test, y_test, 
                      model_name="Model", cv_folds=5):
        """
        ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ (í†µí•©)
        
        Args:
            model: í‰ê°€í•  ëª¨ë¸
            X_train, y_train: í›ˆë ¨ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„°
            X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            model_name: ëª¨ë¸ ì´ë¦„
            cv_folds: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            
        Returns:
            dict: í‰ê°€ ê²°ê³¼
        """
        print(f"\nğŸ” {model_name} ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # 1. ëª¨ë¸ í›ˆë ¨
        start_time = time.time()
        model.fit(X_train, y_train)
        training_time = time.time() - start_time
        
        # 2. ì˜ˆì¸¡
        y_train_pred = model.predict(X_train)
        y_val_pred = model.predict(X_val)
        y_test_pred = model.predict(X_test)
        
        # í™•ë¥  ì˜ˆì¸¡ (ê°€ëŠ¥í•œ ê²½ìš°)
        y_train_proba = None
        y_val_proba = None
        y_test_proba = None
        
        try:
            y_train_proba = model.predict_proba(X_train)[:, 1]
            y_val_proba = model.predict_proba(X_val)[:, 1]
            y_test_proba = model.predict_proba(X_test)[:, 1]
        except:
            print("âš ï¸ í™•ë¥  ì˜ˆì¸¡ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 3. êµì°¨ ê²€ì¦
        cv_results = self.cross_validation(model, X_train, y_train, cv=cv_folds)
        
        # 4. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        train_metrics = self.calculate_performance_metrics(y_train, y_train_pred, y_train_proba)
        val_metrics = self.calculate_performance_metrics(y_val, y_val_pred, y_val_proba)
        test_metrics = self.calculate_performance_metrics(y_test, y_test_pred, y_test_proba)
        
        # 5. ê²°ê³¼ í†µí•©
        evaluation_result = {
            'model_name': model_name,
            'training_time': training_time,
            'cv_results': cv_results,
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'predictions': {
                'train': y_train_pred.tolist(),
                'val': y_val_pred.tolist(),
                'test': y_test_pred.tolist()
            },
            'probabilities': {
                'train': y_train_proba.tolist() if y_train_proba is not None else None,
                'val': y_val_proba.tolist() if y_val_proba is not None else None,
                'test': y_test_proba.tolist() if y_test_proba is not None else None
            }
        }
        
        # ê²°ê³¼ ì €ì¥
        self.evaluation_results[model_name] = evaluation_result
        
        return evaluation_result
    
    def compare_models(self, models_dict):
        """
        ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í•¨ìˆ˜
        
        Args:
            models_dict: {ëª¨ë¸ëª…: ëª¨ë¸ê°ì²´} ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: ë¹„êµ ê²°ê³¼
        """
        print(f"\nğŸ”„ {len(models_dict)}ê°œ ëª¨ë¸ ë¹„êµ ì¤‘...")
        
        comparison_results = {}
        
        for model_name, model in models_dict.items():
            print(f"\n{'='*50}")
            print(f"ğŸ“Š {model_name} ëª¨ë¸ í‰ê°€")
            print(f"{'='*50}")
            
            result = self.evaluate_model(
                model, 
                self.X_train, self.y_train,
                self.X_val, self.y_val,
                self.X_test, self.y_test,
                model_name=model_name
            )
            
            comparison_results[model_name] = result
        
        return comparison_results
    
    def generate_evaluation_report(self, output_path=None):
        """
        í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        if not self.evaluation_results:
            print("âš ï¸ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í‰ê°€í•´ì£¼ì„¸ìš”.")
            return
        
        print("\nğŸ“ í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ ê²°ê³¼ ë³´ê³ ì„œ")
        report_lines.append("=" * 80)
        report_lines.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # ë°ì´í„° ë¶„í•  ì •ë³´
        report_lines.append("ğŸ“Š ë°ì´í„° ë¶„í•  ì •ë³´")
        report_lines.append("-" * 40)
        report_lines.append(f"ì „ì²´ ë°ì´í„°: {self.split_info['total_size']:,}ê°œ")
        report_lines.append(f"í›ˆë ¨ ë°ì´í„°: {self.split_info['train_size']:,}ê°œ ({self.split_info['train_ratio']:.1%})")
        report_lines.append(f"ê²€ì¦ ë°ì´í„°: {self.split_info['val_size']:,}ê°œ ({self.split_info['val_ratio']:.1%})")
        report_lines.append(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.split_info['test_size']:,}ê°œ ({self.split_info['test_ratio']:.1%})")
        report_lines.append("")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        report_lines.append("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        report_lines.append("-" * 40)
        
        # í…Œì´ë¸” í—¤ë”
        header = f"{'ëª¨ë¸ëª…':<15} {'ì •í™•ë„':<8} {'ì •ë°€ë„':<8} {'ì¬í˜„ìœ¨':<8} {'F1':<8} {'ROC-AUC':<8} {'í›ˆë ¨ì‹œê°„':<8}"
        report_lines.append(header)
        report_lines.append("-" * len(header))
        
        # ê° ëª¨ë¸ ê²°ê³¼
        for model_name, result in self.evaluation_results.items():
            test_metrics = result['test_metrics']
            accuracy = test_metrics['accuracy']
            precision = test_metrics['precision']
            recall = test_metrics['recall']
            f1 = test_metrics['f1_score']
            roc_auc = test_metrics['roc_auc'] or 0.0
            training_time = result['training_time']
            
            line = f"{model_name:<15} {accuracy:<8.4f} {precision:<8.4f} {recall:<8.4f} {f1:<8.4f} {roc_auc:<8.4f} {training_time:<8.2f}"
            report_lines.append(line)
        
        report_lines.append("")
        
        # êµì°¨ ê²€ì¦ ê²°ê³¼
        report_lines.append("ğŸ”„ êµì°¨ ê²€ì¦ ê²°ê³¼")
        report_lines.append("-" * 40)
        for model_name, result in self.evaluation_results.items():
            cv_results = result['cv_results']
            report_lines.append(f"{model_name}:")
            report_lines.append(f"  í‰ê·  ì ìˆ˜: {cv_results['mean_score']:.4f} Â± {cv_results['std_score']:.4f}")
            report_lines.append(f"  í´ë“œ ì ìˆ˜: {cv_results['fold_scores']}")
            report_lines.append("")
        
        # ë³´ê³ ì„œ ì €ì¥
        report_content = "\n".join(report_lines)
        
        if output_path is None:
            output_path = os.path.join(os.path.dirname(__file__), '..', 'reports', 'model_evaluation_report.txt')
        
        # ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜
        output_path = Path(output_path)
        ensure_directory_exists(output_path.parent)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ“ í‰ê°€ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return report_content

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ ì‹œì‘")
    
    # í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
    framework = ModelEvaluationFramework(random_state=42)
    
    # ë°ì´í„° ë¡œë“œ
    data_result = framework.load_data()
    if data_result is None:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return
    
    X, y, available_features = data_result
    
    # Train/Validation/Test Split
    split_result = framework.train_validation_test_split(
        X, y, 
        train_size=0.6, 
        val_size=0.2, 
        test_size=0.2,
        stratify=True
    )
    
    # ê¸°ë³¸ ëª¨ë¸ë“¤ ì •ì˜
    models = {
        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
        'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100)
    }
    
    # ëª¨ë¸ í‰ê°€
    comparison_results = framework.compare_models(models)
    
    # ë³´ê³ ì„œ ìƒì„±
    framework.generate_evaluation_report()
    
    print("\nâœ… ëª¨ë¸ í‰ê°€ í”„ë ˆì„ì›Œí¬ ì™„ë£Œ")

if __name__ == "__main__":
    main() 