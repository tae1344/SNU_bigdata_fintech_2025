#!/usr/bin/env python3
"""
ë³€ìˆ˜ë³„ ì „ì²˜ë¦¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ê° ë³€ìˆ˜ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì „ì²˜ë¦¬ ì „ëµì„ ì œì‹œ
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

def analyze_variables():
    """ë³€ìˆ˜ë³„ ìƒì„¸ ë¶„ì„"""
    print("=" * 80)
    print("ë³€ìˆ˜ë³„ ì „ì²˜ë¦¬ ë¶„ì„")
    print("=" * 80)
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('data/lending_club_sample.csv', low_memory=False)
    
    print(f"ğŸ“Š ê¸°ë³¸ ì •ë³´:")
    print(f"  ì´ ë³€ìˆ˜ ìˆ˜: {len(df.columns)}")
    print(f"  ì´ í–‰ ìˆ˜: {len(df)}")
    
    # 1. ë°ì´í„° íƒ€ì…ë³„ ë¶„ë¥˜
    print(f"\nğŸ“‹ ë°ì´í„° íƒ€ì…ë³„ ë¶„ë¥˜:")
    dtype_counts = df.dtypes.value_counts()
    for dtype, count in dtype_counts.items():
        print(f"  {dtype}: {count}ê°œ")
    
    # 2. ê²°ì¸¡ì¹˜ ë¶„ì„
    print(f"\nğŸ” ê²°ì¸¡ì¹˜ ë¶„ì„:")
    missing_vars = df.columns[df.isnull().any()].tolist()
    print(f"  ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜ ìˆ˜: {len(missing_vars)}")
    
    if missing_vars:
        print("  ìƒìœ„ 10ê°œ ê²°ì¸¡ì¹˜ ë³€ìˆ˜:")
        missing_info = []
        for var in missing_vars:
            missing_pct = df[var].isnull().sum() / len(df) * 100
            missing_info.append((var, missing_pct))
        
        missing_info.sort(key=lambda x: x[1], reverse=True)
        for var, pct in missing_info[:10]:
            print(f"    {var}: {pct:.2f}%")
    
    # 3. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„ì„
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    print(f"\nğŸ“ˆ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„ì„ ({len(numeric_cols)}ê°œ):")
    
    # ì´ìƒê°’ì´ ë§ì€ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì°¾ê¸°
    outlier_vars = []
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
        outlier_pct = len(outliers) / len(df) * 100
        if outlier_pct > 5:  # 5% ì´ìƒ ì´ìƒê°’
            outlier_vars.append((col, outlier_pct))
    
    outlier_vars.sort(key=lambda x: x[1], reverse=True)
    print(f"  ì´ìƒê°’ì´ ë§ì€ ë³€ìˆ˜ (5% ì´ìƒ):")
    for var, pct in outlier_vars[:10]:
        print(f"    {var}: {pct:.1f}%")
    
    # 4. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„
    categorical_cols = df.select_dtypes(include=['object']).columns
    print(f"\nğŸ“ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„ ({len(categorical_cols)}ê°œ):")
    
    # ê³ ìœ ê°’ì´ ë§ì€ ë²”ì£¼í˜• ë³€ìˆ˜
    high_cardinality = []
    for col in categorical_cols:
        unique_count = df[col].nunique()
        if unique_count > 20:  # 20ê°œ ì´ìƒ ê³ ìœ ê°’
            high_cardinality.append((col, unique_count))
    
    high_cardinality.sort(key=lambda x: x[1], reverse=True)
    print(f"  ê³ ìœ ê°’ì´ ë§ì€ ë³€ìˆ˜ (20ê°œ ì´ìƒ):")
    for var, count in high_cardinality[:10]:
        print(f"    {var}: {count}ê°œ")
    
    # 5. ë³€ìˆ˜ë³„ íŠ¹ì„± ë¶„ë¥˜
    print(f"\nğŸ¯ ë³€ìˆ˜ë³„ íŠ¹ì„± ë¶„ë¥˜:")
    
    # ê¸ˆì•¡ ê´€ë ¨ ë³€ìˆ˜
    amount_vars = [col for col in df.columns if any(keyword in col.lower() for keyword in ['amnt', 'bal', 'pymnt', 'total', 'revol'])]
    print(f"  ê¸ˆì•¡ ê´€ë ¨ ë³€ìˆ˜: {len(amount_vars)}ê°œ")
    
    # ë¹„ìœ¨ ê´€ë ¨ ë³€ìˆ˜
    ratio_vars = [col for col in df.columns if any(keyword in col.lower() for keyword in ['util', 'ratio', 'pct', 'percent'])]
    print(f"  ë¹„ìœ¨ ê´€ë ¨ ë³€ìˆ˜: {len(ratio_vars)}ê°œ")
    
    # ê°œìˆ˜ ê´€ë ¨ ë³€ìˆ˜
    count_vars = [col for col in df.columns if any(keyword in col.lower() for keyword in ['num_', 'count', 'acc', 'inq'])]
    print(f"  ê°œìˆ˜ ê´€ë ¨ ë³€ìˆ˜: {len(count_vars)}ê°œ")
    
    # ì‹œê°„ ê´€ë ¨ ë³€ìˆ˜
    time_vars = [col for col in df.columns if any(keyword in col.lower() for keyword in ['mths', 'months', 'year', 'date', 'since'])]
    print(f"  ì‹œê°„ ê´€ë ¨ ë³€ìˆ˜: {len(time_vars)}ê°œ")
    
    # FICO ê´€ë ¨ ë³€ìˆ˜
    fico_vars = [col for col in df.columns if 'fico' in col.lower()]
    print(f"  FICO ê´€ë ¨ ë³€ìˆ˜: {len(fico_vars)}ê°œ")
    
    return df, numeric_cols, categorical_cols, missing_vars, outlier_vars

def create_preprocessing_strategy():
    """ì „ì²˜ë¦¬ ì „ëµ ìƒì„±"""
    print("\n" + "=" * 80)
    print("ì „ì²˜ë¦¬ ì „ëµ ê°€ì´ë“œ")
    print("=" * 80)
    
    print("\nğŸ”§ 1. ê³µí†µ ì „ì²˜ë¦¬ ê³¼ì • (ëª¨ë“  ë³€ìˆ˜)")
    print("   A. ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬")
    print("   B. ë°ì´í„° íƒ€ì… ê²€ì¦")
    print("   C. ê¸°ë³¸ ë°ì´í„° ì •ì œ")
    print("   D. ì¤‘ë³µ ë°ì´í„° ì œê±°")
    
    print("\nğŸ“Š 2. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë³„ ì „ì²˜ë¦¬")
    print("   A. ì´ìƒê°’ íƒì§€ ë° ì²˜ë¦¬")
    print("   B. ë¶„í¬ ë¶„ì„ ë° ë³€í™˜")
    print("   C. ìŠ¤ì¼€ì¼ë§/ì •ê·œí™”")
    print("   D. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§")
    
    print("\nğŸ“ 3. ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ì „ì²˜ë¦¬")
    print("   A. ê³ ìœ ê°’ ë¶„ì„")
    print("   B. ì¸ì½”ë”© ë°©ë²• ì„ íƒ")
    print("   C. í¬ê·€ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬")
    print("   D. ìˆœì„œí˜• ë³€ìˆ˜ ì²˜ë¦¬")
    
    print("\nğŸ¯ 4. ë³€ìˆ˜ ìœ í˜•ë³„ íŠ¹í™” ì „ì²˜ë¦¬")
    print("   A. ê¸ˆì•¡ ë³€ìˆ˜: ë¡œê·¸ ë³€í™˜, í†µí™” ì •ê·œí™”")
    print("   B. ë¹„ìœ¨ ë³€ìˆ˜: ë²”ìœ„ ì œí•œ, ì´ìƒê°’ í´ë¦¬í•‘")
    print("   C. ê°œìˆ˜ ë³€ìˆ˜: ì´ì§„í™”, êµ¬ê°„í™”")
    print("   D. ì‹œê°„ ë³€ìˆ˜: ë‚ ì§œ íŒŒì‹±, ê¸°ê°„ ê³„ì‚°")
    print("   E. FICO ë³€ìˆ˜: ì ìˆ˜ ë²”ìœ„ ê²€ì¦, í‰ê·  ê³„ì‚°")

if __name__ == "__main__":
    df, numeric_cols, categorical_cols, missing_vars, outlier_vars = analyze_variables()
    create_preprocessing_strategy() 