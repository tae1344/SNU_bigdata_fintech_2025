{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8b00dc-90e1-4e62-b218-10180fff2319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e0ae62-a103-46a3-81e0-a69b94a0f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tenacity --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f390ab-b6da-4f5e-9bc6-f634c99002da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6b2d4f-7912-49a4-b116-9bd3da0f9190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install termcolor --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34983ba-7e7d-4884-9aae-c711012d9ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd3fe42-53c4-4078-9307-cd930549b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc09b0-bc59-419b-b1b3-51ea6bc26420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored  \n",
    "import requests\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "#GPT_MODEL = \"gpt-4o\"\n",
    "client = OpenAI()\n",
    "\n",
    "#First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state.\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n",
    "\n",
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    \n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"): #GPT가 함수를 호출하는 경우\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"): #GPT 일반 응답\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"function\": #함수결과 반환 메시지\n",
    "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "\n",
    "#Let's create some function specifications to interface with a weather API. \n",
    "#We'll pass these function specification to the Chat Completions API in order to generate function arguments that adhere to the specification.\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "#Define the OpenWeatherMap API key and endpoint\n",
    "#API_KEY = \"your_openweathermap_api_key\"\n",
    "API_KEY = \"api-key\" #Please create your own API key and use it\n",
    "CURRENT_WEATHER_URL = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "#FORECAST_WEATHER_URL = \"https://api.openweathermap.org/data/2.5/forecast\"\n",
    "\n",
    "\n",
    "#Now let's implement the functions that will actually fetch the real-time data from the OpenWeatherMap API.\n",
    "#Function to get current weather\n",
    "def get_current_weather(location, format=\"celsius\"):\n",
    "    units = \"metric\" if format == \"celsius\" else \"imperial\"\n",
    "    params = {\n",
    "        \"q\": location,\n",
    "        \"appid\": API_KEY,\n",
    "        \"units\": units\n",
    "    }\n",
    "    response = requests.get(CURRENT_WEATHER_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # The data fetched from OpenWeatherMap\n",
    "    else:\n",
    "        return {\"error\": \"Failed to fetch current weather data\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e77991-5e2e-4902-a0f1-d312cb28cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps to invoke a function call using Chat Completions API:\n",
    "#Step 1: Prompt the model with content that may result in model selecting a tool to use. \n",
    "    #The description of the tools such as a function names and signature is defined in the 'Tools' list and passed to the model in API call. \n",
    "    #If selected, the function name and parameters are included in the response.\n",
    "#Step 2: Check programmatically if model wanted to call a function. If true, proceed to step 3.\n",
    "#Step 3: Extract the function name and parameters from response, call the function with parameters. Append the result to messages.\n",
    "#Step 4: Invoke the chat completions API with the message list to get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4d232d-13a8-4121-bc3f-3722ee34ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: When real-time information from external sources or functions is available, use them. You are perfectly capable of doing so. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: You are a very thorough assistant who pays close attention to the conversation history. \n",
      "\u001b[0m\n",
      "\u001b[32muser: What's the weather like today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Sure, could you please provide me with the city and state where you are currently located so I can check the weather for you?\n",
      "\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prompt the model with content that may result in model selecting a tool to use. \n",
    "#The description of the tools such as a function names and signature is defined in the 'Tools' list and passed to the model in API call. \n",
    "#If selected, the function name and parameters are included in the response.\n",
    "\n",
    "messages = []\n",
    "\n",
    "messages.append({\"role\": \"system\", \\\n",
    "                 \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. \"})\n",
    "messages.append({\"role\": \"system\", \\\n",
    "                 \"content\": \"When real-time information from external sources or functions is available, use them. You are perfectly capable of doing so. \"})\n",
    "messages.append({\"role\": \"system\", \\\n",
    "                 \"content\": \"You are a very thorough assistant who pays close attention to the conversation history. \"})\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today?\"})\n",
    "\n",
    "chat_response = chat_completion_request(\n",
    "    messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Handle function response\n",
    "response_message = chat_response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    # If true the model will return the name of the tool / function to call and the argument(s)  \n",
    "    tool_call_id = tool_calls[0].id\n",
    "    tool_function_name = tool_calls[0].function.name\n",
    "    \n",
    "    if tool_function_name == \"get_current_weather\":\n",
    "        location = json.loads(tool_calls[0].function.arguments)['location']\n",
    "        format = json.loads(tool_calls[0].function.arguments)['format']\n",
    "        results = get_current_weather(location, format)\n",
    "\n",
    "        weather_main = results['weather'][0]['main']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        temperature = results['main']['temp']\n",
    "\n",
    "        if format == \"celsius\":\n",
    "            temp_unit = \"°C\"\n",
    "        else:\n",
    "            temp_unit = \"°F\"\n",
    "\n",
    "        formatted_weather = (\n",
    "            f\"To generate the response, I am going to use the information that the current weather in {location} is {weather_main} ({weather_description})\"\n",
    "            f\"The temperature is {temperature}{temp_unit}.\"\n",
    "            f\"I am always capable of providing real-time data. Now I answer your earlier question.\"\n",
    "        )\n",
    "\n",
    "        messages.append({\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\": formatted_weather \n",
    "        })\n",
    "\n",
    "        model_response_with_function_call = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "        )  \n",
    "        # get a new response from the model where it can see the function response\n",
    "\n",
    "        role = model_response_with_function_call.choices[0].message.role\n",
    "        content = model_response_with_function_call.choices[0].message.content\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\" : role,\n",
    "            \"content\": content\n",
    "        })\n",
    "        \n",
    "        pretty_print_conversation(messages)\n",
    "        print(\"\\n\")\n",
    "    else: \n",
    "        print(f\"Error: function {tool_function_name} does not exist\")\n",
    "else: \n",
    "    # Model did not identify a function to call, result can be returned to the user \n",
    "    role = response_message.role\n",
    "    content = response_message.content\n",
    "    messages.append({\n",
    "        \"role\" : role,\n",
    "        \"content\": content\n",
    "    })\n",
    "    pretty_print_conversation(messages)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4d5ca7-66be-4857-8771-c41fbf831c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: When real-time information from external sources or functions is available, use them. You are perfectly capable of doing so. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: You are a very thorough assistant who pays close attention to the conversation history. \n",
      "\u001b[0m\n",
      "\u001b[32muser: What's the weather like today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Sure, could you please provide me with the city and state where you are currently located so I can check the weather for you?\n",
      "\u001b[0m\n",
      "\u001b[32muser: I am currently in Seoul, Korea, and will be so for a while.\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To generate the response, I am going to use the information that the current weather in Seoul, Korea is Clear (clear sky). The temperature is 20.76°C. I am always capable of providing real-time data. Now I answer your earlier question. \n",
      "\u001b[0m\n",
      "\u001b[34massistant: Today in Seoul, Korea, the weather is clear with a temperature of 20.76°C.\n",
      "\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"I am currently in Seoul, Korea, and will be so for a while.\"})\n",
    "\n",
    "chat_response = chat_completion_request(\n",
    "    messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Handle function response\n",
    "response_message = chat_response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    # If true the model will return the name of the tool / function to call and the argument(s)  \n",
    "    tool_call_id = tool_calls[0].id\n",
    "    tool_function_name = tool_calls[0].function.name\n",
    "    \n",
    "    if tool_function_name == \"get_current_weather\":\n",
    "        location = json.loads(tool_calls[0].function.arguments)['location']\n",
    "        format = json.loads(tool_calls[0].function.arguments)['format']\n",
    "        results = get_current_weather(location, format)\n",
    "\n",
    "        weather_main = results['weather'][0]['main']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        temperature = results['main']['temp']\n",
    "\n",
    "        if format == \"celsius\":\n",
    "            temp_unit = \"°C\"\n",
    "        else:\n",
    "            temp_unit = \"°F\"\n",
    "\n",
    "        formatted_weather = (\n",
    "            f\"To generate the response, I am going to use the information that the current weather in {location} is {weather_main} ({weather_description}). \"\n",
    "            f\"The temperature is {temperature}{temp_unit}. \"\n",
    "            f\"I am always capable of providing real-time data. Now I answer your earlier question. \"\n",
    "        )\n",
    "\n",
    "        messages.append({\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\": formatted_weather \n",
    "        })\n",
    "\n",
    "        model_response_with_function_call = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "        )  \n",
    "        # get a new response from the model where it can see the function response\n",
    "\n",
    "        #role = role\n",
    "        role = model_response_with_function_call.choices[0].message.role\n",
    "        content = model_response_with_function_call.choices[0].message.content\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\" : role,\n",
    "            \"content\": content\n",
    "        })\n",
    "        \n",
    "        pretty_print_conversation(messages)\n",
    "        print(\"\\n\")\n",
    "    else: \n",
    "        print(f\"Error: function {tool_function_name} does not exist\")\n",
    "else: \n",
    "    # Model did not identify a function to call, result can be returned to the user \n",
    "    role = response_message.role\n",
    "    content = response_message.content\n",
    "    messages.append({\n",
    "        \"role\" : role,\n",
    "        \"content\": content\n",
    "    })\n",
    "    pretty_print_conversation(messages)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10977ea3-3d04-4502-b0b6-232a5c166dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: When real-time information from external sources or functions is available, use them. You are perfectly capable of doing so. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: You are a very thorough assistant who pays close attention to the conversation history. \n",
      "\u001b[0m\n",
      "\u001b[32muser: What's the weather like today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Sure, could you please provide me with the city and state where you are currently located so I can check the weather for you?\n",
      "\u001b[0m\n",
      "\u001b[32muser: I am currently in Seoul, Korea, and will be so for a while.\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To generate the response, I am going to use the information that the current weather in Seoul, Korea is Clear (clear sky). The temperature is 20.76°C. I am always capable of providing real-time data. Now I answer your earlier question. \n",
      "\u001b[0m\n",
      "\u001b[34massistant: Today in Seoul, Korea, the weather is clear with a temperature of 20.76°C.\n",
      "\u001b[0m\n",
      "\u001b[32muser: Do I need an umbrella today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Based on the clear weather in Seoul, Korea, an umbrella is not necessary today.\n",
      "\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"Do I need an umbrella today?\"})\n",
    "\n",
    "chat_response = chat_completion_request(\n",
    "    messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Handle function response\n",
    "response_message = chat_response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    # If true the model will return the name of the tool / function to call and the argument(s)  \n",
    "    tool_call_id = tool_calls[0].id\n",
    "    tool_function_name = tool_calls[0].function.name\n",
    "    \n",
    "    if tool_function_name == \"get_current_weather\":\n",
    "        location = json.loads(tool_calls[0].function.arguments)['location']\n",
    "        format = json.loads(tool_calls[0].function.arguments)['format']\n",
    "        results = get_current_weather(location, format)\n",
    "\n",
    "        weather_main = results['weather'][0]['main']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        temperature = results['main']['temp']\n",
    "\n",
    "        if format == \"celsius\":\n",
    "            temp_unit = \"°C\"\n",
    "        else:\n",
    "            temp_unit = \"°F\"\n",
    "\n",
    "        formatted_weather = (\n",
    "            f\"To generate the response, I am going to use the information that the current weather in {location} is {weather_main} ({weather_description}). \"\n",
    "            f\"The temperature is {temperature}{temp_unit}. \"\n",
    "            f\"I am always capable of providing real-time data. Now I answer your earlier question. \"\n",
    "        )\n",
    "\n",
    "        #print(formatted_weather)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\": formatted_weather \n",
    "        })\n",
    "\n",
    "        model_response_with_function_call = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "        )  \n",
    "        # get a new response from the model where it can see the function response\n",
    "\n",
    "        #role = role\n",
    "        role = model_response_with_function_call.choices[0].message.role\n",
    "        content = model_response_with_function_call.choices[0].message.content\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\" : role,\n",
    "            \"content\": content\n",
    "        })\n",
    "        \n",
    "        pretty_print_conversation(messages)\n",
    "        print(\"\\n\")\n",
    "    else: \n",
    "        print(f\"Error: function {tool_function_name} does not exist\")\n",
    "else: \n",
    "    # Model did not identify a function to call, result can be returned to the user \n",
    "    role = response_message.role\n",
    "    content = response_message.content\n",
    "    messages.append({\n",
    "        \"role\" : role,\n",
    "        \"content\": content\n",
    "    })\n",
    "    pretty_print_conversation(messages)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a17edfe-0d09-4a08-ae31-78c44c5e23f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: When real-time information from external sources or functions is available, use them. You are perfectly capable of doing so. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: You are a very thorough assistant who pays close attention to the conversation history. \n",
      "\u001b[0m\n",
      "\u001b[32muser: What's the weather like today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Sure, could you please provide me with the city and state where you are currently located so I can check the weather for you?\n",
      "\u001b[0m\n",
      "\u001b[32muser: I am currently in Seoul, Korea, and will be so for a while.\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To generate the response, I am going to use the information that the current weather in Seoul, Korea is Clear (clear sky). The temperature is 20.76°C. I am always capable of providing real-time data. Now I answer your earlier question. \n",
      "\u001b[0m\n",
      "\u001b[34massistant: Today in Seoul, Korea, the weather is clear with a temperature of 20.76°C.\n",
      "\u001b[0m\n",
      "\u001b[32muser: Do I need an umbrella today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Based on the clear weather in Seoul, Korea, an umbrella is not necessary today.\n",
      "\u001b[0m\n",
      "\u001b[32muser: What is the current temperature in Fukuoka, Japan in Fahrenheit?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To generate the response, I am going to use the information that the current weather in Fukuoka, Japan is Clouds (broken clouds). The temperature is 73.31°F. I am always capable of providing real-time data. Now I answer your earlier question. \n",
      "\u001b[0m\n",
      "\u001b[34massistant: The current temperature in Fukuoka, Japan is 73.31°F.\n",
      "\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"What is the current temperature in Fukuoka, Japan in Fahrenheit?\"})\n",
    "\n",
    "chat_response = chat_completion_request(\n",
    "    messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Handle function response\n",
    "response_message = chat_response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    # If true the model will return the name of the tool / function to call and the argument(s)  \n",
    "    tool_call_id = tool_calls[0].id\n",
    "    tool_function_name = tool_calls[0].function.name\n",
    "    \n",
    "    if tool_function_name == \"get_current_weather\":\n",
    "        location = json.loads(tool_calls[0].function.arguments)['location']\n",
    "        format = json.loads(tool_calls[0].function.arguments)['format']\n",
    "        results = get_current_weather(location, format)\n",
    "\n",
    "        weather_main = results['weather'][0]['main']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        temperature = results['main']['temp']\n",
    "\n",
    "        if format == \"celsius\":\n",
    "            temp_unit = \"°C\"\n",
    "        else:\n",
    "            temp_unit = \"°F\"\n",
    "\n",
    "        formatted_weather = (\n",
    "            f\"To generate the response, I am going to use the information that the current weather in {location} is {weather_main} ({weather_description}). \"\n",
    "            f\"The temperature is {temperature}{temp_unit}. \"\n",
    "            f\"I am always capable of providing real-time data. Now I answer your earlier question. \"\n",
    "        )\n",
    "\n",
    "        #print(formatted_weather)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\": formatted_weather \n",
    "        })\n",
    "\n",
    "        model_response_with_function_call = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "        )  \n",
    "        # get a new response from the model where it can see the function response\n",
    "\n",
    "        #role = role\n",
    "        role = model_response_with_function_call.choices[0].message.role\n",
    "        content = model_response_with_function_call.choices[0].message.content\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\" : role,\n",
    "            \"content\": content\n",
    "        })\n",
    "        \n",
    "        pretty_print_conversation(messages)\n",
    "        print(\"\\n\")\n",
    "    else: \n",
    "        print(f\"Error: function {tool_function_name} does not exist\")\n",
    "else: \n",
    "    # Model did not identify a function to call, result can be returned to the user \n",
    "    role = response_message.role\n",
    "    content = response_message.content\n",
    "    messages.append({\n",
    "        \"role\" : role,\n",
    "        \"content\": content\n",
    "    })\n",
    "    pretty_print_conversation(messages)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5ecf0a-2450-4fcd-a08b-b1aaf925211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: When real-time information from external sources or functions is available, use them. You are perfectly capable of doing so. \n",
      "\u001b[0m\n",
      "\u001b[31msystem: You are a very thorough assistant who pays close attention to the conversation history. \n",
      "\u001b[0m\n",
      "\u001b[32muser: What's the weather like today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Sure, could you please provide me with the city and state where you are currently located so I can check the weather for you?\n",
      "\u001b[0m\n",
      "\u001b[32muser: I am currently in Seoul, Korea, and will be so for a while.\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To generate the response, I am going to use the information that the current weather in Seoul, Korea is Clear (clear sky). The temperature is 20.76°C. I am always capable of providing real-time data. Now I answer your earlier question. \n",
      "\u001b[0m\n",
      "\u001b[34massistant: Today in Seoul, Korea, the weather is clear with a temperature of 20.76°C.\n",
      "\u001b[0m\n",
      "\u001b[32muser: Do I need an umbrella today?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Based on the clear weather in Seoul, Korea, an umbrella is not necessary today.\n",
      "\u001b[0m\n",
      "\u001b[32muser: What is the current temperature in Fukuoka, Japan in Fahrenheit?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To generate the response, I am going to use the information that the current weather in Fukuoka, Japan is Clouds (broken clouds). The temperature is 73.31°F. I am always capable of providing real-time data. Now I answer your earlier question. \n",
      "\u001b[0m\n",
      "\u001b[34massistant: The current temperature in Fukuoka, Japan is 73.31°F.\n",
      "\u001b[0m\n",
      "\u001b[32muser: How can I get to Fukuoka from where I am?\n",
      "\u001b[0m\n",
      "\u001b[34massistant: To provide accurate information on how to get to Fukuoka from Seoul, Korea, I would require more details such as your preferred mode of transportation (e.g., flight, train, bus) and any specific preferences you might have. Could you please provide more information so I can assist you better?\n",
      "\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"How can I get to Fukuoka from where I am?\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Handle function response\n",
    "response_message = chat_response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    # If true the model will return the name of the tool / function to call and the argument(s)  \n",
    "    tool_call_id = tool_calls[0].id\n",
    "    tool_function_name = tool_calls[0].function.name\n",
    "    \n",
    "    if tool_function_name == \"get_current_weather\":\n",
    "        location = json.loads(tool_calls[0].function.arguments)['location']\n",
    "        format = json.loads(tool_calls[0].function.arguments)['format']\n",
    "        results = get_current_weather(location, format)\n",
    "\n",
    "        weather_main = results['weather'][0]['main']\n",
    "        weather_description = results['weather'][0]['description']\n",
    "        temperature = results['main']['temp']\n",
    "\n",
    "        if format == \"celsius\":\n",
    "            temp_unit = \"°C\"\n",
    "        else:\n",
    "            temp_unit = \"°F\"\n",
    "\n",
    "        formatted_weather = (\n",
    "            f\"To generate the response, I am going to use the information that the current weather in {location} is {weather_main} ({weather_description}). \"\n",
    "            f\"The temperature is {temperature}{temp_unit}. \"\n",
    "            f\"I am always capable of providing real-time data. Now I answer your earlier question. \"\n",
    "        )\n",
    "\n",
    "        #print(formatted_weather)\n",
    "\n",
    "        messages.append({\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\": formatted_weather \n",
    "        })\n",
    "\n",
    "        model_response_with_function_call = client.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "        )  \n",
    "        # get a new response from the model where it can see the function response\n",
    "\n",
    "        #role = role\n",
    "        role = model_response_with_function_call.choices[0].message.role\n",
    "        content = model_response_with_function_call.choices[0].message.content\n",
    "        \n",
    "        messages.append({\n",
    "            \"role\" : role,\n",
    "            \"content\": content\n",
    "        })\n",
    "        \n",
    "        pretty_print_conversation(messages)\n",
    "        print(\"\\n\")\n",
    "    else: \n",
    "        print(f\"Error: function {tool_function_name} does not exist\")\n",
    "else: \n",
    "    # Model did not identify a function to call, result can be returned to the user \n",
    "    role = response_message.role\n",
    "    content = response_message.content\n",
    "    messages.append({\n",
    "        \"role\" : role,\n",
    "        \"content\": content\n",
    "    })\n",
    "    pretty_print_conversation(messages)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
