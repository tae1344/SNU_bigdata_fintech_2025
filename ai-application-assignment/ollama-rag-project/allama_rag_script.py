import warnings
import streamlit as st
import glob
import os
import json
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, Language
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.documents import Document

# HTML íŒŒì¼ ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import UnstructuredHTMLLoader
from bs4 import BeautifulSoup

warnings.filterwarnings("ignore")


def extract(file_path):
    try:
        with open(file_path) as f:
            notebook = json.load(f)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""
    code_cells = []
    for cell in notebook["cells"]:
        if cell["cell_type"] == "code":
            code_cells.append("".join(cell["source"]))
        elif cell["cell_type"] == "markdown":
            code_cells.append("".join(cell["source"]))

    all_text = "\n\n".join(code_cells)

    return all_text


def extract_text_from_notebook(file_path):
    lines = extract(file_path).split("\n")

    imports = []
    functions = []
    classes = []
    code_blocks = []

    in_code_block = False
    current_code_block = []

    for line in lines:
        line = line.strip()

        # Python ì½”ë“œ íŒ¨í„´ ê°ì§€
        if (
            line.startswith("import")
            or line.startswith("from")
            or line.startswith("def")
            or line.startswith("class")
            or line.startswith("if")
            or line.startswith("for")
            or line.startswith("while")
            or "=" in line
        ):

            if line.startswith("import") or line.startswith("from"):
                imports.append(line)
            elif line.startswith("def "):
                functions.append(line)
            elif line.startswith("class "):
                classes.append(line)

            current_code_block.append(line)
            in_code_block = True
        elif in_code_block and line == "":
            if current_code_block:
                code_blocks.append("\n".join(current_code_block))
            current_code_block = []
            in_code_block = False
        elif in_code_block:
            current_code_block.append(line)

    # ë§ˆì§€ë§‰ ì½”ë“œ ë¸”ë¡ ì¶”ê°€
    if current_code_block:
        code_blocks.append("\n".join(current_code_block))

    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    imports = list(set([imp for imp in imports if imp]))
    functions = list(set([func for func in functions if func]))
    classes = list(set([cls for cls in classes if cls]))
    code_blocks = [block for block in code_blocks if block and len(block.strip()) > 5]

    project_type = "Unknown"
    if any("tensorflow" in imp.lower() or "keras" in imp.lower() for imp in imports):
        project_type = "Deep Learning (TensorFlow/Keras)"
    elif any("torch" in imp.lower() or "pytorch" in imp.lower() for imp in imports):
        project_type = "Deep Learning (PyTorch)"
    elif any("sklearn" in imp.lower() or "scikit" in imp.lower() for imp in imports):
        project_type = "Machine Learning (Scikit-learn)"
    elif any("pandas" in imp.lower() or "numpy" in imp.lower() for imp in imports):
        project_type = "Data Analysis"
    elif any(
        "streamlit" in imp.lower() or "flask" in imp.lower() or "fastapi" in imp.lower()
        for imp in imports
    ):
        project_type = "Web Application"

    analysis_result = {
        "project_title": os.path.basename(file_path).split(".")[0],
        "file_path": file_path,
        "project_type": project_type,
        "total_imports": len(imports),
        "total_functions": len(functions),
        "total_classes": len(classes),
        "total_code_blocks": len(code_blocks),
        "imports": imports[:15],  # ì²˜ìŒ 15ê°œë§Œ
        "functions": functions[:10],  # ì²˜ìŒ 10ê°œë§Œ
        "classes": classes[:10],  # ì²˜ìŒ 10ê°œë§Œ
        "code": code_blocks if code_blocks else [],
    }

    return analysis_result


def create_html_documents_for_rag(all_projects):
    """
    ë¶„ì„ëœ HTML í”„ë¡œì íŠ¸ë“¤ì„ RAG ì‹œìŠ¤í…œìš© ë¬¸ì„œë¡œ ë³€í™˜
    """
    documents = []

    for project in all_projects:
        # í”„ë¡œì íŠ¸ ê°œìš” ë¬¸ì„œ ìƒì„±
        doc_content = f"""
í”„ë¡œì íŠ¸ëª…: {project['project_title']}
íŒŒì¼ ê²½ë¡œ: {project['file_path']}
í”„ë¡œì íŠ¸ ìœ í˜•: {project['project_type']}

=== í†µê³„ ì •ë³´ ===
- ì´ ì„í¬íŠ¸ ìˆ˜: {project['total_imports']}
- ì´ í•¨ìˆ˜ ìˆ˜: {project['total_functions']}
- ì´ í´ë˜ìŠ¤ ìˆ˜: {project['total_classes']}
- ì´ ì½”ë“œ ë¸”ë¡ ìˆ˜: {project['total_code_blocks']}

=== ì£¼ìš” ì„í¬íŠ¸ ===
{chr(10).join(project['imports'])}

=== í•¨ìˆ˜ ì •ì˜ ===
{chr(10).join(project['functions'])}

=== í´ë˜ìŠ¤ ì •ì˜ ===  
{chr(10).join(project['classes'])}

=== ì½”ë“œ ì˜ˆì‹œ ===
{chr(10).join(project['code'])}
"""

        doc = Document(
            page_content=doc_content,
            metadata={
                "source": project["file_path"],
                "project_name": project["project_title"],
                "project_type": project["project_type"],
            },
        )
        documents.append(doc)

    return documents


# HTML í”„ë¡œì íŠ¸ìš© RAG ì²´ì¸ ìƒì„±
def create_html_project_rag_chain(all_projects):
    """
    HTML í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì²´ì¸ ìƒì„±
    """
    # ë°ì´í„° ê²€ì¦
    if not all_projects:
        raise ValueError("ë¶„ì„í•  í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(f"ì´ {len(all_projects)}ê°œ í”„ë¡œì íŠ¸ ì²˜ë¦¬ ì¤‘...")

    # ë¬¸ì„œ ìƒì„±
    documents = create_html_documents_for_rag(all_projects)
    print(f"{len(documents)}ê°œ ë¬¸ì„œ ìƒì„±ë¨")

    # ë¬¸ì„œ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.PYTHON, chunk_size=3000, chunk_overlap=500
    )
    split_docs = splitter.split_documents(documents)
    print(f"{len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")

    # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    embeddings = HuggingFaceEmbeddings(
        model_name="jinaai/jina-embeddings-v2-base-code",
        model_kwargs={"device": "mps"},
        encode_kwargs={"normalize_embeddings": True},
    )

    vectorstore = FAISS.from_documents(split_docs, embeddings)
    # retriever ì„¤ì •ì„ ëª¨ë“  ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
    retriever = vectorstore.as_retriever(
        search_type="similarity", search_kwargs={"k": len(split_docs)}  # ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
    )
    print(f"ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±ë¨ (ì´ {len(split_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰)")

    # LLM ì„¤ì •
    llm = ChatOllama(model="llama3.1:8b")

    # í”„ë¡¬í”„íŠ¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì›)
    prompt = ChatPromptTemplate.from_template(
        """
        ë‹¹ì‹ ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë° ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
        <context>
        {context}
        </context>

        ì§ˆë¬¸: {input}

        ë‹µë³€:
      """
    )

    # RAG ì²´ì¸ ìƒì„±
    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    return retrieval_chain, retriever


def main():
    # RAG ì²´ì¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸

    DATA_PATH = "./data"
    notebook_name = "AT"

    all_projects = [extract_text_from_notebook(f"{DATA_PATH}/{notebook_name}.ipynb")]

    print("ğŸ¤– HTML í”„ë¡œì íŠ¸ ë¶„ì„ AI ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")
    html_rag_chain, html_retriever = create_html_project_rag_chain(all_projects)
    print("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ!")

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = ["í•´ë‹¹ í”„ë¡œì íŠ¸ì˜ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ì–´ìˆë‚˜ìš”?"]

    print("\nì§ˆë¬¸ ì‹¤í–‰:")
    for i, question in enumerate(test_questions, 1):
        print(f"\n=== ì§ˆë¬¸ {i}: {question} ===")

        retrieved_docs = html_retriever.get_relevant_documents(question)
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")

        # ëª¨ë“  í”„ë¡œì íŠ¸ ì´ë¦„ í™•ì¸
        project_names = [
            doc.metadata.get("project_name", "Unknown") for doc in retrieved_docs
        ]
        print(f"ê²€ìƒ‰ëœ í”„ë¡œì íŠ¸ë“¤: {set(project_names)}")

        docount = 0
        for j, doc in enumerate(retrieved_docs, 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"ë¬¸ì„œ {j}ì˜ í”„ë¡œì íŠ¸ ì´ë¦„: {doc.metadata['project_name']}")
            docount += 1

        print(f"{docount}ê°œ ë¬¸ì„œì—ì„œ ë‹µë³€ ìƒì„± ì¤‘...")

        response = html_rag_chain.invoke({"input": question})
        print("ğŸ’¬ ë‹µë³€:", response["answer"])
        print("-" * 50)


if __name__ == "__main__":
    main()
